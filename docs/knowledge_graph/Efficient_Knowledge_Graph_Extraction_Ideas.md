# Efficient Knowledge Graph Extraction: Incremental Curriculum + Case‚ÄëBased Policy Selection

> A practical playbook to speed up and cheapen ACE‚Äëstyle (Extractor ‚Üí Reflector/Curator ‚Üí ER/Dedup) knowledge‚Äëgraph extraction while preserving global coherence.

---

## TL;DR

* **Combine two tactics**:

  1. **Incremental curriculum learning** for rapid extractor iteration on small, informative samples.
  2. **Case‚Äëbased policy selection** to reuse the best extraction ‚Äúrecipe‚Äù (model+prompt+params) from similar, previously solved pages.
* **Protect global quality** with a cheap, document‚Äëwide **sketch pass** and **incremental ER/dedup** that only touches affected blocks.
* Expect **2‚Äì5√ó faster inner loops** and **50%+ token savings** with equal or better final graph quality.

---

## Goals & Constraints

* **Goal:** Minimize time/$ per iteration while maintaining or improving final graph quality.
* **Constraints:**

  * Global properties (dedup, entity resolution, cross‚Äëreference relations) require broad context.
  * Iterations should avoid full reprocessing unless schema/ER primitives change.

---

## Components (Glossary)

* **Extractor:** LLM- or rule-based entity/relation extraction per chunk/page.
* **Reflector/Curator:** Evaluates outputs, proposes fixes (prompts, schema, code).
* **ER/Dedup:** Global entity resolution and duplication control.
* **Sketch Pass:** Cheap, whole‚Äëdocument scan producing candidate entities, alias blocks, co‚Äëmention hints and layout signatures.
* **Entity Card:** Canonical record per entity (name, aliases, type, summary, cites, embedding).
* **Recipe:** The extraction configuration: `{model, system_prompt_id, user_prompt_id, few_shots_id, chunking, retrieval, parser, postproc_flags}`.

---

## Strategy A ‚Äî Incremental Curriculum Learning (Sampling + Gates)

### When it shines

* Prompt engineering, schema adherence, local extraction quality, lightweight bug fixes.

### Sampling design

* **Start small:** 5‚Äì15% stratified sample (by section/template/entity density).
* **Grow geometrically:** Double the sample when gates are green (e.g., 10% ‚Üí 20% ‚Üí 40%).
* **Active mix:**

  * 40% **uncertainty** (low confidence/high disagreement)
  * 30% **coverage gaps** (rare types/patterns unseen)
  * 20% **regression probes** (known hard cases)
  * 10% **random baseline**
* **Holdout canary:** 20‚Äì50 labeled exemplars never used for tuning.

### Staged loop (one document / site)

1. **Stage 1 ‚Äî Local extraction quality (sampling)**

   * Iterate on prompts/code using the sample + canary.
2. **Stage 2 ‚Äî First full pass (validation)**

   * Run a full extraction to surface global issues (ER, dedup, cross‚Äërefs).
3. **Stage 3 ‚Äî Targeted re‚Äëtests (sampling)**

   * Re‚Äësample problematic regions/templates to verify fixes cheaply.
4. **Stage 4 ‚Äî Final full pass (confirmation)**

   * Produce the release artifact and compute final global metrics.

### Promotion gates (accept change only if all pass)

* **Local Gate:** ŒîF1‚Üë (by type), schema‚Äëadherence ‚â• baseline, hallucination‚Üì.
* **Global Gate (from sketch + micrograph):** alias entropy‚Üì, block collisions‚Üì, relation histogram stable, merge/split errors not worse on canary.

### When to trigger a full run

* Schema change, ER blocking change, major prompt/template overhaul, or proxy metrics plateau.

---

## Strategy B ‚Äî Case‚ÄëBased Policy Selection (Recipe Memory)

### Idea

For each new page/chunk, **retrieve similar solved pages** and start with the **recipe** that worked best there. This cuts retries, tokens, and latency.

### What to store per solved page (recipe memory)

* **Descriptors (for retrieval):**

  * Text embedding of the page/chunk
  * Structure/DOM/lightweight layout signature
  * Cheap NER histogram, token/section stats
* **Recipe fingerprint:** model, prompt IDs, few‚Äëshot ID, chunking, retrieval, parser, postproc flags
* **Outcomes:** schema pass rate, local F1 proxies, tokens used, latency, and any recorded global side‚Äëeffects (merge/split touches)

### Retrieval & selection

1. Compute descriptors for the new page.
2. kNN search (top‚ÄëK=20) over validated pages; re‚Äërank with text+structure+sketch.
3. Score candidate recipes:

   [score(r) = w_sim¬∑avg_sim + w_q¬∑E[quality|r] ‚àí w_cost¬∑E[cost|r] + w_rec¬∑recency ‚àí w_var¬∑Var(quality|r)]
4. **Explore‚Äìexploit:** Œµ‚Äëgreedy or Thompson sampling to avoid lock‚Äëin and handle drift.
5. **Racing:** Try the best 1‚Äì3 recipes with small budgets; early‚Äëstop on first high‚Äëconfidence pass.

### Cold start & drift

* If no neighbor above similarity threshold (‚âà0.8‚Äì0.85 after re‚Äërank), fall back to **baseline recipe**.
* Bootstrap new domains with 5‚Äì10 diverse pages (by template/path) to seed memory.
* Monitor rolling hit‚Äërate; raise Œµ when drift is detected.

---

## Safeguards for Global Coherence

### Cheap **Sketch Pass** (whole document/site)

Compute once per iteration:

* Small‚Äëmodel NER on sentences ‚Üí candidate entity strings + embeddings
* Alias blocking via normalized names + n‚Äëgram LSH/MinHash
* Co‚Äëmention hints (entity pairs within 2‚Äì3 sentences)
* Light structure signature (DOM path patterns, heading shapes)

Use these to:

* Flag likely alias collisions before heavy extraction
* Estimate relation‚Äëtype drift
* Prioritize risky regions for active sampling

### **Incremental ER/Dedup**

* Deterministic stable IDs (hash of canonical label+type)
* Blocked reclustering only for **affected blocks** when new extractions land
* Union‚Äëfind for merges; negative cache for proven non‚Äëmatches
* Entity cards passed to LLM only on decision boundaries

### Proxies ‚Üî Final metrics

* Correlate proxy metrics (alias entropy, collision count, cluster cohesion) with full‚Äërun ER scores (B¬≥/CEAF‚ÄëE). If correlation is high, defer full runs.

---

## Pipeline Overview

1. **Sketch:** cheap global scan ‚Üí alias blocks, co‚Äëmentions, structure.
2. **Sample:** select stratified/active subset for heavy extraction.
3. **Policy Select:** choose recipe for each page via recipe memory (kNN + bandit).
4. **Extract (heavy only where needed):** two‚Äëstage (light proposal ‚Üí LLM on uncertain cases).
5. **ER/Dedup (incremental):** recluster only touched blocks; update entity cards.
6. **Gates:** local + global; accept or revert changes.
7. **Grow sample** geometrically when stable; **full run** when required.

---

## Pseudocode

### Case‚ÄëBased Policy Selection

```python
def select_recipe(new_page):
    x = build_descriptor(new_page)  # text + structure + sketch stats
    nbrs = vector_search(x, top_k=20, filter=validated=True)
    candidates = aggregate_recipes(nbrs)  # {recipe_id: stats}

    def score(s):
        return (w_sim*s.avg_similarity
              + w_q*s.exp_quality
              - w_cost*s.exp_cost
              + w_rec*s.recency
              - w_var*s.quality_variance)

    ranked = sorted(candidates.values(), key=score, reverse=True)
    pool = explore_exploit(ranked, epsilon=0.1)  # small exploration

    for recipe in pool[:3]:  # racing with small budgets
        out = run_extraction(new_page, recipe, budget="small")
        if passes_validators(out):
            return out, recipe

    return run_extraction(new_page, baseline_recipe, budget="normal")
```

### Outer Loop (Incremental Curriculum + Global Safeguards)

```python
while True:
    sketch = run_sketch_pass(all_pages)
    sample = pick_active_sample(pages, sketch, canary, size=geometric())

    for page in sample:
        result, recipe = select_recipe(page)
        persist(page, result, recipe)

    impacted_blocks = infer_impacted_blocks(sample, sketch)
    incremental_er_dedup(impacted_blocks)

    metrics_local = eval_local(sample, canary)
    metrics_global = eval_global_proxies(sketch, impacted_blocks)

    if gates_pass(metrics_local, metrics_global):
        promote_changes()
        maybe_grow_sample()
    else:
        revert_last_changes()

    if need_full_run(metrics_trend, changeset):
        full_results = heavy_extract(all_pages, policy_select=True)
        full_er_dedup()
        update_proxy_correlations(full_results)
```

---

## Data Schemas (suggested)

### Recipe Memory Record

```json
{
  "page_id": "...",
  "timestamp": "...",
  "descriptors": {
    "text_emb": [ ... ],
    "structure_sig": { "dom_path_hist": {"H1/P": 12, ...}, "len": 2310 },
    "ner_hist": {"PERSON": 4, "ORG": 7, "DATE": 3}
  },
  "recipe": {
    "model": "gpt-4o-mini",
    "system_prompt_id": "sp_17",
    "user_prompt_id": "up_42",
    "few_shots_id": "fs_news_v2",
    "chunking": {"size": 900, "overlap": 100},
    "parser": "json_schema_v3",
    "postproc": {"normalize_dates": true, "unit_harmonize": true}
  },
  "outcomes": {
    "schema_pass_rate": 0.98,
    "local_f1_proxy": 0.86,
    "tokens": 8200,
    "latency_ms": 3300,
    "global_touches": {"merges": 3, "splits": 0}
  }
}
```

### Entity Card

```json
{
  "entity_id": "hash(name|type)",
  "type": "PERSON|ORG|...",
  "canonical": "James Smith",
  "aliases": ["Dr. James Smith", "J. Smith"],
  "summary": "Professor‚Ä¶",
  "top_citations": [ {"page_id": "p1", "sent_id": 33 }, ... ],
  "embedding": [ ... ],
  "last_updated": "..."
}
```

### Delta Log (Incremental ER/Dedup)

```json
{
  "iteration": 123,
  "affected_blocks": ["block_7a", "block_c3"],
  "merges": [["e_12","e_98"]],
  "splits": ["e_44"],
  "negatives": [["e_21","e_77"]]
}
```

---

## Metrics & Dashboards

* **Local Extraction:** precision/recall/F1 by type; schema adherence; hallucination rate.
* **ER/Dedup:** B¬≥ / CEAF‚ÄëE; merge/split error counts; cluster stability; dup ratio.
* **Operational:** tokens/page; $/page; throughput (pages/min); retries/page; time‚Äëto‚Äëfirst‚Äëvalid.
* **Proxies:** alias entropy; block collisions; relation histogram KL‚Äëdivergence vs. baseline.

**Promotion Rules (defaults):**

* ŒîF1 on canary ‚â• **+1‚Äì2 pts** and no type drops > **1 pt**.
* Alias entropy **‚Üì** and block collisions **‚Üì** w.r.t. previous iteration.
* Tokens/page **‚â§ baseline**; latency not worse by > **10%**.

---

## Practical Defaults

* **Sample size:** start 5‚Äì15%; double on two consecutive green gates.
* **kNN:** top‚ÄëK=20 neighbors; similarity threshold 0.8‚Äì0.85 after re‚Äërank.
* **Bandit:** Œµ=0.1 (raise to 0.2 on drift); or Thompson sampling on per‚Äërecipe Beta priors.
* **Racing:** try top 2‚Äì3 recipes with small context budgets; early‚Äëstop on first high‚Äëconfidence pass.
* **Blocking:** normalized names + 3‚Äëgram LSH; cosine bucket on entity embeddings.
* **Full runs:** every 3‚Äì5 accepted changesets **or** when schema/ER primitives change.

---

## Practical Implementation Roadmap (By Scale)

### Context-Aware Strategy Selection

The strategies above are powerful but vary dramatically in complexity and ROI depending on your extraction scale. **Start simple, add complexity only when bottlenecks appear.**

#### üìä Scale 1: Single Document (1-5 books, 50-200 pages each)

**Your Context:**
- Cost: ~$3-5 per full extraction
- Time: 20-40 minutes per run
- Goal: Rapid ACE iteration (V5 ‚Üí V6 ‚Üí V7 ‚Üí V8)

**Recommended Implementation:**
- ‚úÖ **Strategy A** (Incremental Curriculum + Gates) - HIGH ROI, LOW COMPLEXITY
- ‚ùå **Strategy B** (Recipe Memory) - NO VALUE (no diverse documents to learn from)
- ‚ùå **Sketch Pass** - UNNECESSARY (ER/dedup not a bottleneck)
- ‚ùå **Incremental ER/Dedup** - OVERKILL (full runs are cheap)

**Expected Improvements:**
- Time savings: ~25% (save 5-10 minutes per ACE cycle)
- Cost savings: ~20% (save $0.60-1.00 per iteration)
- Quality: Same or better (more targeted fixes)
- Engineering effort: 1-2 days

**What to Build (Priority Order):**

1. **Sampling Mode** (1 day)
   ```bash
   # Add CLI flags to your extraction script
   python extract_kg_v8_book.py --sample-chunks 10 --stratified
   python extract_kg_v8_book.py --full-extraction
   python extract_kg_v8_book.py --resume-from-checkpoint checkpoint_123
   ```

2. **Basic Promotion Gates** (0.5 days)
   ```python
   def passes_local_gate(results):
       return (results['schema_valid'] >= 0.98 and
               results['hallucination_rate'] < 0.05)

   def passes_global_gate(results, baseline):
       return (results['duplicate_count'] <= baseline['duplicate_count'] and
               results['entity_resolution_f1'] >= baseline['entity_resolution_f1'])
   ```

3. **4-Stage Workflow Wrapper** (0.5 days)
   ```python
   # scripts/run_ace_kg_incremental.py

   # Stage 1: Sample extraction (5-10 chunks)
   run_extraction(sample_size=10, skip_full=True)

   # Stage 2: Full extraction (if gates pass)
   if passes_local_gate():
       run_extraction(full=True)

   # Stage 3: Targeted re-test (problem chunks only)
   if has_issues():
       run_extraction(chunks=problem_chunk_ids)

   # Stage 4: Final full pass (confirm improvements)
   run_extraction(full=True, production=True)
   ```

**DON'T BUILD NOW:**
- Recipe memory system (8+ hours engineering, $0 value for single book)
- Sketch pass infrastructure (4+ hours, unnecessary complexity)
- kNN search system (6+ hours, no similar documents to retrieve)

#### üìä Scale 2: Medium Corpus (10-50 books, or 172 podcast episodes)

**Your Context:**
- Processing diverse documents with varying structures
- Some episodes about soil, others about policy, education, business
- Recipe reuse starts to pay dividends

**Recommended Implementation:**
- ‚úÖ **Strategy A** (already implemented)
- ‚úÖ **Strategy B** (Recipe Memory) - HIGH ROI NOW
- ‚ö†Ô∏è **Sketch Pass** - MAYBE (if ER/dedup becomes slow)
- ‚ùå **Incremental ER/Dedup** - NOT YET (full runs still tractable)

**Expected Improvements:**
- Time savings: **2-5√ó speedup** (episode 50 extracts better because of episodes 1-49)
- Cost savings: **50%+** (fewer retries, better recipe selection)
- Quality: Better (learn from diverse examples)
- Engineering effort: 3-5 days

**What to Build (Priority Order):**

1. **Recipe Memory Store** (2 days)
   ```python
   # After each successful extraction, save:
   {
     "page_id": "episode_120_transcript",
     "descriptors": {
       "text_embedding": [...],
       "structure": {"avg_sentence_len": 18, "entity_density": 0.12},
       "ner_hist": {"PERSON": 4, "ORG": 7}
     },
     "recipe": {
       "model": "gpt-4o-mini",
       "prompt_version": "v8",
       "chunking": {"size": 900, "overlap": 100}
     },
     "outcomes": {
       "schema_pass_rate": 0.98,
       "tokens": 8200,
       "quality": 0.95
     }
   }
   ```

2. **Simple kNN Retrieval** (1 day)
   ```python
   def select_recipe_for_new_episode(episode):
       # Compute text embedding
       embedding = openai.embeddings.create(input=episode[:1000])

       # Find top-3 similar episodes
       similar = vector_search(embedding, top_k=3)

       # Use recipe from best-performing similar episode
       return similar[0]['recipe']
   ```

3. **Exploration-Exploitation** (0.5 days)
   ```python
   # 90% use best recipe, 10% try alternatives
   if random.random() < 0.1:
       recipe = baseline_recipe  # explore
   else:
       recipe = best_recipe_from_knn  # exploit
   ```

#### üìä Scale 3: Production System (100+ diverse documents, ongoing extraction)

**Your Context:**
- Continuous extraction from multiple sources
- ER/dedup becomes computationally expensive
- Need <10 second latency per page

**Recommended Implementation:**
- ‚úÖ **Strategy A** (foundation)
- ‚úÖ **Strategy B** (recipe memory)
- ‚úÖ **Sketch Pass** - CRITICAL (avoid expensive full processing)
- ‚úÖ **Incremental ER/Dedup** - ESSENTIAL (reprocessing everything is too slow)

**Expected Improvements:**
- Time savings: **10-20√ó on incremental updates**
- Cost savings: **75%+** (process only what changed)
- Latency: **<10s per page** (with warm cache)
- Engineering effort: 2-3 weeks

This is when the full architecture in this document pays off.

### Implementation Checklist for Scale 1 (Most Users Start Here)

**Week 1: Sampling + Gates**
- [ ] Add `--sample-chunks N` flag to extraction script
- [ ] Implement stratified sampling (random + high entity density + complex relationships)
- [ ] Add checkpoint/resume functionality
- [ ] Implement local promotion gate (schema validation + hallucination check)
- [ ] Implement global promotion gate (dedup trends + ER quality)
- [ ] Test on 50-page book with 5-stage progression (5 ‚Üí 10 ‚Üí 20 ‚Üí 40 ‚Üí full)

**Week 2: Workflow Integration**
- [ ] Create `run_ace_kg_incremental.py` wrapper script
- [ ] Integrate 4-stage workflow (sample ‚Üí full ‚Üí targeted ‚Üí final)
- [ ] Add automatic gate evaluation and decision logic
- [ ] Add metrics dashboard (time saved, cost saved, quality trends)
- [ ] Document usage and examples

**Success Criteria:**
- ACE iterations run 25% faster
- No regression in final graph quality
- Clear metrics showing which stages caught which issues

### Anti-Pattern Warning ‚ö†Ô∏è

**DON'T:**
- ‚ùå Build recipe memory for a single document (waste of time)
- ‚ùå Implement sketch passes before ER/dedup is a bottleneck
- ‚ùå Add complex bandit algorithms when simple Œµ-greedy works
- ‚ùå Optimize before measuring (profile first!)

**DO:**
- ‚úÖ Start with simplest version that provides value
- ‚úÖ Measure before adding complexity
- ‚úÖ Scale infrastructure only when current approach is the bottleneck
- ‚úÖ Keep full extraction as ground truth for validating proxies

**Remember:** The goal is better graphs faster, not impressive infrastructure. Start simple, scale when needed.

---

## Risks & Mitigations

* **Template drift / domain shift:** monitor rolling hit‚Äërate; auto‚Äëincrease exploration; refresh few‚Äëshots.
* **Lock‚Äëin to suboptimal recipes:** enforce exploration floor; purge stale recipes by recency/quality.
* **Sampling blind spots:** maintain regression probes; enforce rare‚Äëtype coverage quota.
* **ER cascading errors:** block‚Äëlocal reclustering only; negative cache; manual overrides for critical entities.

---

## Implementation Notes (CLI/Flags)

* `--sketch-pass all` ‚Üí build alias blocks, co‚Äëmentions, structure signatures.
* `--sample-pass N` ‚Üí select active sample with quotas.
* `--policy-select` ‚Üí kNN + bandit recipe chooser per page.
* `--delta-merge` ‚Üí incremental ER/dedup on affected blocks only.
* `--global-gate from-sketch` ‚Üí alias entropy/collisions checks.
* `--full-run` ‚Üí heavy extract everything; recompute ground‚Äëtruth metrics.
* **Caching:** hash `(extractor_version, prompt_template, chunk_text)` to skip re‚Äëwork; cache negative ER pairs.

---

## Appendix

### Lightweight Sketch Features

* Sentence‚Äëlevel NER (tiny model/regex), name normalization, character n‚Äëgrams.
* MinHash/LSH for alias grouping and near‚Äëduplicate sentence detection.
* Co‚Äëmention counts within 2‚Äì3 sentences; relation type heuristics.
* DOM/structure fingerprints: path histograms, heading patterns, length stats.

### Active Sampling Signals

* Low confidence / high disagreement between two prompts/models.
* Unseen schema types or relation patterns.
* Blocks with high alias entropy or collision risk from the sketch pass.

### Template Detector (optional)

* Simple classifier using URL tokens, DOM depth stats, heading n‚Äëgrams.
* Condition recipe selection on predicted template ID.
