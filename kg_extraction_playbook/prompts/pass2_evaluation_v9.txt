```
Evaluate these {batch_size} extracted relationships using dual-signal analysis.

RELATIONSHIPS TO EVALUATE:
{relationships_json}

For EACH of the {batch_size} relationships above, provide TWO INDEPENDENT evaluations:

## 1. TEXT SIGNAL (ignore world knowledge)

How clearly does the text state this relationship?

**Score 0.0-1.0 based purely on text clarity:**
- **0.9-1.0**: Text explicitly and clearly states this relationship
- **0.7-0.9**: Text strongly implies this relationship
- **0.5-0.7**: Text suggests this relationship with some inference required
- **0.3-0.5**: Text vaguely hints at this relationship
- **0.0-0.3**: Text doesn't really support this relationship

## 2. KNOWLEDGE SIGNAL (ignore the text)

Is this relationship plausible, verifiable, and concrete given world knowledge?

### Knowledge Plausibility Scoring Criteria

**DEFINITION**: knowledge_plausibility (p_true) measures whether a relationship is EMPIRICALLY TESTABLE and FACTUAL, not whether it sounds plausible or inspirational.

**SCORING RULES**:
- **HIGH (0.7-1.0)**: Concrete, testable, factual claims. Examples: 'X contains Y', 'X published in 2018', 'X founded Y'
- **MEDIUM (0.4-0.6)**: Factual but requires domain knowledge to verify. Examples: 'X improves Y', 'X causes Y'
- **LOW (0.0-0.3)**: Philosophical claims, metaphors, subjective opinions, motivational statements

**RED FLAGS FOR LOW SCORES**:
- If relationship contains metaphorical language ('unlock', 'key to', 'door to', 'bridge to') → LOW score
- If target is abstract/philosophical ('the answer', 'spiritual growth', 'transformation') → LOW score
- If relationship expresses subjective opinion ('is beautiful', 'is inspiring') → LOW score
- If relationship is motivational ('empowers us to', 'invites us to') → LOW score

**QUICK EXAMPLES**:
- ✅ HIGH (0.9): (soil, contains, microorganisms) - factual, testable
- ✅ MEDIUM (0.6): (healthy soil, improves, crop yields) - factual but requires measurement
- ❌ LOW (0.2): (soil, is, the answer) - philosophical, not testable
- ❌ LOW (0.3): (cultivating soil, unlocks, spiritual growth) - metaphorical + subjective

**Score based on VERIFIABILITY and CONCRETENESS:**

### 0.0-0.3: Unverifiable / Philosophical / Metaphorical
- Metaphors ("our land is a veritable Eden")
- Philosophical claims ("spiritual flourishing depends on earth care")
- Subjective definitions ("being connected to soil is what it means to be human")
- Abstract opinions not grounded in facts
- Figurative language treated as literal
- Motivational/inspirational statements

**Examples**:
- (spiritual flourishing, depends on, earth care) → **0.2** (philosophical, not verifiable)
- (being connected to soil, is what it means to be, human) → **0.1** (subjective definition)
- (Our land, is, a veritable Eden) → **0.3** (metaphor, not literal fact)
- (the crossroads, is characterized by, immense complexity) → **0.2** (abstract metaphor)
- (cultivating soil, unlocks, spiritual growth) → **0.2** (metaphorical language + abstract target)
- (soil, is, the answer) → **0.2** (philosophical, vague abstract target)

### 0.4-0.6: Debatable / Abstract / Soft Claims
- Causal claims with some factual basis but debatable
- Abstract concepts with empirical support
- General statements that are plausible but not easily verified
- Relationships involving somewhat vague entities

**Examples**:
- (soil health, affects, community wellbeing) → **0.5** (causal claim with some evidence)
- (composting, improves, soil quality) → **0.6** (generally accepted, some evidence)
- (Learning about soil, benefits, families) → **0.5** (plausible but vague entities)

### 0.7-0.9: Concrete Facts / Minor Uncertainty
- Geographic facts
- Historical events
- Scientific consensus
- Well-documented relationships
- Concrete entities with verifiable relationships

**Examples**:
- (Slovenia, is located in, eastern Alpine region) → **0.8** (geographic fact)
- (biochar, sequesters, carbon) → **0.75** (scientific consensus)
- (composting, produces, humus) → **0.8** (verifiable biological process)
- (Y on Earth, hosts, podcast series) → **0.85** (easily verified organizational fact)

### 0.9-1.0: Easily Verifiable Facts
- Authorship (who wrote what)
- Organizational roles (who founded/works for what)
- Basic factual claims
- Relationships with concrete, named entities

**Examples**:
- (Aaron William Perry, authored, Soil Stewardship Handbook) → **1.0** (verifiable authorship)
- (Y on Earth, is, nonprofit organization) → **0.95** (easily verified)
- (Slovenia, is, country) → **1.0** (basic fact)
- (Adrian Del Caro, authored, Grounding the Nietzsche Rhetoric of Earth) → **1.0** (verifiable)

## ENTITY TYPE ASSESSMENT

For the knowledge signal, also consider:
- **What types are the source and target entities?**
- Are they concrete (people, places, organizations) or abstract (concepts, metaphors)?
- Concrete entities → higher knowledge scores
- Abstract entities → lower knowledge scores

## STATEMENT CLASSIFICATION

Classify each relationship with appropriate flags based on its nature:

### Classification Categories:

**FACTUAL** (p_true: 0.7-1.0):
- Verifiable facts, authorship, organizational relationships
- Example: (Aaron William Perry, authored, Soil Stewardship Handbook)

**TESTABLE_CLAIM** (p_true: 0.4-0.9):
- Assertions that evidence could support or oppose
- Example: (composting, improves, soil quality)
- Example: (spiritual practices, enhance, wellbeing)

**PHILOSOPHICAL_CLAIM** (p_true: 0.1-0.4):
- Existential/definitional statements not testable by evidence
- Example: (being connected to soil, is what it means to be, human)
- Example: (spiritual flourishing, depends on, earth care)

**METAPHOR** (p_true: 0.1-0.4):
- Figurative language, poetic comparisons
- Example: (our land, is, veritable Eden)
- Example: (the crossroads, is characterized by, complexity)
- Example: (cultivating soil, unlocks, spiritual growth)

**OPINION** (p_true: 0.3-0.6):
- Subjective viewpoints, preferences, beliefs
- Different from TESTABLE_CLAIM when no evidence path exists

**ABSTRACT_CONCEPT** (p_true: 0.2-0.5):
- Relationships involving highly abstract entities
- May overlap with other flags

### Multiple Flags:
A relationship can have multiple classification flags:
- (spiritual practices, enhance, wellbeing) → [TESTABLE_CLAIM, ABSTRACT_CONCEPT]
- (Slovenia, is, veritable Eden) → [METAPHOR]
- (Aaron Perry, authored, Handbook) → [FACTUAL]

## CONFLICT DETECTION

Set `signals_conflict=true` when signals diverge significantly:

### Case 1: High Text, Low Knowledge (>0.7 text, <0.4 knowledge)
**Example**: Text clearly states a metaphor, but it's not literally true
- Text: "Our land has remained a veritable Eden" → text_confidence: 0.9
- Knowledge: This is a metaphor, not literal → p_true: 0.3
- **Conflict**: "The text clearly says it, but 'veritable Eden' is a metaphor, not a factual description"

### Case 2: Low Text, High Knowledge (<0.4 text, >0.7 knowledge)
**Example**: Text is vague but the relationship is a well-known fact
- Text: Vague reference to a well-known fact → text_confidence: 0.3
- Knowledge: We know this is true → p_true: 0.8
- **Conflict**: "Text doesn't clearly state it, but we know from world knowledge it's true"

### Case 3: Philosophical/Abstract Despite Clear Text
**Example**: "being connected to soil is what it means to be human"
- Text: Sentence clearly states this → text_confidence: 0.9
- Knowledge: This is a philosophical claim, not a fact → p_true: 0.1
- **Conflict**: "Text is clear, but this is a subjective philosophical claim, not a verifiable fact"

### Case 4: Pronoun/Vague Entity Issues
**Example**: "we can connect with the living soil"
- Text: Sentence states this → text_confidence: 0.7
- Knowledge: "we" is too vague, not a concrete entity → p_true: 0.4
- **Conflict**: "Text states the relationship, but 'we' is a pronoun that needs resolution to a specific entity"

**When setting signals_conflict=true:**
- **MUST include** `conflict_explanation` describing WHY the signals conflict
- **SHOULD include** `suggested_correction` if you know how to fix it

**Examples of conflict_explanation**:
- "Text clearly states metaphor 'veritable Eden', but this is figurative language, not literal fact"
- "Philosophical definition ('what it means to be human') - not verifiable or concrete"
- "Source is pronoun 'we' which needs resolution to specific entity like 'humanity' or 'individuals'"
- "'spiritual flourishing' is an abstract concept - relationship is opinion/belief, not verifiable fact"

## OUTPUT FORMAT

For each relationship, return:
```json
{
  "candidate_uid": "<UNCHANGED from input>",
  "text_confidence": 0.0-1.0,
  "p_true": 0.0-1.0,
  "signals_conflict": true/false,
  "conflict_explanation": "string (if signals_conflict=true)",
  "suggested_correction": {"source": "...", "relationship": "...", "target": "..."} (optional),
  "source_type": "entity type",
  "target_type": "entity type",
  "classification_flags": ["FACTUAL" | "TESTABLE_CLAIM" | "PHILOSOPHICAL_CLAIM" | "METAPHOR" | "OPINION" | "ABSTRACT_CONCEPT"]
}
```

**classification_flags** should contain all applicable flags for the relationship.

## CRITICAL REMINDERS

1. **Return candidate_uid UNCHANGED** in every output object
2. **Evaluate all {batch_size} relationships** in the same order as input
3. **Assign classification flags** based on statement type (FACTUAL, METAPHOR, etc.)
4. **Knowledge scores reflect verifiability** - philosophical statements and metaphors score 0.0-0.3, but we still extract them
5. **Set signals_conflict=true** when text and knowledge diverge significantly (>0.3 difference)
6. **Always provide conflict_explanation** when signals_conflict=true
7. **Check for metaphorical language indicators** ('unlock', 'key to', 'bridge to', etc.) and score LOW (≤0.3)
8. **Abstract/philosophical targets** ('the answer', 'spiritual growth', 'transformation') should receive LOW scores (≤0.3)
```