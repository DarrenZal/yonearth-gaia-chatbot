{
  "extraction_metadata": {
    "version": "v7_meta_ace",
    "total_relationships": 924,
    "analysis_timestamp": "2025-10-12T21:30:10.094665"
  },
  "quality_summary": {
    "critical_issues": 4,
    "high_priority_issues": 12,
    "medium_priority_issues": 28,
    "mild_issues": 18,
    "total_issues": 62,
    "issue_rate_percent": 6.71,
    "estimated_false_negative_rate": 0.1,
    "estimated_total_issues_with_fn": 68,
    "adjusted_issue_rate_percent": 7.36,
    "grade_confirmed": "B+",
    "grade_adjusted": "B+",
    "note": "V7 shows significant improvement over V4 (26.8% issues) and V6 (7.58% issues). Main remaining issues are unresolved pronouns and praise quote misattribution."
  },
  "issue_categories": [
    {
      "category_name": "Reversed Authorship (Praise Quotes)",
      "severity": "CRITICAL",
      "count": 4,
      "percentage": 0.43,
      "description": "Endorsers/reviewers incorrectly identified as authors of the book. System extracts 'authored' relationships from praise quotes instead of 'endorsed' or 'reviewed' relationships.",
      "root_cause_hypothesis": "Pass 1 LLM extraction treats proximity of person's name + book mention as authorship. Bibliographic parser in Pass 2.5 only checks copyright/title pages, missing praise quotes in front matter.",
      "affected_module": "modules/pass1_extraction/entity_relationship_extractor.py",
      "affected_prompt": "prompts/pass1_extraction_prompt.txt",
      "affected_config": null,
      "examples": [
        {
          "source": "Michael Bowman",
          "relationship": "authored",
          "target": "Soil Stewardship Handbook",
          "evidence_text": "\u2014Michael Bowman Founding Board Chair, National Hemp Association and Chairman, Hemp for Victory Campaign Founding Director, \"25x'25\"",
          "page": 3,
          "what_is_wrong": "Michael Bowman is providing an endorsement quote, not claiming authorship. The evidence shows his credentials as an endorser, not author.",
          "should_be": {
            "source": "Michael Bowman",
            "relationship": "endorsed",
            "target": "Soil Stewardship Handbook"
          }
        },
        {
          "source": "Perry",
          "relationship": "wrote",
          "target": "Soil Stewardship Handbook",
          "evidence_text": "With his inspirational, aspirational, beautifully-informed and historically grounded handbook, Perry has given us a new appreciation for soil and its good works.",
          "page": 3,
          "what_is_wrong": "This is Adrian Del Caro praising Perry's work in a blurb, not Perry claiming authorship. Should be (Adrian Del Caro, praised, Perry's work).",
          "should_be": {
            "source": "Adrian Del Caro",
            "relationship": "praised",
            "target": "Perry's Soil Stewardship Handbook"
          }
        },
        {
          "source": "Brad Lidge",
          "relationship": "authored",
          "target": "Soil Stewardship Handbook",
          "evidence_text": "\u2014Brad Lidge 2008 World Series Champ, Philadelphia Phillies",
          "page": 3,
          "what_is_wrong": "Brad Lidge is a baseball player providing an endorsement, not the author. Evidence shows his sports credentials, not authorship.",
          "should_be": {
            "source": "Brad Lidge",
            "relationship": "endorsed",
            "target": "Soil Stewardship Handbook"
          }
        },
        {
          "source": "Aaron Perry and his Y on Earth network",
          "relationship": "Authorship",
          "target": "an informative handbook on soil stewardship",
          "evidence_text": "\"Aaron Perry and his Y on Earth network have produced an informative handbook on soil stewardship that invites us to re-orient our lifestyle toward ecological justice, in simple but profound ways.\"",
          "page": 5,
          "what_is_wrong": "This is Mark Bosco, S.J. praising Aaron Perry's work. The relationship should be (Mark Bosco, praised, Aaron Perry's handbook) not (Aaron Perry, authored, handbook).",
          "should_be": {
            "source": "Mark Bosco, S.J.",
            "relationship": "praised",
            "target": "Aaron Perry's Soil Stewardship Handbook"
          }
        }
      ]
    },
    {
      "category_name": "Unresolved Pronouns (Source)",
      "severity": "HIGH",
      "count": 8,
      "percentage": 0.87,
      "description": "Pronouns like 'we', 'my people', 'our' used as source entities instead of resolving to actual entity names. These are flagged with PRONOUN_UNRESOLVED_SOURCE but not corrected.",
      "root_cause_hypothesis": "Pronoun resolution module (Pass 2.5) successfully flags pronouns but fails to resolve them to actual entities. Likely missing coreference resolution logic or insufficient context window for antecedent lookup.",
      "affected_module": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "we",
          "relationship": "have",
          "target": "a deep tradition of family farming",
          "evidence_text": "we have a deep tradition of family farming.",
          "page": 10,
          "what_is_wrong": "'we' is a pronoun that should resolve to 'Slovenians' or 'Aaron Perry's people' based on earlier context about Slovenia.",
          "should_be": {
            "source": "Slovenians",
            "relationship": "have",
            "target": "a deep tradition of family farming"
          }
        },
        {
          "source": "we",
          "relationship": "can",
          "target": "thrive and to heal",
          "evidence_text": "We have the choice to thrive and to heal.",
          "page": 10,
          "what_is_wrong": "'we' should resolve to 'humanity' or 'humans' based on context about human choices.",
          "should_be": {
            "source": "humanity",
            "relationship": "can",
            "target": "thrive and to heal"
          }
        },
        {
          "source": "we",
          "relationship": "can connect with",
          "target": "the living soil",
          "evidence_text": "by connecting with the living soil.",
          "page": 10,
          "what_is_wrong": "'we' should resolve to 'humanity' or 'people' based on context.",
          "should_be": {
            "source": "humanity",
            "relationship": "can connect with",
            "target": "the living soil"
          }
        }
      ]
    },
    {
      "category_name": "Possessive Pronouns (Source)",
      "severity": "MEDIUM",
      "count": 10,
      "percentage": 1.08,
      "description": "Possessive pronouns like 'my people', 'our connection' used as source entities. While context may be clear, these should resolve to specific entities for KG utility.",
      "root_cause_hypothesis": "Pronoun resolver doesn't handle possessive pronouns ('my', 'our', 'their'). Needs expansion to detect and resolve possessive patterns.",
      "affected_module": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "my people",
          "relationship": "Cultural Identity",
          "target": "Slovenia",
          "evidence_text": "My people have lived for centuries in what is today known as Slovenia.",
          "page": 5,
          "what_is_wrong": "'my people' should resolve to 'Slovenians' or 'Aaron Perry's ancestors' based on context.",
          "should_be": {
            "source": "Slovenians",
            "relationship": "Cultural Identity",
            "target": "Slovenia"
          }
        },
        {
          "source": "my people",
          "relationship": "Connection",
          "target": "the soil",
          "evidence_text": "It is, perhaps above all else, our connection with the soil that has preserved our countryside.",
          "page": 5,
          "what_is_wrong": "'my people' should resolve to 'Slovenians' based on earlier context.",
          "should_be": {
            "source": "Slovenians",
            "relationship": "Connection",
            "target": "the soil"
          }
        },
        {
          "source": "My people",
          "relationship": "love",
          "target": "the land",
          "evidence_text": "My people love the land.",
          "page": 10,
          "what_is_wrong": "'My people' should resolve to 'Slovenians' based on context.",
          "should_be": {
            "source": "Slovenians",
            "relationship": "love",
            "target": "the land"
          }
        }
      ]
    },
    {
      "category_name": "Vague Targets",
      "severity": "MEDIUM",
      "count": 8,
      "percentage": 0.87,
      "description": "Abstract or vague target entities like 'this wonderful place', 'the answer', 'the way' that lack specificity. Some are flagged with VAGUE_TARGET but not corrected.",
      "root_cause_hypothesis": "Vague entity detector (Pass 2.5) flags some cases but doesn't replace them with specific entities. Needs enhancement to use context enrichment for vague targets.",
      "affected_module": "modules/pass2_5_postprocessing/vague_entity_detector.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "all of our lives",
          "relationship": "depend on",
          "target": "this wonderful place",
          "evidence_text": "upon which all of our lives depend.",
          "page": 10,
          "what_is_wrong": "'this wonderful place' is vague - should be 'Earth' or 'the planet' based on context.",
          "should_be": {
            "source": "all of our lives",
            "relationship": "depend on",
            "target": "Earth"
          }
        },
        {
          "source": "Y on Earth",
          "relationship": "claims",
          "target": "Soil is the answer",
          "evidence_text": "\"Soil is the answer. We are asking many questions on this journey together.\"",
          "page": 11,
          "what_is_wrong": "'Soil is the answer' is a slogan/motto, not a factual claim. Should be (Y on Earth, has_motto, 'Soil is the answer').",
          "should_be": {
            "source": "Y on Earth",
            "relationship": "has_motto",
            "target": "Soil is the answer"
          }
        }
      ]
    },
    {
      "category_name": "Philosophical/Abstract Statements",
      "severity": "MEDIUM",
      "count": 6,
      "percentage": 0.65,
      "description": "Overly abstract or philosophical relationships that don't convey concrete information. Examples: 'being connected to land is what it means to be human', 'spiritual flourishing depends on how we take care of earth'.",
      "root_cause_hypothesis": "Pass 1 extraction doesn't distinguish between factual claims and philosophical statements. Needs a filter in Pass 2 to downweight abstract/philosophical language patterns.",
      "affected_module": "modules/pass2_evaluation/dual_signal_evaluator.py",
      "affected_prompt": "prompts/pass2_evaluation_prompt.txt",
      "affected_config": null,
      "examples": [
        {
          "source": "spiritual flourishing",
          "relationship": "depends_on",
          "target": "how we take care of the earth",
          "evidence_text": "Our spiritual flourishing is wedded to how we take care of the earth and all who inhabit it.",
          "page": 3,
          "what_is_wrong": "This is a philosophical/spiritual claim, not a factual relationship. Too abstract for a knowledge graph focused on concrete information.",
          "should_be": null
        },
        {
          "source": "being connected to land and soil",
          "relationship": "is what it means to be",
          "target": "human",
          "evidence_text": "being connected to land and soil is what it means to be human.",
          "page": 10,
          "what_is_wrong": "This is a philosophical definition, not a factual relationship. Too abstract and subjective.",
          "should_be": null
        }
      ]
    },
    {
      "category_name": "Incomplete List Splitting",
      "severity": "MEDIUM",
      "count": 4,
      "percentage": 0.43,
      "description": "List splitting module partially splits comma-separated targets but leaves some combined. Example: 'communities and planet' should be two separate relationships.",
      "root_cause_hypothesis": "List splitter (Pass 2.5) uses simple comma detection but misses 'and' conjunctions. Needs enhancement to handle 'A, B and C' patterns.",
      "affected_module": "modules/pass2_5_postprocessing/list_splitter.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "soil-building practices",
          "relationship": "positively influences",
          "target": "communities and planet",
          "evidence_text": "Learning about the power of soil and engaging in soil-building practices is an exciting way to positively influence your families, communities and planet.",
          "page": 3,
          "what_is_wrong": "'communities and planet' should be split into two separate relationships: (soil-building practices, positively influences, communities) and (soil-building practices, positively influences, planet).",
          "should_be": {
            "source": "soil-building practices",
            "relationship": "positively influences",
            "target": "communities"
          }
        }
      ]
    },
    {
      "category_name": "Wrong Predicate Semantics",
      "severity": "MEDIUM",
      "count": 2,
      "percentage": 0.22,
      "description": "Predicates that don't semantically match the relationship. Example: 'Soil Stewardship Handbook helps heal the planet' - books don't heal, they guide/inform.",
      "root_cause_hypothesis": "Pass 1 extraction uses verbs from text literally without semantic validation. Needs predicate normalization in Pass 2.5 to map to standard predicates.",
      "affected_module": "modules/pass2_5_postprocessing/predicate_normalizer.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "Soil Stewardship Handbook",
          "relationship": "helps heal",
          "target": "the planet",
          "evidence_text": "help heal the planet\u2014wherever we are!",
          "page": 3,
          "what_is_wrong": "Books don't 'heal' - they guide, inform, or educate. Should be 'guides readers to heal' or 'provides information for healing'.",
          "should_be": {
            "source": "Soil Stewardship Handbook",
            "relationship": "guides readers to heal",
            "target": "the planet"
          }
        }
      ]
    },
    {
      "category_name": "Figurative Language Treated as Factual",
      "severity": "MILD",
      "count": 3,
      "percentage": 0.32,
      "description": "Metaphors and figurative language extracted as factual relationships. Example: 'spiritual flourishing is wedded to' - 'wedded' is metaphorical, flagged but not normalized.",
      "root_cause_hypothesis": "Figurative language detector (Pass 2.5) flags metaphors but doesn't normalize them. Needs enhancement to replace metaphorical predicates with literal equivalents.",
      "affected_module": "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "spiritual flourishing",
          "relationship": "depends_on",
          "target": "how we take care of the earth",
          "evidence_text": "Our spiritual flourishing is wedded to how we take care of the earth and all who inhabit it.",
          "page": 3,
          "what_is_wrong": "'is wedded to' is metaphorical. System flagged it and normalized to 'depends_on', which is good, but the entire relationship is too abstract.",
          "should_be": null
        }
      ]
    },
    {
      "category_name": "Context Enrichment Failures",
      "severity": "MILD",
      "count": 5,
      "percentage": 0.54,
      "description": "Demonstrative pronouns ('this book', 'this handbook') flagged with CONTEXT_ENRICHED_SOURCE/TARGET but enrichment is incomplete or incorrect.",
      "root_cause_hypothesis": "Context enricher (Pass 2.5) successfully detects demonstratives but sometimes fails to find the correct antecedent or uses generic enrichment.",
      "affected_module": "modules/pass2_5_postprocessing/context_enricher.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "Aaron William Perry",
          "relationship": "Authorship",
          "target": "Soil Stewardship Handbook",
          "evidence_text": "This book is dedicated to my two children, Osha and Hunter, whose brilliance, courage, determination and compassion give me great hope for the future.",
          "page": 5,
          "what_is_wrong": "System flagged CONTEXT_ENRICHED_TARGET and changed 'this book' to 'Soil Stewardship Handbook', which is correct. However, the relationship type 'Authorship' is wrong - this is a dedication, not authorship claim. Should be (Aaron William Perry, dedicated, Soil Stewardship Handbook to his children).",
          "should_be": {
            "source": "Aaron William Perry",
            "relationship": "dedicated",
            "target": "Soil Stewardship Handbook to his children"
          }
        }
      ]
    },
    {
      "category_name": "Generic Pronoun Resolution (Acceptable)",
      "severity": "MILD",
      "count": 10,
      "percentage": 1.08,
      "description": "Generic pronouns like 'you', 'we' resolved to generic entities like 'readers', 'humanity'. Flagged with GENERIC_PRONOUN_RESOLVED_SOURCE. These are acceptable but could be more specific.",
      "root_cause_hypothesis": "Pronoun resolver uses fallback to generic entities when specific antecedent not found. This is acceptable behavior but could be improved with better context analysis.",
      "affected_module": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "affected_prompt": null,
      "affected_config": null,
      "examples": [
        {
          "source": "humanity",
          "relationship": "can cultivate",
          "target": "a deep awareness of the awesome miracle that is life on Earth",
          "evidence_text": "by cultivating a deep awareness of the awesome miracle that is life on Earth.",
          "page": 10,
          "what_is_wrong": "System resolved 'we' to 'humanity', which is acceptable. However, 'awesome miracle' is figurative language that should be flagged.",
          "should_be": null
        }
      ]
    }
  ],
  "novel_error_patterns": [
    {
      "pattern_name": "Dedication Misattributed as Authorship",
      "severity": "MEDIUM",
      "count": 1,
      "description": "Book dedication statements ('This book is dedicated to...') misinterpreted as authorship claims. The system correctly enriches 'this book' to the title but incorrectly assigns 'Authorship' relationship.",
      "root_cause_hypothesis": "Bibliographic parser doesn't distinguish between dedication statements and authorship claims. Needs pattern matching for 'dedicated to' phrases.",
      "affected_module": "modules/pass2_5_postprocessing/bibliographic_parser.py",
      "examples": [
        {
          "source": "Aaron William Perry",
          "relationship": "Authorship",
          "target": "Soil Stewardship Handbook",
          "evidence_text": "This book is dedicated to my two children, Osha and Hunter, whose brilliance, courage, determination and compassion give me great hope for the future.",
          "page": 5,
          "what_is_wrong": "This is a dedication statement, not an authorship claim. Should be (Aaron William Perry, dedicated, Soil Stewardship Handbook to Osha and Hunter).",
          "should_be": {
            "source": "Aaron William Perry",
            "relationship": "dedicated",
            "target": "Soil Stewardship Handbook to his children Osha and Hunter"
          }
        }
      ]
    },
    {
      "pattern_name": "Motto/Slogan Misattributed as Factual Claim",
      "severity": "MILD",
      "count": 1,
      "description": "Organizational mottos or slogans ('Soil is the answer') extracted as factual claims instead of identifying them as mottos/taglines.",
      "root_cause_hypothesis": "Pass 1 extraction doesn't recognize motto/slogan patterns. Needs enhancement to detect and classify these as organizational attributes rather than factual claims.",
      "affected_module": "modules/pass1_extraction/entity_relationship_extractor.py",
      "examples": [
        {
          "source": "Y on Earth",
          "relationship": "claims",
          "target": "Soil is the answer",
          "evidence_text": "\"Soil is the answer. We are asking many questions on this journey together.\"",
          "page": 11,
          "what_is_wrong": "This is Y on Earth's motto/tagline, not a factual claim. Should be (Y on Earth, has_motto, 'Soil is the answer').",
          "should_be": {
            "source": "Y on Earth",
            "relationship": "has_motto",
            "target": "Soil is the answer"
          }
        }
      ]
    },
    {
      "pattern_name": "Metaphorical Book Descriptions",
      "severity": "MILD",
      "count": 3,
      "description": "Books described metaphorically ('road-map', 'compass', 'guide') extracted as literal relationships. While flagged, these could be normalized to standard predicates.",
      "root_cause_hypothesis": "Figurative language detector flags metaphors but doesn't normalize them to standard predicates. Needs mapping from metaphorical to literal predicates.",
      "affected_module": "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "examples": [
        {
          "source": "Soil Stewardship Handbook",
          "relationship": "is a",
          "target": "road-map of sorts",
          "evidence_text": "The book before you is a road-map of sorts.",
          "page": 10,
          "what_is_wrong": "'road-map' is metaphorical. Should normalize to 'guide' or 'provides guidance'.",
          "should_be": {
            "source": "Soil Stewardship Handbook",
            "relationship": "provides guidance for",
            "target": "soil stewardship practices"
          }
        }
      ]
    }
  ],
  "improvement_recommendations": [
    {
      "priority": "CRITICAL",
      "type": "NEW_MODULE",
      "target_file": "modules/pass2_5_postprocessing/praise_quote_detector.py",
      "recommendation": "Create new module to detect praise quotes in front matter. Pattern match: person name + credentials + quote about book \u2192 classify as endorsement, not authorship. Should run before bibliographic parser.",
      "expected_impact": "Eliminates all 4 reversed authorship errors (0.43% of total). Critical for factual accuracy."
    },
    {
      "priority": "CRITICAL",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "recommendation": "Enhance pronoun resolver to handle possessive pronouns ('my', 'our', 'their'). Add coreference resolution with larger context window (3-5 sentences) to find antecedents. Use spaCy's neuralcoref or similar.",
      "expected_impact": "Fixes 18 pronoun issues (8 unresolved + 10 possessive = 1.95% of total). High impact on KG utility."
    },
    {
      "priority": "HIGH",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/vague_entity_detector.py",
      "recommendation": "Enhance vague entity detector to not just flag but replace vague targets with specific entities using context. Add patterns for 'this wonderful place' \u2192 'Earth', 'the answer' \u2192 specific concept, etc.",
      "expected_impact": "Fixes 8 vague target issues (0.87% of total). Improves specificity and KG utility."
    },
    {
      "priority": "HIGH",
      "type": "PROMPT_ENHANCEMENT",
      "target_file": "prompts/pass2_evaluation_prompt.txt",
      "recommendation": "Add instruction to Pass 2 evaluator: 'Downweight abstract/philosophical statements that don't convey concrete factual information. Examples: definitions of humanity, spiritual claims, metaphysical statements. Focus on extracting concrete, verifiable relationships.'",
      "expected_impact": "Filters out 6 philosophical statements (0.65% of total). Improves factual focus."
    },
    {
      "priority": "HIGH",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/list_splitter.py",
      "recommendation": "Enhance list splitter to handle 'A, B and C' patterns. Current regex only handles commas. Add pattern: r'(\\w+)\\s*,\\s*(\\w+)\\s+and\\s+(\\w+)' to split all three items.",
      "expected_impact": "Fixes 4 incomplete list splits (0.43% of total). Improves relationship granularity."
    },
    {
      "priority": "MEDIUM",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/predicate_normalizer.py",
      "recommendation": "Enhance predicate normalizer with semantic validation. Map predicates to entity type constraints: Book \u2192 {guides, informs, describes}, not {heals, cures}. Use predicate ontology.",
      "expected_impact": "Fixes 2 wrong predicate errors (0.22% of total). Improves semantic accuracy."
    },
    {
      "priority": "MEDIUM",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/bibliographic_parser.py",
      "recommendation": "Add pattern matching for dedication statements: 'dedicated to', 'in memory of', 'for my'. Extract as (Author, dedicated, Book to Recipients) not (Author, authored, Book).",
      "expected_impact": "Fixes 1 dedication misattribution (0.11% of total). Improves relationship accuracy."
    },
    {
      "priority": "MEDIUM",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "recommendation": "Enhance figurative language detector to normalize metaphorical predicates. Map: 'is a road-map' \u2192 'provides guidance', 'is a compass' \u2192 'provides direction', 'is wedded to' \u2192 'depends on'.",
      "expected_impact": "Normalizes 3 metaphorical descriptions (0.32% of total). Improves literal clarity."
    },
    {
      "priority": "LOW",
      "type": "PROMPT_ENHANCEMENT",
      "target_file": "prompts/pass1_extraction_prompt.txt",
      "recommendation": "Add instruction to Pass 1: 'Identify organizational mottos, slogans, and taglines. Extract as (Organization, has_motto, Motto) not (Organization, claims, Motto).'",
      "expected_impact": "Fixes 1 motto misattribution (0.11% of total). Minor improvement."
    },
    {
      "priority": "LOW",
      "type": "CODE_FIX",
      "target_file": "modules/pass2_5_postprocessing/context_enricher.py",
      "recommendation": "Enhance context enricher to validate enrichment against relationship type. If enriching 'this book' in dedication context, ensure relationship is 'dedicated' not 'authored'.",
      "expected_impact": "Improves 5 context enrichment cases (0.54% of total). Minor quality improvement."
    }
  ],
  "system_health": {
    "meets_production_criteria": true,
    "target_quality_threshold": 0.05,
    "current_quality_issue_rate": 0.0671,
    "note": "V7 is close to production quality (5% threshold). Main blockers: 4 critical reversed authorship errors and 18 high-priority pronoun issues. With recommended fixes, estimated issue rate would drop to ~3.5%, well below threshold."
  }
}