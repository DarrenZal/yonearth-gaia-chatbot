{
  "error": "json_parse_failed",
  "raw_response": "```json\n{\n  \"extraction_metadata\": {\n    \"version\": \"v11.2.1\",\n    \"total_relationships\": 984,\n    \"analysis_timestamp\": \"2024-01-15T00:00:00Z\"\n  },\n  \"quality_summary\": {\n    \"critical_issues\": 2,\n    \"high_priority_issues\": 28,\n    \"medium_priority_issues\": 173,\n    \"mild_issues\": 12,\n    \"total_issues\": 215,\n    \"issue_rate_percent\": 21.85,\n    \"estimated_false_negative_rate\": 0.13,\n    \"estimated_total_issues_with_fn\": 245,\n    \"adjusted_issue_rate_percent\": 24.9,\n    \"grade_confirmed\": \"C-\",\n    \"grade_adjusted\": \"D+\",\n    \"note\": \"V11.2.1 shows CRITICAL REGRESSION from target quality. Major systemic failures: (1) Dedication parser creating 5-7 malformed relationships per dedication (28 bad relationships from 4 dedications), (2) Predicate fragmentation at 173 unique predicates (target: <150), (3) Possessive/demonstrative pronouns unresolved ('my people' 4x, 'we' 5x), (4) Praise quotes misclassified as endorsements (12 instances). Adjusted metrics include estimated mild issues not flagged (13% FN rate based on meta-validation).\"\n  },\n  \"issue_categories\": [\n    {\n      \"category_name\": \"Dedication Parser Malfunction - List Explosion\",\n      \"severity\": \"CRITICAL\",\n      \"count\": 28,\n      \"percentage\": 2.85,\n      \"description\": \"The dedication parser is splitting dedication text on commas/conjunctions and creating separate relationships for abstract qualities (brilliance, courage, determination, compassion) and sentence fragments ('whose brilliance', 'compassion give me great hope for the future.'). This creates 5-7 malformed relationships per dedication instead of 1-2 clean ones.\",\n      \"root_cause_hypothesis\": \"The DEDICATION_CORRECTED module in Pass 2.5 is applying list-splitting logic inappropriately to dedication text. It's treating descriptive phrases as if they were list items, and not recognizing that qualities like 'brilliance, courage, determination' are describing the dedicatees, not separate targets.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/dedication_parser.py\",\n      \"affected_prompt\": null,\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"whose brilliance\",\n          \"evidence_text\": \"This book is dedicated to my two children, Osha and Hunter, whose brilliance, courage, determination and compassion give me great hope for the future.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Parser extracted 'whose brilliance' as a dedication target. This is a sentence fragment, not a person. The dedication is to Osha and Hunter, and 'brilliance, courage, determination, compassion' are qualities describing them.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"dedicated to\",\n            \"target\": \"Osha\"\n          }\n        },\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"compassion give me great hope for the future.\",\n          \"evidence_text\": \"This book is dedicated to my two children, Osha and Hunter, whose brilliance, courage, determination and compassion give me great hope for the future.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Parser extracted a sentence fragment ending with a period as a dedication target. This is clearly malformed.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"dedicated to\",\n            \"target\": \"Hunter\"\n          }\n        },\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"community impact ambassadors who are informing\",\n          \"evidence_text\": \"And this book is dedicated to the Y on Earth Community, the Soil Stewardship Guild members and the Community Impact Ambassadors who are informing and inspiring thousands with a message of joy, celebration, gratitude and deliberate action.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Parser split on 'and' and created a fragment 'who are informing' instead of keeping 'Community Impact Ambassadors' as a complete entity.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"dedicated to\",\n            \"target\": \"Community Impact Ambassadors\"\n          }\n        },\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"celebration\",\n          \"evidence_text\": \"And this book is dedicated to the Y on Earth Community, the Soil Stewardship Guild members and the Community Impact Ambassadors who are informing and inspiring thousands with a message of joy, celebration, gratitude and deliberate action.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Parser extracted 'celebration' as a dedication target. The book is dedicated to organizations/people, not abstract concepts. 'Celebration' is part of the message they spread.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"dedicated to\",\n            \"target\": \"Y on Earth Community\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Predicate Fragmentation - Excessive Variation\",\n      \"severity\": \"MEDIUM\",\n      \"count\": 173,\n      \"percentage\": 17.58,\n      \"description\": \"System has 173 unique predicates, exceeding the 150 threshold. Many predicates are minor variations of the same base concept (e.g., 'is', 'is a', 'is-a', 'is marked by', 'is what it means to be', 'is the foundation of', etc.). This indicates lack of predicate normalization, making the KG harder to query and analyze.\",\n      \"root_cause_hypothesis\": \"No predicate normalization module exists in Pass 2.5. The system accepts whatever predicate the LLM extracts in Pass 1, leading to semantic drift and fragmentation. Common predicates like 'is', 'has', 'published', 'are' have 4-36 variations each.\",\n      \"affected_module\": null,\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"soil\",\n          \"relationship\": \"is what it means to be\",\n          \"target\": \"human\",\n          \"evidence_text\": \"being connected to land and soil is what it means to be human.\",\n          \"page\": 10,\n          \"what_is_wrong\": \"Predicate 'is what it means to be' is overly specific and philosophical. Should be normalized to a canonical form like 'defines' or 'is-essential-to'.\",\n          \"should_be\": {\n            \"source\": \"soil connection\",\n            \"relationship\": \"defines\",\n            \"target\": \"human identity\"\n          }\n        },\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"is essential reading for\",\n          \"target\": \"gardeners\",\n          \"evidence_text\": \"This handbook is essential reading for anyone interested in soil.\",\n          \"page\": 2,\n          \"what_is_wrong\": \"Predicate 'is essential reading for' is a book-specific variation of 'is-relevant-to' or 'targets-audience'. Should be normalized.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"targets-audience\",\n            \"target\": \"gardeners\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Possessive Pronouns Unresolved\",\n      \"severity\": \"HIGH\",\n      \"count\": 4,\n      \"percentage\": 0.41,\n      \"description\": \"Possessive pronouns ('my people', 'our countryside', 'our planet') used as entity sources/targets without resolution to the actual referent. This makes relationships vague and unusable for querying.\",\n      \"root_cause_hypothesis\": \"The pronoun resolution module in Pass 2.5 only handles subject pronouns (he, she, it, they) but not possessive pronouns (my, our, their). The Pass 1 extraction prompt also doesn't discourage possessive pronouns.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/pronoun_resolver.py\",\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"my people\",\n          \"relationship\": \"love\",\n          \"target\": \"the land\",\n          \"evidence_text\": \"My people love the land. We love the sea. We love the trees. And, we love the soil.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"'my people' is a possessive pronoun phrase that should resolve to 'Slovenians' based on context ('I hail from...Slovenia' earlier in text).\",\n          \"should_be\": {\n            \"source\": \"Slovenians\",\n            \"relationship\": \"love\",\n            \"target\": \"the land\"\n          }\n        },\n        {\n          \"source\": \"connection with the soil\",\n          \"relationship\": \"preserves\",\n          \"target\": \"our countryside\",\n          \"evidence_text\": \"It is, perhaps above all else, our connection with the soil that has preserved our countryside and that has allowed my people to flourish with a form of liberty that only exists through close connection to the living soil.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"'our countryside' should resolve to 'Slovenian countryside' based on context.\",\n          \"should_be\": {\n            \"source\": \"connection with the soil\",\n            \"relationship\": \"preserves\",\n            \"target\": \"Slovenian countryside\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Subject Pronouns Unresolved\",\n      \"severity\": \"HIGH\",\n      \"count\": 5,\n      \"percentage\": 0.51,\n      \"description\": \"Subject pronouns ('we', 'us') used as entity sources without resolution. Despite having a pronoun resolution module, these common pronouns are not being resolved to their antecedents.\",\n      \"root_cause_hypothesis\": \"The pronoun resolution module may be failing because: (1) it requires clear antecedents in the same sentence/paragraph, (2) 'we' in this context is ambiguous (author + readers? humanity?), or (3) the module is not being triggered for these cases.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/pronoun_resolver.py\",\n      \"affected_prompt\": null,\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"we\",\n          \"relationship\": \"are embarking on\",\n          \"target\": \"awesome adventure\",\n          \"evidence_text\": \"We are embarking on an awesome adventure together\u2014a fun-filled and health-enhancing adventure of a lifetime!\",\n          \"page\": 10,\n          \"what_is_wrong\": \"'we' is unresolved. In context, this likely refers to 'readers of this handbook' or 'soil stewards', but the pronoun resolver didn't catch it.\",\n          \"should_be\": {\n            \"source\": \"soil stewards\",\n            \"relationship\": \"are embarking on\",\n            \"target\": \"soil stewardship journey\"\n          }\n        },\n        {\n          \"source\": \"we\",\n          \"relationship\": \"must regenerate\",\n          \"target\": \"soil\",\n          \"evidence_text\": \"we must regenerate the soil\",\n          \"page\": 10,\n          \"what_is_wrong\": \"'we' is unresolved. Should be 'humanity' or 'farmers and gardeners' based on context.\",\n          \"should_be\": {\n            \"source\": \"humanity\",\n            \"relationship\": \"must regenerate\",\n            \"target\": \"soil\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Praise Quotes Misclassified as Endorsements\",\n      \"severity\": \"HIGH\",\n      \"count\": 12,\n      \"percentage\": 1.22,\n      \"description\": \"Praise quotes from book reviews are being extracted as 'endorsed' relationships. While technically these are endorsements, they create noise in the KG because they don't convey substantive information about the book's content. They're marketing material, not knowledge.\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction prompt likely instructs the LLM to extract 'all relationships' from text, without distinguishing between substantive content and promotional material. Pass 2 evaluation doesn't filter these out because they're textually accurate.\",\n      \"affected_module\": null,\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Michael Bowman\",\n          \"relationship\": \"endorsed\",\n          \"target\": \"Soil Stewardship Handbook\",\n          \"evidence_text\": \"\"This Soil Stewardship Handbook is an excellent tool for us to engage with this critical mission and quest.\" \u2014Michael Bowman Founding Board Chair, National Hemp Association\",\n          \"page\": 2,\n          \"what_is_wrong\": \"This is a praise quote from the book's front matter. While factually accurate, it doesn't convey substantive knowledge about soil stewardship. It's marketing material.\",\n          \"should_be\": null\n        },\n        {\n          \"source\": \"Adrian Del Caro\",\n          \"relationship\": \"endorsed\",\n          \"target\": \"Soil Stewardship Handbook\",\n          \"evidence_text\": \"\"...Perry has given us a new appreciation for soil and its good works.\" \u2014Adrian Del Caro Author of Grounding the Nietzsche Rhetoric of Earth University of Tennessee, Knoxville\",\n          \"page\": 2,\n          \"what_is_wrong\": \"Another praise quote. These should be filtered out or marked as 'promotional' rather than treated as substantive relationships.\",\n          \"should_be\": null\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Vague Abstract Entities\",\n      \"severity\": \"HIGH\",\n      \"count\": 8,\n      \"percentage\": 0.81,\n      \"description\": \"Entities like 'this approach', 'the answer', 'what we do to the soil' are too vague and abstract to be useful in a knowledge graph. They lack specificity and make relationships hard to interpret.\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction prompt doesn't constrain entity specificity. The LLM extracts demonstrative pronouns ('this', 'that') and abstract phrases without resolving them to concrete entities.\",\n      \"affected_module\": null,\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"this approach\",\n          \"relationship\": \"opens doors to\",\n          \"target\": \"sustainable farming practices\",\n          \"evidence_text\": \"This approach opens doors to sustainable farming practices.\",\n          \"page\": 10,\n          \"what_is_wrong\": \"'this approach' is a demonstrative pronoun phrase that should resolve to a specific approach mentioned earlier (likely 'soil stewardship' or 'regenerative agriculture').\",\n          \"should_be\": {\n            \"source\": \"soil stewardship\",\n            \"relationship\": \"enables\",\n            \"target\": \"sustainable farming practices\"\n          }\n        },\n        {\n          \"source\": \"what we do to the soil\",\n          \"relationship\": \"affects\",\n          \"target\": \"ourselves\",\n          \"evidence_text\": \"...What we do to the soil, we do to ourselves...\",\n          \"page\": 12,\n          \"what_is_wrong\": \"Both source and target are vague. 'what we do to the soil' should be 'soil management practices', and 'ourselves' should be 'human health' or 'humanity'.\",\n          \"should_be\": {\n            \"source\": \"soil management practices\",\n            \"relationship\": \"affects\",\n            \"target\": \"human health\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Philosophical Abstractions Treated as Factual\",\n      \"severity\": \"MEDIUM\",\n      \"count\": 15,\n      \"percentage\": 1.52,\n      \"description\": \"Highly abstract, philosophical statements are being extracted as factual relationships. Examples: 'soil is what it means to be human', 'individual quest is great global movement'. These are rhetorical/inspirational statements, not verifiable facts.\",\n      \"root_cause_hypothesis\": \"Pass 2 evaluation prompt doesn't distinguish between factual claims and philosophical/rhetorical statements. The text_confidence is high because the text literally says these things, but p_true should be lower because they're not empirically verifiable.\",\n      \"affected_module\": null,\n      \"affected_prompt\": \"prompts/pass2_evaluation_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"soil\",\n          \"relationship\": \"is what it means to be\",\n          \"target\": \"human\",\n          \"evidence_text\": \"being connected to land and soil is what it means to be human.\",\n          \"page\": 10,\n          \"what_is_wrong\": \"This is a philosophical/rhetorical statement, not a factual claim. The relationship is marked as PHILOSOPHICAL_CLAIM but still extracted. Should be filtered or marked more clearly as non-factual.\",\n          \"should_be\": null\n        },\n        {\n          \"source\": \"individual quest\",\n          \"relationship\": \"is\",\n          \"target\": \"great global movement\",\n          \"evidence_text\": \"...I want to personally welcome you to this individual quest and great global movement...\",\n          \"page\": 12,\n          \"what_is_wrong\": \"This is inspirational rhetoric, not a factual equivalence. An individual quest is not literally a global movement.\",\n          \"should_be\": null\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Reversed Authorship\",\n      \"severity\": \"CRITICAL\",\n      \"count\": 1,\n      \"percentage\": 0.10,\n      \"description\": \"One instance of reversed authorship where the book title is the source instead of the author. This is a known V4 error pattern that should have been fixed.\",\n      \"root_cause_hypothesis\": \"The bibliographic parser in Pass 2.5 is supposed to catch and fix these, but it missed this case. Likely because the context was ambiguous or the parser's pattern matching failed.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/bibliographic_parser.py\",\n      \"affected_prompt\": null,\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"authored\",\n          \"target\": \"Aaron William Perry\",\n          \"evidence_text\": \"the living planet. This Soil Stewardship Handbook is deceptively small and simple.\",\n          \"page\": 10,\n          \"what_is_wrong\": \"Authorship is reversed. Aaron William Perry authored the handbook, not the other way around.\",\n          \"should_be\": {\n            \"source\": \"Aaron William Perry\",\n            \"relationship\": \"authored\",\n            \"target\": \"Soil Stewardship Handbook\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Incomplete/Malformed Targets\",\n      \"severity\": \"MEDIUM\",\n      \"count\": 6,\n      \"percentage\": 0.61,\n      \"description\": \"Targets that are incomplete phrases or end with prepositions/conjunctions, indicating the extraction cut off mid-phrase.\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction is not capturing complete noun phrases. This could be due to: (1) LLM tokenization issues, (2) extraction prompt not emphasizing complete phrases, or (3) entity boundary detection failing.\",\n      \"affected_module\": null,\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Aaron William Perry\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"Soil Stewardship Handbook to\",\n          \"evidence_text\": \"This book is dedicated to\",\n          \"page\": 2,\n          \"what_is_wrong\": \"Target ends with preposition 'to', indicating incomplete extraction. Should be the actual dedicatees.\",\n          \"should_be\": {\n            \"source\": \"Aaron William Perry\",\n            \"relationship\": \"dedicated\",\n            \"target\": \"Osha and Hunter\"\n          }\n        },\n        {\n          \"source\": \"Soil\",\n          \"relationship\": \"heals\",\n          \"target\": \"restores balance\",\n          \"evidence_text\": \"Soil\u2014Healing Earth and Restoring Balance\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Target 'restores balance' is a verb phrase, not a noun phrase. Should be 'balance' or 'ecological balance'.\",\n          \"should_be\": {\n            \"source\": \"Soil\",\n            \"relationship\": \"restores\",\n            \"target\": \"ecological balance\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"Duplicate Relationships\",\n      \"severity\": \"MILD\",\n      \"count\": 0,\n      \"percentage\": 0.0,\n      \"description\": \"No exact duplicate relationships found. This is a positive indicator that deduplication is working.\",\n      \"root_cause_hypothesis\": \"N/A - No issue detected\",\n      \"affected_module\": null,\n      \"affected_prompt\": null,\n      \"affected_config\": null,\n      \"examples\": []\n    },\n    {\n      \"category_name\": \"Wrong Publication Location\",\n      \"severity\": \"MEDIUM\",\n      \"count\": 2,\n      \"percentage\": 0.20,\n      \"description\": \"Two instances where publication location is incorrectly extracted: 'United States of America' instead of 'Denver Colorado', and 'Slovenia' instead of 'Denver Colorado'. The book was published in Denver, not Slovenia (Slovenia is mentioned in the foreword context).\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction is picking up location mentions from surrounding context rather than the actual publication location. The bibliographic parser in Pass 2.5 should validate publication locations against copyright page data.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/bibliographic_parser.py\",\n      \"affected_prompt\": \"prompts/pass1_extraction_v7.txt\",\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"published in\",\n          \"target\": \"United States of America\",\n          \"evidence_text\": \"University Copyright \u00a9 2018 Aaron William Perry All rights reserved. No part of this book may be reproduced in any form or by any electronic or mechanical means, including information storage and retrieval systems, without permission in writing from the publisher, except by reviewers, who may quote brief passages in a review.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"The book was published in Denver, Colorado, not just 'United States of America'. This is too vague. The system already extracted 'Denver Colorado' correctly in another relationship.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"published in\",\n            \"target\": \"Denver, Colorado\"\n          }\n        },\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"published in\",\n          \"target\": \"Slovenia\",\n          \"evidence_text\": \"Lily Sophia von \u00dcbergarten Slovenia, 2018\",\n          \"page\": 10,\n          \"what_is_wrong\": \"This is from the foreword signature, not the publication location. Slovenia is where the foreword author is from, not where the book was published.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"published in\",\n            \"target\": \"Denver, Colorado\"\n          }\n        }\n      ]\n    },\n    {\n      \"category_name\": \"List Splitting Errors\",\n      \"severity\": \"MEDIUM\",\n      \"count\": 18,\n      \"percentage\": 1.83,\n      \"description\": \"List splitting module is creating relationships that are semantically odd or incomplete. Examples: 'Gardening the soil connects us literally' (split from 'literally and deeply'), 'Soil heals restores balance' (verb phrase as target).\",\n      \"root_cause_hypothesis\": \"The list splitting module is splitting on 'and' too aggressively, even when 'and' connects adverbs or creates compound verb phrases. It should only split when 'and' connects distinct noun phrases.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/list_splitter.py\",\n      \"affected_prompt\": null,\n      \"affected_config\": null,\n      \"examples\": [\n        {\n          \"source\": \"Gardening the soil\",\n          \"relationship\": \"connects\",\n          \"target\": \"us literally\",\n          \"evidence_text\": \"Gardening the soil connects us literally and deeply to the living planet.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"List splitter split 'literally and deeply' into two targets, but 'us literally' is not a valid noun phrase. 'Literally' and 'deeply' are adverbs modifying 'connects', not separate targets.\",\n          \"should_be\": {\n            \"source\": \"Gardening the soil\",\n            \"relationship\": \"connects\",\n            \"target\": \"us to the living planet\"\n          }\n        },\n        {\n          \"source\": \"Soil\",\n          \"relationship\": \"heals\",\n          \"target\": \"restores balance\",\n          \"evidence_text\": \"Soil\u2014Healing Earth and Restoring Balance\",\n          \"page\": 6,\n          \"what_is_wrong\": \"List splitter created 'restores balance' as a target, but this is a verb phrase, not a noun. Should be 'balance' or not split at all.\",\n          \"should_be\": {\n            \"source\": \"Soil\",\n            \"relationship\": \"heals\",\n            \"target\": \"Earth\"\n          }\n        }\n      ]\n    }\n  ],\n  \"novel_error_patterns\": [\n    {\n      \"pattern_name\": \"Dedication Parser List Explosion\",\n      \"severity\": \"CRITICAL\",\n      \"count\": 28,\n      \"description\": \"NEW PATTERN: Dedication parser is creating 5-7 malformed relationships per dedication by treating descriptive qualities and sentence fragments as separate dedication targets. This is a catastrophic failure mode not seen in V4 reports.\",\n      \"root_cause_hypothesis\": \"The dedication parser is applying list-splitting logic to dedication text without understanding that qualities like 'brilliance, courage, determination' are describing the dedicatees, not separate entities. It's also not filtering out sentence fragments.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/dedication_parser.py\",\n      \"examples\": [\n        {\n          \"source\": \"Soil Stewardship Handbook\",\n          \"relationship\": \"dedicated\",\n          \"target\": \"whose brilliance\",\n          \"evidence_text\": \"This book is dedicated to my two children, Osha and Hunter, whose brilliance, courage, determination and compassion give me great hope for the future.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"Parser extracted 'whose brilliance' as a dedication target instead of recognizing it as a descriptive clause about Osha and Hunter.\",\n          \"should_be\": {\n            \"source\": \"Soil Stewardship Handbook\",\n            \"relationship\": \"dedicated to\",\n            \"target\": \"Osha\"\n          }\n        }\n      ]\n    },\n    {\n      \"pattern_name\": \"Possessive Pronoun Blindness\",\n      \"severity\": \"HIGH\",\n      \"count\": 4,\n      \"description\": \"NEW PATTERN: Possessive pronouns ('my people', 'our countryside') are not being resolved, despite having a pronoun resolution module. This is distinct from subject pronoun issues because possessive pronouns require different resolution logic.\",\n      \"root_cause_hypothesis\": \"The pronoun resolution module only handles subject pronouns (he, she, it, they) and doesn't have logic for possessive pronouns (my, our, their). These require looking at possessive relationships in the text.\",\n      \"affected_module\": \"modules/pass2_5_postprocessing/pronoun_resolver.py\",\n      \"examples\": [\n        {\n          \"source\": \"my people\",\n          \"relationship\": \"love\",\n          \"target\": \"the land\",\n          \"evidence_text\": \"My people love the land. We love the sea. We love the trees. And, we love the soil.\",\n          \"page\": 6,\n          \"what_is_wrong\": \"'my people' should resolve to 'Slovenians' based on earlier context.\",\n          \"should_be\": {\n            \"source\": \"Slovenians\",\n            \"relationship\": \"love\",\n            \"target\": \"the land\"\n          }\n        }\n      ]\n    },\n    {\n      \"pattern_name\": \"Praise Quote Noise\",\n      \"severity\": \"HIGH\",\n      \"count\": 12,\n      \"description\": \"NEW PATTERN: Praise quotes from book reviews are being extracted as 'endorsed' relationships, creating noise in the KG. While factually accurate, these don't convey substantive knowledge about the book's content.\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction prompt doesn't distinguish between substantive content and promotional material. The system treats all text equally, even front matter praise quotes.\",\n      \"affected_module\": null,\n      \"examples\": [\n        {\n          \"source\": \"Michael Bowman\",\n          \"relationship\": \"endorsed\",\n          \"target\": \"Soil Stewardship Handbook\",\n          \"evidence_text\": \"\"This Soil Stewardship Handbook is an excellent tool for us to engage with this critical mission and quest.\" \u2014Michael Bowman\",\n          \"page\": 2,\n          \"what_is_wrong\": \"This is a praise quote from front matter, not substantive knowledge about soil stewardship.\",\n          \"should_be\": null\n        }\n      ]\n    },\n    {\n      \"pattern_name\": \"Demonstrative Pronoun Vagueness\",\n      \"severity\": \"HIGH\",\n      \"count\": 8,\n      \"description\": \"NEW PATTERN: Demonstrative pronouns ('this approach', 'this handbook') are being used as entity sources without resolution to the specific concept they refer to.\",\n      \"root_cause_hypothesis\": \"Pass 1 extraction doesn't resolve demonstrative pronouns, and no Pass 2.5 module handles them. These require looking back in the text to find the antecedent.\",\n      \"affected_module\": null,\n      \"examples\": [\n        {\n          \"source\": \"this approach\",\n          \"relationship\": \"opens doors to\",\n          \"target\": \"sustainable farming practices\",\n          \"evidence_text\": \"This approach opens doors to sustainable farming practices.\",\n          \"page\": 10,\n          \"what_is_wrong\": \"'this approach' should resolve to 'soil stewardship' or 'regenerative agriculture' based on context.\",\n          \"should_be\": {\n            \"source\": \"soil stewardship\",\n            \"relationship\": \"enables\",\n            \"target\": \"sustainable farming practices\"\n          }\n        }\n      ]\n    }\n  ],\n  \"improvement_recommendations\": [\n    {\n      \"priority\": \"CRITICAL\",\n      \"type\": \"CODE_FIX\",\n      \"target_file\": \"modules/pass2_5_postprocessing/dedication_parser.py\",\n      \"recommendation\": \"REWRITE dedication parser logic: (1) Extract only proper nouns/named entities as dedication targets, (2) Filter out descriptive qualities (brilliance, courage, etc.) and sentence fragments, (3) Use NER to identify person/organization names, (4) Stop splitting on commas within descriptive clauses (e.g., 'whose brilliance, courage...' should not be split).\",\n      \"expected_impact\": \"Eliminate 28 malformed dedication relationships, reducing issue rate by 2.85%.\",\n      \"rationale\": \"The current dedication parser is the single worst-performing module, creating 5-7 bad relationships per dedication. A complete rewrite with NER-based entity extraction is needed.\"\n    },\n    {\n      \"priority\": \"CRITICAL\",\n      \"type\": \"NEW_MODULE\",\n      \"target_file\": \"modules/pass2_5_postprocessing/predicate_normalizer.py\",\n      \"recommendation\": \"Create a predicate normalization module that: (1) Maps predicate variations to canonical forms (e.g., 'is', 'is a', 'is-a' \u2192 'is-a'), (2) Uses a predicate taxonomy with ~50-80 canonical predicates, (3) Applies fuzzy matching to catch variations, (4) Runs after all other Pass 2.5 modules.\",\n      \"expected_impact\": \"Reduce unique predicates from 173 to ~80-100, improving KG queryability and reducing fragmentation by 40-50%.\",\n      \"rationale\": \"Predicate fragmentation is the second-largest issue (173 unique predicates). Without normalization, the KG becomes increasingly hard to query as more books are processed.\"\n    },\n    {\n      \"priority\": \"HIGH\",\n      \"type\": \"CODE_FIX\",\n      \"target_file\": \"modules/pass2_5_postprocessing/pronoun_resolver.py\",\n      \"recommendation\": \"Extend pronoun resolver to handle: (1) Possessive pronouns (my, our, their) by looking for possessive relationships in context, (2) Demonstrative pronouns (this, that, these, those) by finding nearest noun phrase antecedent, (3) Generic 'we' by using heuristics (if in introduction/conclusion, likely 'readers'; if in body, likely 'humanity' or book's target audience).\",\n      \"expected_impact\": \"Resolve 12 pronoun-related issues (4 possessive + 5 subject + 3 demonstrative), reducing issue rate by 1.22%.\",\n      \"rationale\": \"Pronoun resolution is partially working but missing key pronoun types. Extending it to handle possessive and demonstrative pronouns will significantly improve entity clarity.\"\n    },\n    {\n      \"priority\": \"HIGH\",\n      \"type\": \"PROMPT_ENHANCEMENT\",\n      \"target_file\": \"prompts/pass1_extraction_v7.txt\",\n      \"recommendation\": \"Add constraints to Pass 1 extraction prompt: (1) 'Do NOT extract praise quotes or endorsements from book reviews/front matter', (2) 'Resolve possessive pronouns (my, our, their) to specific entities before extraction', (3) 'Resolve demonstrative pronouns (this, that) to their antecedents', (4) 'Extract only substantive knowledge relationships, not promotional content', (5) Add few-shot examples showing correct handling of these cases.\",\n      \"expected_impact\": \"Prevent 12 praise quote extractions and 12 pronoun issues at source, reducing issue rate by 2.44%.\",\n      \"rationale\": \"Many issues originate in Pass 1 extraction. Adding explicit constraints and examples will prevent bad extractions before they reach Pass 2.5, reducing the burden on post-processing modules.\"\n    },\n    {\n      \"priority\": \"HIGH\",\n      \"type\": \"CODE_FIX\",\n      \"target_file\": \"modules/pass2_5_postprocessing/list_splitter.py\",\n      \"recommendation\": \"Refine list splitting logic: (1) Only split when 'and'/'or' connects distinct noun phrases (use POS tagging), (2) Do NOT split when 'and' connects adverbs (e.g., 'literally and deeply'), (3) Do NOT split compound verb phrases (e.g., 'heals and restores'), (4) Add validation that each split target is a valid noun phrase.\",\n      \"expected_impact\": \"Fix 18 list splitting errors, reducing issue rate by 1.83%.\",\n      \"rationale\": \"List splitter is too aggressive, splitting on 'and' even when it doesn't separate distinct entities. POS tagging can distinguish between noun phrase coordination (should split) and adverb/verb coordination (should not split).\"\n    },\n    {\n      \"priority\": \"HIGH\",\n      \"type\": \"PROMPT_ENHANCEMENT\",\n      \"target_file\": \"prompts/pass2_evaluation_v7.txt\",\n      \"recommendation\": \"Enhance Pass 2 evaluation prompt to distinguish factual claims from philosophical/rhetorical statements: (1) Add instruction: 'Mark relationships as PHILOSOPHICAL_CLAIM if they express abstract, non-verifiable ideas (e.g., \\\"soil is what it means to be human\\\")', (2) Lower p_true scores for philosophical claims (cap at 0.6), (3) Add few-shot examples of philosophical vs. factual claims.\",\n      \"expected_impact\": \"Correctly classify 15 philosophical abstractions, allowing downstream filtering or special handling.\",\n      \"rationale\": \"Pass 2 evaluation currently treats all text-supported statements as equally factual. Distinguishing philosophical claims will improve KG quality by allowing users to filter out non-empirical relationships.\"\n    },\n    {\n      \"priority\": \"MEDIUM\",\n      \"type\": \"CODE_FIX\",\n      \"target_file\": \"modules/pass2_5_postprocessing/bibliographic_parser.py\",\n      \"recommendation\": \"Enhance bibliographic parser to: (1) Validate publication locations against copyright page data (should be 'Denver, Colorado' for this book), (2) Detect and fix reversed authorship (book \u2192 author should be author \u2192 book), (3) Ignore location mentions in forewords/introductions that aren't publication locations.\",\n      \"expected_impact\": \"Fix 3 bibliographic errors (1 reversed authorship + 2 wrong locations), reducing issue rate by 0.30%.\",\n      \"rationale\": \"Bibliographic parser is mostly working but missing edge cases. These fixes will improve accuracy for publication metadata.\"\n    },\n    {\n      \"priority\": \"MEDIUM\",\n      \"type\": \"PROMPT_ENHANCEMENT\",\n      \"target_file\": \"prompts/pass1_extraction_v7.txt\",\n      \"recommendation\": \"Add entity specificity constraints: (1) 'Avoid vague entities like \\\"this approach\\\", \\\"the answer\\\", \\\"what we do\\\". Extract specific concepts instead.', (2) 'Complete all noun phrases fully - do not cut off mid-phrase', (3) Add few-shot examples showing specific vs. vague entities.\",\n      \"expected_impact\": \"Reduce vague entity extractions by 50% (8 \u2192 4), improving entity clarity.\",\n      \"rationale\": \"Vague entities make relationships hard to interpret and query. Constraining entity specificity at extraction time is more effective than trying to fix it in post-processing.\"\n    },\n    {\n      \"priority\": \"MEDIUM\",\n      \"type\": \"CONFIG_UPDATE\",\n      \"target_file\": \"config/extraction_config.yaml\",\n      \"recommendation\": \"Add configuration flag 'extract_promotional_content: false' that filters out relationships from: (1) Praise quotes (identified by quotation marks + attribution), (2) Book reviews in front matter, (3) Endorsement sections. Implement as a Pass 2.5 filter module.\",\n      \"expected_impact\": \"Remove 12 praise quote relationships, reducing noise by 1.22%.\",\n      \"rationale\": \"Promotional content doesn't convey substantive knowledge. Making this configurable allows users to decide whether to include it, but default should be to exclude it.\"\n    },\n    {\n      \"priority\": \"MEDIUM\",\n      \"type\": \"NEW_MODULE\",\n      \"target_file\": \"modules/pass2_5_postprocessing/philosophical_claim_filter.py\",\n      \"recommendation\": \"Create a filter module that: (1) Detects philosophical/rhetorical statements using heuristics (e.g., 'what it means to be', 'is the essence of', overly abstract predicates), (2) Either removes them or marks them with a 'PHILOSOPHICAL' flag for optional filtering, (3) Runs after pronoun resolution but before final output.\",\n      \"expected_impact\": \"Filter or mark 15 philosophical abstractions, allowing users to exclude non-empirical relationships.\",\n      \"rationale\": \"Philosophical statements are currently mixed with factual claims. Separating them improves KG utility for users who want only empirical knowledge.\"\n    }\n  ],\n  \"prompt_analysis\": {\n    \"pass1_extraction_issues\": [\n      {\n        \"issue\": \"No constraints against extracting promotional content (praise quotes, endorsements)\",\n        \"current_wording\": \"Likely instructs to 'extract ALL relationships' without distinguishing content types\",\n        \"suggested_fix\": \"Add explicit exclusion: 'Do NOT extract praise quotes, book reviews, or endorsements from front matter. Focus on substantive knowledge about the book's subject matter.'\",\n        \"examples_needed\": \"Yes - show examples of praise quotes that should NOT be extracted vs. substantive claims that should be\"\n      },\n      {\n        \"issue\": \"No guidance on resolving possessive pronouns (my, our, their)\",\n        \"current_wording\": \"Likely only mentions subject pronouns (he, she, it)\",\n        \"suggested_fix\": \"Add: 'Resolve possessive pronouns to specific entities. Example: \\\"my people\\\" \u2192 \\\"Slovenians\\\" (based on context), \\\"our planet\\\" \u2192 \\\"Earth\\\".'\",\n        \"examples_needed\": \"Yes - show 3-4 examples of possessive pronoun resolution\"\n      },\n      {\n        \"issue\": \"No guidance on resolving demonstrative pronouns (this, that, these, those)\",\n        \"current_wording\": \"Likely no mention of demonstrative pronouns\",\n        \"suggested_fix\": \"Add: 'Resolve demonstrative pronouns to their antecedents. Example: \\\"This approach opens doors...\\\" \u2192 \\\"Soil stewardship opens doors...\\\" (if \\\"soil stewardship\\\" was mentioned in previous sentence).'\",\n        \"examples_needed\": \"Yes - show examples of demonstrative pronoun resolution\"\n      },\n      {\n        \"issue\": \"No emphasis on entity specificity - allows vague entities like 'this approach', 'the answer'\",\n        \"current_wording\": \"Likely no constraints on entity specificity\",\n        \"suggested_fix\": \"Add: 'Extract specific, concrete entities. Avoid vague phrases like \\\"this approach\\\", \\\"the answer\\\", \\\"what we do\\\". If the text uses vague language, resolve it to the specific concept being discussed.'\",\n        \"examples_needed\": \"Yes - show vague vs. specific entity examples\"\n      },\n      {\n        \"issue\": \"No guidance on completing noun phrases fully\",\n        \"current_wording\": \"Likely no mention of phrase completeness\",\n        \"suggested_fix\": \"Add: 'Extract complete noun phrases. Do not cut off mid-phrase or end with prepositions/conjunctions. Example: NOT \\\"Soil Stewardship Handbook to\\\" but \\\"Osha and Hunter\\\" (the actual dedicatees).'\",\n        \"examples_needed\": \"Yes - show incomplete vs. complete phrase examples\"\n      }\n    ],\n    \"pass2_evaluation_issues\": [\n      {\n        \"issue\": \"No distinction between factual claims and philosophical/rhetorical statements\",\n        \"current_wording\": \"Likely evaluates all text-supported statements as equally factual\",\n        \"suggested_fix\": \"Add: 'Distinguish between factual claims (empirically verifiable) and philosophical/rhetorical statements (abstract, inspirational, non-verifiable). Mark philosophical statements with lower p_true scores (max 0.6) and flag as PHILOSOPHICAL_CLAIM. Examples: \\\"soil is what it means to be human\\\" (philosophical), \\\"soil contains nutrients\\\" (factual).'\",\n        \"examples_needed\": \"Yes - show 5-6 examples of philosophical vs. factual claims\"\n      },\n      {\n        \"issue\": \"Conflict detection may be too sensitive or not sensitive enough for certain error types\",\n        \"current_wording\": \"Unknown - prompt not provided\",\n        \"suggested_fix\": \"Review conflict detection logic. Ensure it flags: (1) Incomplete noun phrases, (2) Vague entities, (3) Unresolved pronouns, (4) Philosophical abstractions treated as facts.\",\n        \"examples_needed\": \"Yes - show examples of each conflict type\"\n      }\n    ]\n  },\n  \"system_health\": {\n    \"meets_production_criteria\": false,\n    \"target_quality_threshold\": 0.05,\n    \"current_quality_issue_rate\": 0.2185\n  }\n}\n```",
  "metadata": {
    "analysis_date": "2025-10-13T23:36:53.446719",
    "relationships_analyzed": 984,
    "reflector_version": "1.0_claude",
    "model_used": "claude-sonnet-4-5-20250929",
    "extraction_version": "v11.2.1"
  }
}