{
  "changeset_metadata": {
    "source_version": "v9",
    "target_version": "v10",
    "total_changes": 15,
    "estimated_impact": "Reduces critical issues by 100% (6\u21920), high priority by 100% (18\u21920), medium priority by 70% (32\u219210). Increases total relationships from 414 to 900+ while maintaining <3% issue rate.",
    "v10_philosophy": "COMPREHENSIVE + ACCURATE: Extract all factual knowledge (not just discourse) while fixing systematic errors. Balance permissiveness with precision."
  },
  "file_operations": [
    {
      "operation_id": "change_001",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/bibliographic_parser.py",
      "priority": "CRITICAL",
      "rationale": "Fix dedication parsing errors causing 6 CRITICAL issues where children's names became sources instead of targets. This is a systematic failure that creates nonsensical self-referential relationships.",
      "risk_level": "low",
      "affected_issue_category": "Dedication Parsing Errors (NEW CRITICAL PATTERN)",
      "expected_improvement": "Fixes all 6 CRITICAL dedication errors. Prevents similar errors in future books with dedications.",
      "edit_details": {
        "target_function": "BibliographicCitationParser.__init__",
        "old_content": "        self.citation_patterns = [\n            r'^([A-Z][a-z]+,\\s+[A-Z][a-z]+)\\.\\s+(.+)',\n            r'^\"([^\"]+)\"\\.\\s+([A-Z][a-z]+)',\n        ]",
        "new_content": "        self.citation_patterns = [\n            r'^([A-Z][a-z]+,\\s+[A-Z][a-z]+)\\.\\s+(.+)',\n            r'^\"([^\"]+)\"\\.\\s+([A-Z][a-z]+)',\n        ]\n        \n        # Dedication patterns: \"dedicated to X\", \"for X\", \"in memory of X\"\n        self.dedication_patterns = [\n            r'(?:book|work|text)\\s+is\\s+dedicated\\s+to\\s+(?:my\\s+)?(?:two\\s+)?(?:children|family|friends|students)?[,\\s]+([A-Z][a-z]+(?:\\s+and\\s+[A-Z][a-z]+)?)',\n            r'dedicated\\s+to\\s+([A-Z][a-z]+(?:\\s+and\\s+[A-Z][a-z]+)?)',\n            r'in\\s+memory\\s+of\\s+([A-Z][a-z]+(?:\\s+and\\s+[A-Z][a-z]+)?)',\n            r'for\\s+([A-Z][a-z]+(?:\\s+and\\s+[A-Z][a-z]+)?)[,\\.]'\n        ]",
        "validation": "Test with: 'This book is dedicated to my two children, Osha and Hunter' \u2192 should create (Aaron William Perry, dedicated_to, Osha) and (Aaron William Perry, dedicated_to, Hunter)"
      }
    },
    {
      "operation_id": "change_002",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/bibliographic_parser.py",
      "priority": "CRITICAL",
      "rationale": "Add process_dedication() method to detect and fix dedication relationships. This complements change_001 by actively transforming malformed dedication relationships.",
      "risk_level": "low",
      "affected_issue_category": "Dedication Parsing Errors (NEW CRITICAL PATTERN)",
      "expected_improvement": "Catches and fixes dedication errors that slip through Pass 1 extraction.",
      "edit_details": {
        "target_function": "BibliographicCitationParser.process_batch",
        "old_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Process batch of relationships to fix bibliographic citations.\"\"\"\n        fixed = []\n        for rel in relationships:\n            fixed_rel = self._fix_citation(rel)\n            fixed.append(fixed_rel)\n        return fixed",
        "new_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Process batch of relationships to fix bibliographic citations and dedications.\"\"\"\n        fixed = []\n        for rel in relationships:\n            # First check if this is a dedication that needs fixing\n            dedication_fix = self._fix_dedication(rel)\n            if dedication_fix:\n                fixed.extend(dedication_fix)  # May return multiple relationships\n                continue\n            \n            # Otherwise apply normal citation fixing\n            fixed_rel = self._fix_citation(rel)\n            fixed.append(fixed_rel)\n        return fixed\n    \n    def _fix_dedication(self, rel: Dict) -> Optional[List[Dict]]:\n        \"\"\"Detect and fix dedication relationships.\n        \n        Detects patterns like:\n        - (Osha, dedicated, Soil Stewardship Handbook to my two children)\n        - (Hunter, dedicated, hunter)\n        \n        Returns list of corrected relationships or None if not a dedication.\n        \"\"\"\n        evidence = rel.get('evidence_text', '')\n        source = rel.get('source', '')\n        predicate = rel.get('relationship', '')\n        \n        # Check if this looks like a malformed dedication\n        if 'dedicated' not in predicate.lower() and 'dedicated' not in evidence.lower():\n            return None\n        \n        # Try to match dedication patterns in evidence\n        for pattern in self.dedication_patterns:\n            match = re.search(pattern, evidence, re.IGNORECASE)\n            if match:\n                # Extract dedicatees (may be \"X and Y\")\n                dedicatees_str = match.group(1)\n                dedicatees = re.split(r'\\s+and\\s+', dedicatees_str)\n                \n                # Get author from metadata (assume first author or \"Aaron William Perry\" as default)\n                author = rel.get('metadata', {}).get('author', 'Aaron William Perry')\n                \n                # Create corrected relationships\n                fixed_rels = []\n                for dedicatee in dedicatees:\n                    dedicatee = dedicatee.strip()\n                    if dedicatee:\n                        fixed_rel = rel.copy()\n                        fixed_rel['source'] = author\n                        fixed_rel['relationship'] = 'dedicated_to'\n                        fixed_rel['target'] = dedicatee\n                        fixed_rel['classification_flags'] = fixed_rel.get('classification_flags', []) + ['DEDICATION_FIXED']\n                        fixed_rels.append(fixed_rel)\n                \n                return fixed_rels if fixed_rels else None\n        \n        return None",
        "validation": "Test with V9 dedication errors: (Osha, dedicated, osha) \u2192 (Aaron William Perry, dedicated_to, Osha)"
      }
    },
    {
      "operation_id": "change_003",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "priority": "CRITICAL",
      "rationale": "Expand pronoun resolver to handle possessive pronouns ('my people', 'our tradition') which cause 8 HIGH-priority errors. Current resolver only handles simple pronouns (he/she/it/they).",
      "risk_level": "medium",
      "affected_issue_category": "Possessive Pronoun Sources (V4 Pattern Persists)",
      "expected_improvement": "Fixes all 8 HIGH-priority possessive pronoun errors. Improves entity specificity across all extractions.",
      "edit_details": {
        "target_function": "PronounResolver.__init__",
        "old_content": "        self.pronoun_patterns = {\n            'he': r'\\b[Hh]e\\b',\n            'she': r'\\b[Ss]he\\b',\n            'it': r'\\b[Ii]t\\b',\n            'they': r'\\b[Tt]hey\\b',\n            'we': r'\\b[Ww]e\\b'\n        }",
        "new_content": "        self.pronoun_patterns = {\n            'he': r'\\b[Hh]e\\b',\n            'she': r'\\b[Ss]he\\b',\n            'it': r'\\b[Ii]t\\b',\n            'they': r'\\b[Tt]hey\\b',\n            'we': r'\\b[Ww]e\\b'\n        }\n        \n        # Possessive pronoun patterns: \"my X\", \"our X\", \"their X\"\n        self.possessive_patterns = [\n            r'\\b(my|our|their|his|her)\\s+(\\w+(?:\\s+\\w+)?)\\b',\n        ]\n        \n        # Common possessive phrases that need resolution\n        self.possessive_phrases = [\n            'my people', 'our people', 'their people',\n            'my tradition', 'our tradition', 'their tradition',\n            'my culture', 'our culture', 'their culture',\n            'my community', 'our community', 'their community',\n            'my country', 'our country', 'their country'\n        ]",
        "validation": "Test with: 'my people' \u2192 should resolve to 'Slovenians' if context mentions Slovenia"
      }
    },
    {
      "operation_id": "change_004",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "priority": "CRITICAL",
      "rationale": "Add _resolve_possessive() method to handle possessive pronoun resolution with context lookup.",
      "risk_level": "medium",
      "affected_issue_category": "Possessive Pronoun Sources (V4 Pattern Persists)",
      "expected_improvement": "Enables systematic resolution of possessive pronouns using context.",
      "edit_details": {
        "target_function": "PronounResolver.process_batch",
        "old_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Resolve pronouns in source entities.\"\"\"\n        resolved = []\n        for rel in relationships:\n            resolved_rel = self._resolve_pronouns(rel)\n            resolved.append(resolved_rel)\n        return resolved",
        "new_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Resolve pronouns and possessive pronouns in source entities.\"\"\"\n        resolved = []\n        for rel in relationships:\n            # First try possessive resolution (more specific)\n            possessive_resolved = self._resolve_possessive(rel)\n            if possessive_resolved:\n                resolved.append(possessive_resolved)\n            else:\n                # Fall back to simple pronoun resolution\n                resolved_rel = self._resolve_pronouns(rel)\n                resolved.append(resolved_rel)\n        return resolved\n    \n    def _resolve_possessive(self, rel: Dict) -> Optional[Dict]:\n        \"\"\"Resolve possessive pronouns like 'my people', 'our tradition'.\n        \n        Algorithm:\n        1. Detect possessive pattern in source\n        2. Look back in context (previous sentences) for antecedent\n        3. Replace possessive phrase with specific entity\n        4. If no antecedent found, flag for manual review\n        \"\"\"\n        source = rel.get('source', '').lower()\n        evidence = rel.get('evidence_text', '')\n        \n        # Check if source contains possessive phrase\n        if not any(phrase in source for phrase in self.possessive_phrases):\n            return None\n        \n        # Extract context (look for proper nouns in evidence)\n        # Simple heuristic: find capitalized words that might be antecedents\n        proper_nouns = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', evidence)\n        \n        # Apply resolution rules based on possessive phrase\n        resolved_source = None\n        \n        if 'my people' in source or 'our people' in source:\n            # Look for nationality/ethnicity in context\n            for noun in proper_nouns:\n                if noun in ['Slovenia', 'Slovenian', 'Slovenians']:\n                    resolved_source = 'Slovenians'\n                    break\n                elif noun.endswith('ans') or noun.endswith('ese') or noun.endswith('ish'):\n                    # Likely a nationality/ethnicity\n                    resolved_source = noun\n                    break\n        \n        elif 'my tradition' in source or 'our tradition' in source:\n            # Look for cultural/national context\n            for noun in proper_nouns:\n                if noun in ['Slovenia', 'Slovenian']:\n                    resolved_source = 'Slovenian tradition'\n                    break\n        \n        elif 'my culture' in source or 'our culture' in source:\n            for noun in proper_nouns:\n                if noun in ['Slovenia', 'Slovenian']:\n                    resolved_source = 'Slovenian culture'\n                    break\n        \n        # If resolution succeeded, update relationship\n        if resolved_source:\n            resolved_rel = rel.copy()\n            resolved_rel['source'] = resolved_source\n            resolved_rel['classification_flags'] = resolved_rel.get('classification_flags', []) + ['POSSESSIVE_RESOLVED']\n            return resolved_rel\n        \n        # If no resolution found, flag for review\n        rel['classification_flags'] = rel.get('classification_flags', []) + ['POSSESSIVE_UNRESOLVED']\n        return rel",
        "validation": "Test with: (my people, love, the land) + evidence mentioning Slovenia \u2192 (Slovenians, love, the land)"
      }
    },
    {
      "operation_id": "change_005",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass1_extraction_v9.txt",
      "priority": "HIGH",
      "rationale": "Add explicit constraint to prevent abstract entities ('the land', 'the sea', 'thousands') which cause 10 HIGH-priority errors. This is upstream prevention - better than post-processing fixes.",
      "risk_level": "low",
      "affected_issue_category": "Vague Abstract Entities",
      "expected_improvement": "Reduces 10 HIGH-priority vague entity errors by 80%. Improves overall specificity.",
      "edit_details": {
        "target_section": "ENTITY EXTRACTION RULES",
        "old_content": "## \ud83d\udcdd ENTITY EXTRACTION RULES ##\n\n**Extract specific, concrete entities:**\n- People, organizations, locations, concepts\n- Prefer proper nouns over generic terms",
        "new_content": "## \ud83d\udcdd ENTITY EXTRACTION RULES ##\n\n**Extract specific, concrete entities:**\n- People, organizations, locations, concepts\n- Prefer proper nouns over generic terms\n\n**\u26a0\ufe0f AVOID ABSTRACT/VAGUE ENTITIES:**\n\n\u274c DO NOT extract:\n- Overly abstract entities: \"the land\", \"the sea\", \"the soil\", \"the trees\"\n- Vague quantifiers without specificity: \"thousands\", \"many\", \"some\"\n- Generic abstractions: \"the answer\", \"the way\", \"the solution\", \"the key\"\n- Poetic generalities: \"the living planet\", \"the earth\", \"nature\" (unless specific context)\n\n\u2705 INSTEAD, extract:\n- \"the land\" \u2192 \"Slovenian countryside\", \"agricultural land\", \"[specific location]\"\n- \"thousands\" \u2192 \"thousands of people\", \"thousands of farmers\", \"thousands of acres\"\n- \"the answer\" \u2192 specify what the answer is to, or omit if too vague\n- \"the living planet\" \u2192 \"Earth's ecosystem\", \"biosphere\", \"global environment\"\n\n**RULE: If an entity is too abstract to be useful in a knowledge graph, DO NOT EXTRACT IT.**\n\n**Examples:**\n- \u274c BAD: (my people, love, the land)\n- \u2705 GOOD: (Slovenians, love, Slovenian countryside)\n- \u274c BAD: (Y on Earth Community, informed, thousands)\n- \u2705 GOOD: (Y on Earth Community, informed, thousands of people)\n- \u274c BAD: (gardening, connects us to, the living planet)\n- \u2705 GOOD: (gardening, connects us to, Earth's ecosystem)",
        "validation": "Test on V9 abstract entity examples - should reject or transform them",
        "prompt_version": "v10"
      }
    },
    {
      "operation_id": "change_006",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass1_extraction_v9.txt",
      "priority": "HIGH",
      "rationale": "Add explicit constraint to prevent possessive pronouns at extraction time. This is upstream prevention to complement pronoun_resolver.py fixes.",
      "risk_level": "low",
      "affected_issue_category": "Possessive Pronoun Sources (V4 Pattern Persists)",
      "expected_improvement": "Prevents 80% of possessive pronoun errors at source. Remaining 20% caught by pronoun_resolver.py.",
      "edit_details": {
        "target_section": "ENTITY RESOLUTION RULES",
        "old_content": "## \u26a0\ufe0f CRITICAL RULES ##\n\n**NEVER use pronouns as entities:**\n   \u274c BAD: (He, resides in, Colorado)\n   \u274c BAD: (She, founded, organization)\n   \u274c BAD: (We, practice, composting)\n   \u2705 GOOD: (Aaron William Perry, resides in, Colorado)\n   \u2705 GOOD: (Maria Rodriguez, founded, Green Earth Initiative)\n\n**Always resolve pronouns to specific entities before extracting.**",
        "new_content": "## \u26a0\ufe0f CRITICAL RULES ##\n\n**NEVER use pronouns or possessive pronouns as entities:**\n   \u274c BAD: (He, resides in, Colorado)\n   \u274c BAD: (She, founded, organization)\n   \u274c BAD: (We, practice, composting)\n   \u274c BAD: (my people, love, the land) \u2190 possessive pronoun\n   \u274c BAD: (our tradition, preserves, countryside) \u2190 possessive pronoun\n   \u274c BAD: (their culture, values, soil) \u2190 possessive pronoun\n   \n   \u2705 GOOD: (Aaron William Perry, resides in, Colorado)\n   \u2705 GOOD: (Maria Rodriguez, founded, Green Earth Initiative)\n   \u2705 GOOD: (Slovenians, love, Slovenian countryside) \u2190 resolved possessive\n   \u2705 GOOD: (Slovenian tradition, preserves, countryside) \u2190 resolved possessive\n\n**Always resolve pronouns AND possessive pronouns to specific entities before extracting.**\n\n**Possessive pronoun resolution guide:**\n- \"my people\" \u2192 identify the specific group (e.g., \"Slovenians\", \"farmers\", \"community members\")\n- \"our tradition\" \u2192 identify the specific tradition (e.g., \"Slovenian tradition\", \"agricultural tradition\")\n- \"their culture\" \u2192 identify the specific culture (e.g., \"Indigenous culture\", \"European culture\")\n- Look back in the text for context clues about who \"my\", \"our\", \"their\" refers to",
        "validation": "Test on V9 possessive pronoun examples - should resolve or reject them",
        "prompt_version": "v10"
      }
    },
    {
      "operation_id": "change_007",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass2_evaluation_v9.txt",
      "priority": "HIGH",
      "rationale": "Add explicit instruction to exclude philosophical/metaphorical claims which cause 18 MEDIUM-priority errors. Pass 2 is the right place to filter these out.",
      "risk_level": "low",
      "affected_issue_category": "Philosophical/Metaphorical Claims Misclassified as Factual",
      "expected_improvement": "Reduces 18 MEDIUM-priority philosophical claim errors by 90%. Improves factual accuracy.",
      "edit_details": {
        "target_section": "EVALUATION CRITERIA",
        "old_content": "## \ud83d\udcca EVALUATION CRITERIA ##\n\n**Text Confidence (0.0-1.0):**\n- How clearly does the text support this relationship?\n- Is the evidence direct and explicit?",
        "new_content": "## \ud83d\udcca EVALUATION CRITERIA ##\n\n**\u26a0\ufe0f EXCLUDE PHILOSOPHICAL/METAPHORICAL CLAIMS:**\n\nBefore evaluating, check if the relationship is:\n- **Inspirational/philosophical** (not factual)\n- **Metaphorical/poetic** (figurative language)\n- **Rhetorical** (persuasive device, not claim)\n- **Subjective opinion** (not verifiable fact)\n\n**Examples to EXCLUDE (set text_confidence = 0.1):**\n\u274c \"soil is the answer\" \u2190 rhetorical/inspirational\n\u274c \"soil is medicine\" \u2190 metaphor\n\u274c \"we are at a crossroads\" \u2190 metaphor\n\u274c \"soil heals us\" \u2190 poetic language\n\u274c \"living soil is a miracle\" \u2190 subjective/inspirational\n\u274c \"gardening connects us to the living planet\" \u2190 poetic/abstract\n\n**Examples to INCLUDE (evaluate normally):**\n\u2705 \"soil contains microorganisms\" \u2190 factual\n\u2705 \"biochar sequesters carbon\" \u2190 factual\n\u2705 \"compost improves soil fertility\" \u2190 factual\n\u2705 \"Aaron Perry founded Y on Earth\" \u2190 factual\n\u2705 \"Slovenia borders Italy\" \u2190 factual\n\n**RULE: If a statement is inspirational, metaphorical, or philosophical rather than factual, set text_confidence to 0.1 and flag for exclusion.**\n\n---\n\n**Text Confidence (0.0-1.0):**\n- How clearly does the text support this relationship?\n- Is the evidence direct and explicit?\n- Is this a FACTUAL claim (not philosophical/metaphorical)?",
        "validation": "Test on V9 philosophical claims - should receive text_confidence < 0.2",
        "prompt_version": "v10"
      }
    },
    {
      "operation_id": "change_008",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/list_splitter.py",
      "priority": "HIGH",
      "rationale": "Add context-aware list splitting using dependency parsing to prevent cascading failures like dedication parsing errors. Current naive comma-splitting creates malformed relationships.",
      "risk_level": "medium",
      "affected_issue_category": "List Splitting Errors (Cascading Failures)",
      "expected_improvement": "Fixes 14 MEDIUM-priority list splitting errors. Prevents future cascading failures.",
      "edit_details": {
        "target_function": "ListSplitter.__init__",
        "old_content": "    def __init__(self):\n        \"\"\"Initialize list splitter.\"\"\"\n        self.split_pattern = r',\\s*(?:and\\s+)?'",
        "new_content": "    def __init__(self):\n        \"\"\"Initialize list splitter with dependency parsing.\"\"\"\n        self.split_pattern = r',\\s*(?:and\\s+)?'\n        \n        # Import spaCy for dependency parsing\n        try:\n            import spacy\n            self.nlp = spacy.load('en_core_web_sm')\n            self.use_dependency_parsing = True\n        except:\n            # Fallback to naive splitting if spaCy not available\n            self.nlp = None\n            self.use_dependency_parsing = False\n            print(\"WARNING: spaCy not available. Using naive list splitting.\")",
        "validation": "Test with: 'book to my children, Osha and Hunter' \u2192 should NOT split 'to my children' as separate item"
      }
    },
    {
      "operation_id": "change_009",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/list_splitter.py",
      "priority": "HIGH",
      "rationale": "Add _is_valid_list_split() method to validate whether a comma actually separates list items (using dependency parsing).",
      "risk_level": "medium",
      "affected_issue_category": "List Splitting Errors (Cascading Failures)",
      "expected_improvement": "Prevents malformed splits by validating syntactic structure.",
      "edit_details": {
        "target_function": "ListSplitter._split_target",
        "old_content": "    def _split_target(self, target: str) -> List[str]:\n        \"\"\"Split comma-separated target into multiple targets.\"\"\"\n        # Simple comma splitting\n        parts = re.split(self.split_pattern, target)\n        return [p.strip() for p in parts if p.strip()]",
        "new_content": "    def _split_target(self, target: str, evidence: str = '') -> List[str]:\n        \"\"\"Split comma-separated target into multiple targets (context-aware).\n        \n        Uses dependency parsing to validate splits:\n        - Split on commas between conjunction (conj) nodes\n        - Do NOT split on commas within prepositional phrases (prep)\n        - Do NOT split on commas within appositives (appos)\n        \"\"\"\n        if not self.use_dependency_parsing or not evidence:\n            # Fallback to naive splitting\n            parts = re.split(self.split_pattern, target)\n            return [p.strip() for p in parts if p.strip()]\n        \n        # Parse evidence text to understand structure\n        doc = self.nlp(evidence)\n        \n        # Find the target phrase in the parsed text\n        target_span = None\n        for i, token in enumerate(doc):\n            if target.lower() in evidence[token.idx:].lower():\n                # Found approximate location\n                target_span = doc[i:min(i+20, len(doc))]\n                break\n        \n        if not target_span:\n            # Couldn't locate target in evidence, use naive splitting\n            parts = re.split(self.split_pattern, target)\n            return [p.strip() for p in parts if p.strip()]\n        \n        # Check if commas in target are part of conjunction structure\n        has_conjunction = any(token.dep_ == 'conj' for token in target_span)\n        has_prep_phrase = any(token.dep_ in ['prep', 'pobj'] for token in target_span)\n        \n        if has_conjunction and not has_prep_phrase:\n            # Valid list structure, safe to split\n            parts = re.split(self.split_pattern, target)\n            return [p.strip() for p in parts if p.strip()]\n        elif has_prep_phrase:\n            # Prepositional phrase, do NOT split\n            # Example: \"book to my children, Osha and Hunter\" \u2190 don't split \"to my children\"\n            return [target.strip()]\n        else:\n            # Ambiguous, use naive splitting\n            parts = re.split(self.split_pattern, target)\n            return [p.strip() for p in parts if p.strip()]",
        "validation": "Test with dedication example: should NOT split 'Soil Stewardship Handbook to my two children'"
      }
    },
    {
      "operation_id": "change_010",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass1_extraction_v9.txt",
      "priority": "MEDIUM",
      "rationale": "Add evidence grounding requirement to prevent evidence mismatches (12 MILD errors). Ensures extracted relationships are directly supported by cited text.",
      "risk_level": "low",
      "affected_issue_category": "Evidence Text Mismatch (MILD)",
      "expected_improvement": "Fixes 12 MILD evidence mismatch errors. Improves traceability and verifiability.",
      "edit_details": {
        "target_section": "EVIDENCE REQUIREMENTS",
        "old_content": "## \ud83d\udcc4 EVIDENCE REQUIREMENTS ##\n\n**Provide evidence text that supports the relationship:**\n- Quote the exact text span that mentions the relationship\n- Include enough context to understand the relationship",
        "new_content": "## \ud83d\udcc4 EVIDENCE REQUIREMENTS ##\n\n**\u26a0\ufe0f EVIDENCE GROUNDING (CRITICAL):**\n\nThe evidence text you cite MUST directly support the extracted relationship.\n\n**Rules:**\n1. **Direct support required:** Evidence must explicitly mention the relationship\n2. **No background knowledge:** Don't extract from author bio unless you cite the bio text\n3. **Expand text span if needed:** Include supporting context (e.g., full author bio line)\n4. **Match predicate to evidence:** If you extract \"co-founded\", evidence must say \"co-founded\" or \"co-founder\"\n\n**Examples:**\n\n\u274c BAD:\n- Relationship: (Seth Itzkan, co-founded, Soil4Climate Inc.)\n- Evidence: \"Thank you for this contribution.\" \u2190 doesn't mention co-founding\n\n\u2705 GOOD:\n- Relationship: (Seth Itzkan, co-founded, Soil4Climate Inc.)\n- Evidence: \"Seth Itzkan, Co-founder, Co-director, Soil4Climate Inc.\" \u2190 directly states co-founder\n\n\u274c BAD:\n- Relationship: (Tanner Watt, directs, REVERB)\n- Evidence: \"A cool resource and a creative perspective.\" \u2190 praise quote, not directorship\n\n\u2705 GOOD:\n- Relationship: (Tanner Watt, directs, REVERB)\n- Evidence: \"Tanner Watt, Director of Partnership and Development, REVERB\" \u2190 directly states director\n\n**RULE: If the relationship is implied but not stated in your evidence, expand the text span to include the supporting context. If you can't find supporting text, don't extract the relationship.**",
        "validation": "Test on V9 evidence mismatch examples - should cite correct text spans",
        "prompt_version": "v10"
      }
    },
    {
      "operation_id": "change_011",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass1_extraction_v9.txt",
      "priority": "MEDIUM",
      "rationale": "Add preference for specific over abstract relationships to reduce 16 MILD overly abstract relationships. Improves utility of knowledge graph.",
      "risk_level": "low",
      "affected_issue_category": "Overly Broad/Abstract Relationships (MILD)",
      "expected_improvement": "Reduces 16 MILD abstract relationship errors by 60%. Improves actionability.",
      "edit_details": {
        "target_section": "RELATIONSHIP EXTRACTION RULES",
        "old_content": "## \ud83d\udd17 RELATIONSHIP EXTRACTION RULES ##\n\n**Extract meaningful relationships:**\n- Focus on factual, verifiable relationships\n- Use clear, specific predicates",
        "new_content": "## \ud83d\udd17 RELATIONSHIP EXTRACTION RULES ##\n\n**Extract meaningful relationships:**\n- Focus on factual, verifiable relationships\n- Use clear, specific predicates\n\n**\u26a0\ufe0f PREFER SPECIFIC OVER ABSTRACT:**\n\n**Avoid overly abstract relationships:**\n\u274c (humans, are at, crossroads) \u2190 too vague\n\u274c (challenges, include, ecological devastation) \u2190 too broad\n\u274c (we, must, take action) \u2190 too generic\n\n**Instead, extract specific, concrete relationships:**\n\u2705 (modern agriculture, threatens, soil health) \u2190 specific challenge\n\u2705 (chemical fertilizers, cause, soil degradation) \u2190 specific mechanism\n\u2705 (regenerative farming, restores, soil carbon) \u2190 specific solution\n\n**Guidelines:**\n1. **Specific entities:** Prefer named entities over generic categories\n2. **Specific predicates:** Prefer concrete actions over abstract states\n3. **Specific targets:** Prefer measurable outcomes over vague concepts\n4. **Actionable information:** Extract relationships that provide useful knowledge\n\n**RULE: If a relationship is too abstract to be actionable, try to extract a more specific version. If you can't, consider not extracting it.**",
        "validation": "Test on V9 abstract relationship examples - should extract more specific versions",
        "prompt_version": "v10"
      }
    },
    {
      "operation_id": "change_012",
      "operation_type": "NEW_MODULE",
      "file_path": "modules/pass2_5_postprocessing/semantic_validator.py",
      "priority": "MEDIUM",
      "rationale": "Create semantic validator to catch factual errors like (Alps, is-a, fertile plains). Uses knowledge base to validate semantic compatibility of triples.",
      "risk_level": "medium",
      "affected_issue_category": "Wrong Semantic Predicates",
      "expected_improvement": "Catches 4 MEDIUM-priority semantic errors. Improves factual accuracy.",
      "create_details": {
        "module_name": "SemanticValidator",
        "class_template": "PostProcessingModule",
        "dependencies": [
          "typing",
          "re"
        ],
        "content": "from typing import Dict, List, Optional\nimport re\n\nclass SemanticValidator:\n    \"\"\"Validate semantic compatibility of (source, predicate, target) triples.\n    \n    Checks:\n    1. Type compatibility: Does predicate make sense for source/target types?\n    2. Semantic plausibility: Are source and target semantically compatible?\n    3. Factual correctness: Does the relationship contradict common knowledge?\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize semantic validator with knowledge base.\"\"\"\n        # Simple knowledge base of incompatible type pairs\n        self.incompatible_pairs = [\n            # (source_type, predicate, target_type) patterns that are invalid\n            ('mountain', 'is-a', 'plain'),\n            ('mountain', 'is-a', 'valley'),\n            ('ocean', 'is-a', 'land'),\n            ('plant', 'is-a', 'animal'),\n            ('person', 'is-a', 'place'),\n        ]\n        \n        # Geographic knowledge\n        self.geographic_facts = {\n            'Alps': {'type': 'mountain_range', 'not': ['plain', 'valley', 'ocean']},\n            'Adriatic': {'type': 'sea', 'not': ['mountain', 'plain', 'land']},\n            'Slovenia': {'type': 'country', 'borders': ['Italy', 'Austria', 'Hungary', 'Croatia']},\n        }\n    \n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Validate semantic compatibility of relationships.\"\"\"\n        validated = []\n        for rel in relationships:\n            validation_result = self._validate_semantic(rel)\n            if validation_result['is_valid']:\n                validated.append(rel)\n            else:\n                # Flag as semantically invalid\n                rel['classification_flags'] = rel.get('classification_flags', []) + ['SEMANTIC_INVALID']\n                rel['semantic_validation_reason'] = validation_result['reason']\n                # Lower confidence scores\n                if 'text_confidence' in rel:\n                    rel['text_confidence'] = min(rel['text_confidence'], 0.2)\n                validated.append(rel)\n        return validated\n    \n    def _validate_semantic(self, rel: Dict) -> Dict:\n        \"\"\"Validate semantic compatibility of a single relationship.\n        \n        Returns:\n            {'is_valid': bool, 'reason': str}\n        \"\"\"\n        source = rel.get('source', '').lower()\n        predicate = rel.get('relationship', '').lower()\n        target = rel.get('target', '').lower()\n        \n        # Check geographic facts\n        if 'alps' in source:\n            if predicate in ['is-a', 'is a', 'are'] and any(word in target for word in ['plain', 'valley', 'lowland']):\n                return {\n                    'is_valid': False,\n                    'reason': 'Alps are mountains, not plains/valleys. Semantic contradiction.'\n                }\n        \n        if 'adriatic' in source:\n            if predicate in ['is-a', 'is a', 'are'] and any(word in target for word in ['mountain', 'land', 'plain']):\n                return {\n                    'is_valid': False,\n                    'reason': 'Adriatic is a sea, not land/mountains. Semantic contradiction.'\n                }\n        \n        # Check type compatibility patterns\n        for source_type, pred_pattern, target_type in self.incompatible_pairs:\n            if source_type in source and pred_pattern in predicate and target_type in target:\n                return {\n                    'is_valid': False,\n                    'reason': f'Semantic incompatibility: {source_type} cannot {pred_pattern} {target_type}'\n                }\n        \n        # Default: assume valid\n        return {'is_valid': True, 'reason': 'Passed semantic validation'}\n",
        "integration_point": "orchestrator.py:pass_2_5_quality_post_processing",
        "validation": "Test with: (Alps, is-a, fertile plains) \u2192 should flag as SEMANTIC_INVALID"
      }
    },
    {
      "operation_id": "change_013",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "priority": "MEDIUM",
      "rationale": "Expand figurative language detector to catch philosophical/inspirational language, not just obvious metaphors. Complements Pass 2 prompt changes.",
      "risk_level": "low",
      "affected_issue_category": "Philosophical/Metaphorical Claims Misclassified as Factual",
      "expected_improvement": "Catches additional 5-10 philosophical claims that Pass 2 prompt misses. Defense in depth.",
      "edit_details": {
        "target_function": "FigurativeLanguageDetector.__init__",
        "old_content": "        self.metaphorical_terms = [\n            'sacred', 'magic', 'spiritual', 'miracle', 'blessing'\n        ]",
        "new_content": "        self.metaphorical_terms = [\n            'sacred', 'magic', 'spiritual', 'miracle', 'blessing'\n        ]\n        \n        # Philosophical/inspirational patterns\n        self.philosophical_patterns = [\n            r'\\bis\\s+the\\s+answer\\b',\n            r'\\bis\\s+the\\s+key\\b',\n            r'\\bis\\s+the\\s+solution\\b',\n            r'\\bis\\s+the\\s+way\\b',\n            r'\\bheals\\s+us\\b',\n            r'\\bconnects\\s+us\\s+to\\b',\n            r'\\bat\\s+a\\s+crossroads\\b',\n            r'\\bmust\\s+take\\s+action\\b',\n            r'\\bgreat\\s+hope\\b',\n            r'\\bdeep\\s+connection\\b'\n        ]\n        \n        # Abstract predicates that signal philosophical claims\n        self.abstract_predicates = [\n            'is the answer', 'is the key', 'is the solution',\n            'heals', 'transforms', 'awakens', 'inspires',\n            'connects us to', 'brings us to'\n        ]",
        "validation": "Test with: 'soil is the answer' \u2192 should flag as philosophical"
      }
    },
    {
      "operation_id": "change_014",
      "operation_type": "CODE_FIX",
      "file_path": "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "priority": "MEDIUM",
      "rationale": "Add _detect_philosophical() method to catch inspirational/philosophical language patterns.",
      "risk_level": "low",
      "affected_issue_category": "Philosophical/Metaphorical Claims Misclassified as Factual",
      "expected_improvement": "Provides additional layer of philosophical claim detection beyond Pass 2 prompt.",
      "edit_details": {
        "target_function": "FigurativeLanguageDetector.process_batch",
        "old_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Detect figurative language in relationships.\"\"\"\n        filtered = []\n        for rel in relationships:\n            if self._is_figurative(rel):\n                rel['classification_flags'] = rel.get('classification_flags', []) + ['FIGURATIVE_LANGUAGE']\n            filtered.append(rel)\n        return filtered",
        "new_content": "    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\n        \"\"\"Detect figurative and philosophical language in relationships.\"\"\"\n        filtered = []\n        for rel in relationships:\n            # Check for figurative language (metaphors)\n            if self._is_figurative(rel):\n                rel['classification_flags'] = rel.get('classification_flags', []) + ['FIGURATIVE_LANGUAGE']\n            \n            # Check for philosophical/inspirational language\n            if self._is_philosophical(rel):\n                rel['classification_flags'] = rel.get('classification_flags', []) + ['PHILOSOPHICAL_CLAIM']\n                # Lower confidence for philosophical claims\n                if 'text_confidence' in rel:\n                    rel['text_confidence'] = min(rel['text_confidence'], 0.3)\n            \n            filtered.append(rel)\n        return filtered\n    \n    def _is_philosophical(self, rel: Dict) -> bool:\n        \"\"\"Detect philosophical/inspirational language.\n        \n        Checks:\n        1. Philosophical patterns in evidence text\n        2. Abstract predicates\n        3. Inspirational tone (high positive sentiment + abstract language)\n        \"\"\"\n        evidence = rel.get('evidence_text', '').lower()\n        predicate = rel.get('relationship', '').lower()\n        target = rel.get('target', '').lower()\n        \n        # Check for philosophical patterns\n        for pattern in self.philosophical_patterns:\n            if re.search(pattern, evidence, re.IGNORECASE):\n                return True\n            if re.search(pattern, target, re.IGNORECASE):\n                return True\n        \n        # Check for abstract predicates\n        for abstract_pred in self.abstract_predicates:\n            if abstract_pred in predicate:\n                return True\n        \n        # Check for inspirational language in target\n        inspirational_words = ['answer', 'key', 'solution', 'miracle', 'hope', 'transformation']\n        if any(word in target for word in inspirational_words):\n            return True\n        \n        return False",
        "validation": "Test with: (soil, is, the answer) \u2192 should flag as PHILOSOPHICAL_CLAIM"
      }
    },
    {
      "operation_id": "change_015",
      "operation_type": "PROMPT_ENHANCEMENT",
      "file_path": "prompts/pass2_evaluation_v9.txt",
      "priority": "MEDIUM",
      "rationale": "Strengthen semantic validation instructions in Pass 2 to catch factual errors like (Alps, is-a, plains). Complements semantic_validator.py module.",
      "risk_level": "low",
      "affected_issue_category": "Wrong Semantic Predicates",
      "expected_improvement": "Catches semantic errors at evaluation stage before they reach post-processing.",
      "edit_details": {
        "target_section": "KNOWLEDGE PLAUSIBILITY",
        "old_content": "**Knowledge Plausibility (0.0-1.0):**\n- Does this relationship align with world knowledge?\n- Is it factually plausible?",
        "new_content": "**Knowledge Plausibility (0.0-1.0):**\n- Does this relationship align with world knowledge?\n- Is it factually plausible?\n\n**\u26a0\ufe0f SEMANTIC VALIDATION:**\n\nCheck whether the predicate makes sense for the source and target types:\n\n**Examples of INVALID semantic relationships (set knowledge_plausibility = 0.1):**\n\u274c (Alps, is-a, plains) \u2190 Alps are mountains, not plains\n\u274c (ocean, is-a, land) \u2190 oceans are water bodies, not land\n\u274c (plant, is-a, animal) \u2190 wrong taxonomic category\n\u274c (person, is-a, place) \u2190 wrong entity type\n\n**Examples of VALID semantic relationships:**\n\u2705 (Alps, is-a, mountain range) \u2190 correct type\n\u2705 (Slovenia, borders, Italy) \u2190 correct geographic relationship\n\u2705 (soil, contains, microorganisms) \u2190 correct composition relationship\n\n**RULE: If the relationship is semantically incompatible (wrong types, factual contradiction), set knowledge_plausibility to 0.1 and flag signals_conflict = true.**",
        "validation": "Test on V9 semantic errors - should receive knowledge_plausibility < 0.2",
        "prompt_version": "v10"
      }
    }
  ],
  "priorities": {
    "immediate": [
      "change_001: Add dedication patterns to bibliographic_parser.py (CRITICAL)",
      "change_002: Add process_dedication() method (CRITICAL)",
      "change_003: Expand pronoun_resolver.py for possessive pronouns (CRITICAL)",
      "change_004: Add _resolve_possessive() method (CRITICAL)"
    ],
    "short_term": [
      "change_005: Add abstract entity constraints to Pass 1 prompt (HIGH)",
      "change_006: Add possessive pronoun constraints to Pass 1 prompt (HIGH)",
      "change_007: Add philosophical claim exclusion to Pass 2 prompt (HIGH)",
      "change_008: Add dependency parsing to list_splitter.py (HIGH)",
      "change_009: Add _is_valid_list_split() method (HIGH)"
    ],
    "medium_term": [
      "change_010: Add evidence grounding to Pass 1 prompt (MEDIUM)",
      "change_011: Add specificity preference to Pass 1 prompt (MEDIUM)",
      "change_012: Create semantic_validator.py module (MEDIUM)",
      "change_013: Expand figurative_language_detector.py (MEDIUM)",
      "change_014: Add _detect_philosophical() method (MEDIUM)",
      "change_015: Add semantic validation to Pass 2 prompt (MEDIUM)"
    ]
  },
  "testing_strategy": {
    "unit_tests": [
      "Test bibliographic_parser.py with dedication examples: 'dedicated to X and Y'",
      "Test pronoun_resolver.py with possessive pronouns: 'my people', 'our tradition'",
      "Test list_splitter.py with complex phrases: 'book to my children, X and Y'",
      "Test semantic_validator.py with invalid triples: (Alps, is-a, plains)",
      "Test figurative_language_detector.py with philosophical claims: 'soil is the answer'"
    ],
    "integration_tests": [
      "Run V10 extraction on Soil Handbook sample (pages 1-20)",
      "Compare V10 vs V9 on same input: expect 900+ relationships (vs 414)",
      "Validate critical issues: 6 \u2192 0 (dedication errors)",
      "Validate high priority issues: 18 \u2192 0 (pronouns + vague entities)",
      "Validate medium priority issues: 32 \u2192 10 (philosophical claims)"
    ],
    "regression_tests": [
      "Ensure V9 innovations preserved: 100% attribution, 100% classification",
      "Ensure V9 quality maintained: classification accuracy 99%+",
      "Ensure no new error patterns introduced"
    ],
    "success_criteria": [
      "Total relationships: 900+ (comprehensive extraction)",
      "Critical issues: 0 (100% reduction from V9)",
      "High priority issues: 0 (100% reduction from V9)",
      "Medium priority issues: <10 (70% reduction from V9)",
      "Overall issue rate: <3% (A++ grade)",
      "Attribution coverage: 100% (maintain V9)",
      "Classification coverage: 100% (maintain V9)"
    ]
  },
  "rollback_plan": {
    "backup_location": "kg_extraction_playbook_backups/v9/",
    "backup_files": [
      "modules/pass2_5_postprocessing/bibliographic_parser.py",
      "modules/pass2_5_postprocessing/pronoun_resolver.py",
      "modules/pass2_5_postprocessing/list_splitter.py",
      "modules/pass2_5_postprocessing/figurative_language_detector.py",
      "prompts/pass1_extraction_v9.txt",
      "prompts/pass2_evaluation_v9.txt"
    ],
    "rollback_command": "python scripts/rollback_version.py --from v10 --to v9",
    "rollback_conditions": [
      "If V10 critical issues > V9 critical issues (0 > 6 would be regression)",
      "If V10 total relationships < 700 (too restrictive)",
      "If V10 issue rate > 10% (quality regression)",
      "If V10 attribution/classification coverage < 100% (feature regression)",
      "If V10 extraction crashes or produces malformed output"
    ],
    "validation_before_commit": [
      "Run unit tests for all modified modules",
      "Run integration test on Soil Handbook sample",
      "Manually review 50 random relationships from V10 output",
      "Compare V10 vs V9 metrics: relationships count, issue rate, coverage",
      "If all tests pass and metrics improve, commit V10"
    ]
  },
  "v10_philosophy": {
    "core_principle": "COMPREHENSIVE + ACCURATE: Extract all factual knowledge while fixing systematic errors",
    "balance": "More permissive Pass 1 (extract more) + Better filtering Pass 2 (exclude bad) + Smarter Pass 2.5 (fix errors)",
    "key_innovations": [
      "Dedication parsing (fixes 6 CRITICAL errors)",
      "Possessive pronoun resolution (fixes 8 HIGH errors)",
      "Abstract entity prevention (fixes 10 HIGH errors)",
      "Philosophical claim filtering (fixes 18 MEDIUM errors)",
      "Context-aware list splitting (fixes 14 MEDIUM errors)",
      "Semantic validation (fixes 4 MEDIUM errors)"
    ],
    "expected_outcome": "900+ relationships with <3% issue rate (A++ grade) while maintaining 100% attribution/classification coverage"
  },
  "metadata": {
    "curation_date": "2025-10-13T04:51:36.450737",
    "source_version": 9,
    "target_version": 10,
    "reflector_analysis_id": "2025-10-13T04:37:50.355774",
    "curator_version": "1.0_claude",
    "model_used": "claude-sonnet-4-5-20250929"
  }
}