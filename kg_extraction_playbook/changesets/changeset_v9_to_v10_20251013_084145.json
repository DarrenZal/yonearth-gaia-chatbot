{
  "error": "json_parse_failed",
  "raw_response": "```json\n{\n  \"changeset_metadata\": {\n    \"source_version\": \"v9\",\n    \"target_version\": \"v10\",\n    \"total_changes\": 15,\n    \"estimated_impact\": \"Reduces critical issues by 100% (12\u21920), high priority by 79% (38\u21928), medium priority by 67% (45\u219215). Overall issue rate: 13.6% \u2192 3.2%\",\n    \"regression_context\": \"V9 shows 2.0x regression from V7 (6.71%\u219213.6%). V10 targets sub-5% threshold with focus on catastrophic dedication parser failure and systemic prompt weaknesses.\"\n  },\n  \"file_operations\": [\n    {\n      \"operation_id\": \"change_001\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/dedication_parser.py\",\n      \"priority\": \"CRITICAL\",\n      \"rationale\": \"Dedication parser is creating 6+ malformed relationships per dedication (12 critical issues, 1.4% of total). Root cause: Multiple parsing strategies executing simultaneously and concatenating results. Complete rewrite needed to use single-strategy approach with proper deduplication.\",\n      \"risk_level\": \"medium\",\n      \"affected_issue_category\": \"Dedication Parsing Catastrophic Failure\",\n      \"expected_improvement\": \"Eliminates all 12 CRITICAL issues. Reduces issue rate by 1.4 percentage points (13.6%\u219212.2%).\",\n\n      \"edit_details\": {\n        \"target_function\": \"DedicationParser.process_batch\",\n        \"old_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Process dedication relationships.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            if rel['relationship'].lower() in ['dedicated', 'dedicated to']:\\n                # Extract dedication targets\\n                target = rel['target']\\n                # Split on commas and 'and'\\n                parts = re.split(r',|\\\\band\\\\b', target)\\n                for part in parts:\\n                    new_rel = rel.copy()\\n                    new_rel['target'] = part.strip()\\n                    processed.append(new_rel)\\n                # Also add full target\\n                processed.append(rel)\\n            else:\\n                processed.append(rel)\\n        return processed\",\n        \"new_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Process dedication relationships with single-strategy parsing.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            if rel['relationship'].lower() in ['dedicated', 'dedicated to']:\\n                # Extract and clean dedication targets\\n                target = rel['target']\\n                \\n                # Step 1: Remove book title if present (common pattern: \\\"Book to Person\\\")\\n                # Look for pattern: \\\"[Title] to [Names]\\\"\\n                title_pattern = r'^(.+?)\\\\s+to\\\\s+(.+)$'\\n                title_match = re.match(title_pattern, target, re.IGNORECASE)\\n                if title_match:\\n                    # Check if first part looks like a book title (contains \\\"Handbook\\\", \\\"Guide\\\", etc.)\\n                    potential_title = title_match.group(1)\\n                    if any(word in potential_title for word in ['Handbook', 'Guide', 'Book', 'Manual']):\\n                        target = title_match.group(2)  # Keep only the names part\\n                \\n                # Step 2: Remove possessive phrases (\\\"my two children\\\", \\\"our family\\\")\\n                target = re.sub(r'\\\\b(my|our|their)\\\\s+(\\\\w+\\\\s+)*(children|family|friends|colleagues)\\\\b', '', target, flags=re.IGNORECASE)\\n                \\n                # Step 3: Split on commas and conjunctions to get individual dedicatees\\n                # Split on: comma, \\\"and\\\", \\\"&\\\"\\n                parts = re.split(r'\\\\s*[,&]\\\\s*|\\\\s+and\\\\s+', target, flags=re.IGNORECASE)\\n                \\n                # Step 4: Clean each dedicatee name\\n                dedicatees = []\\n                for part in parts:\\n                    cleaned = part.strip()\\n                    # Remove trailing punctuation\\n                    cleaned = re.sub(r'[.,;:!?]+$', '', cleaned)\\n                    # Remove leading articles/prepositions\\n                    cleaned = re.sub(r'^(to|the)\\\\s+', '', cleaned, flags=re.IGNORECASE)\\n                    # Skip empty or very short strings\\n                    if len(cleaned) > 1 and cleaned.lower() not in ['to', 'and', 'or', 'the']:\\n                        # Normalize case: Title Case for proper names\\n                        if cleaned[0].isupper():  # Likely a proper name\\n                            cleaned = cleaned.title()\\n                        dedicatees.append(cleaned)\\n                \\n                # Step 5: Deduplicate (case-insensitive)\\n                seen = set()\\n                unique_dedicatees = []\\n                for name in dedicatees:\\n                    name_lower = name.lower()\\n                    if name_lower not in seen:\\n                        seen.add(name_lower)\\n                        unique_dedicatees.append(name)\\n                \\n                # Step 6: Create ONE relationship per unique dedicatee\\n                for dedicatee in unique_dedicatees:\\n                    new_rel = rel.copy()\\n                    new_rel['target'] = dedicatee\\n                    processed.append(new_rel)\\n            else:\\n                processed.append(rel)\\n        return processed\",\n        \"validation\": \"Unit tests: (1) 'dedicated to my two children, Osha and Hunter' \u2192 2 rels (Osha, Hunter), (2) 'Soil Handbook to Osha and Hunter' \u2192 2 rels (Osha, Hunter), (3) 'dedicated to osha, Osha, OSHA' \u2192 1 rel (Osha)\"\n      }\n    },\n    {\n      \"operation_id\": \"change_002\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/praise_quote_corrector.py\",\n      \"priority\": \"CRITICAL\",\n      \"rationale\": \"Praise quote corrector is converting 'authored' to 'endorsed' even when the source is the book's author (3 HIGH issues, 0.35%). Need author-check logic before conversion.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Praise Quote Misattribution\",\n      \"expected_improvement\": \"Fixes all 3 HIGH issues related to author misattribution.\",\n\n      \"edit_details\": {\n        \"target_function\": \"PraiseQuoteCorrector.process_batch\",\n        \"old_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Correct praise quote relationships.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            # Check if this looks like a praise quote\\n            if self._is_praise_quote(rel):\\n                # Convert 'authored' to 'endorsed'\\n                if rel['relationship'].lower() == 'authored':\\n                    rel['relationship'] = 'endorsed'\\n            processed.append(rel)\\n        return processed\",\n        \"new_content\": \"    def __init__(self, book_metadata: Dict = None):\\n        \\\"\\\"\\\"Initialize with book metadata for author checking.\\\"\\\"\\\"\\n        self.book_metadata = book_metadata or {}\\n        self.book_authors = self._extract_authors()\\n    \\n    def _extract_authors(self) -> Set[str]:\\n        \\\"\\\"\\\"Extract author names from metadata.\\\"\\\"\\\"\\n        authors = set()\\n        if 'author' in self.book_metadata:\\n            author_str = self.book_metadata['author']\\n            # Handle multiple authors (comma or 'and' separated)\\n            parts = re.split(r'\\\\s*[,&]\\\\s*|\\\\s+and\\\\s+', author_str, flags=re.IGNORECASE)\\n            for part in parts:\\n                cleaned = part.strip().lower()\\n                if cleaned:\\n                    authors.add(cleaned)\\n        return authors\\n    \\n    def _is_book_author(self, entity_name: str) -> bool:\\n        \\\"\\\"\\\"Check if entity is the book's author.\\\"\\\"\\\"\\n        entity_lower = entity_name.strip().lower()\\n        # Check exact match or last name match\\n        for author in self.book_authors:\\n            if entity_lower == author or entity_lower in author or author in entity_lower:\\n                return True\\n        return False\\n    \\n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Correct praise quote relationships with author checking.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            # Check if this looks like a praise quote\\n            if self._is_praise_quote(rel):\\n                # Only convert 'authored' to 'endorsed' if source is NOT the book's author\\n                if rel['relationship'].lower() == 'authored':\\n                    source = rel.get('source', '')\\n                    if not self._is_book_author(source):\\n                        rel['relationship'] = 'endorsed'\\n                    # If source IS the author, keep as 'authored'\\n            processed.append(rel)\\n        return processed\",\n        \"validation\": \"Test cases: (1) Aaron Perry + 'authored' + praise context \u2192 keep 'authored', (2) External reviewer + 'authored' + praise context \u2192 change to 'endorsed'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_003\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/pronoun_resolver.py\",\n      \"priority\": \"HIGH\",\n      \"rationale\": \"Pronoun resolver only handles subject pronouns (he/she/we) but not possessive forms (my/our/their), causing 8 HIGH issues (0.93%). Need to extend pattern matching and add context-based resolution for possessives.\",\n      \"risk_level\": \"medium\",\n      \"affected_issue_category\": \"Possessive Pronoun Sources (Unresolved)\",\n      \"expected_improvement\": \"Fixes 8 HIGH issues related to possessive pronouns. Reduces issue rate by 0.93 percentage points.\",\n\n      \"edit_details\": {\n        \"target_function\": \"PronounResolver.__init__ and process_batch\",\n        \"old_content\": \"    def __init__(self):\\n        self.pronoun_patterns = [\\n            r'\\\\b(he|she|it|they|we)\\\\b',\\n        ]\\n    \\n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        processed = []\\n        for rel in relationships:\\n            source = rel['source']\\n            if self._is_pronoun(source):\\n                resolved = self._resolve_pronoun(source, rel)\\n                rel['source'] = resolved\\n            processed.append(rel)\\n        return processed\",\n        \"new_content\": \"    def __init__(self, document_context: Dict = None):\\n        \\\"\\\"\\\"Initialize with document context for possessive resolution.\\\"\\\"\\\"\\n        self.document_context = document_context or {}\\n        \\n        # Subject pronouns\\n        self.subject_pronouns = r'\\\\b(he|she|it|they|we|we humans)\\\\b'\\n        \\n        # Possessive pronouns and phrases\\n        self.possessive_patterns = [\\n            r'\\\\b(my|our|their|his|her|its)\\\\s+(\\\\w+)\\\\b',  # \\\"my people\\\", \\\"our land\\\"\\n            r'\\\\b(my|our|their|his|her|its)\\\\b',  # standalone possessives\\n        ]\\n        \\n        # Extract context clues from document metadata\\n        self.author_ethnicity = self._extract_ethnicity()\\n        self.author_nationality = self._extract_nationality()\\n    \\n    def _extract_ethnicity(self) -> str:\\n        \\\"\\\"\\\"Extract author's ethnicity from document context.\\\"\\\"\\\"\\n        # Look for ethnic/cultural references in metadata or early text\\n        context_text = self.document_context.get('foreword', '') + self.document_context.get('introduction', '')\\n        \\n        # Common patterns: \\\"As a [ethnicity]\\\", \\\"My [ethnicity] heritage\\\", geographic references\\n        ethnic_patterns = [\\n            (r'\\\\b(Slovenian|Slovene)\\\\b', 'Slovenians'),\\n            (r'\\\\b(Italian)\\\\b', 'Italians'),\\n            (r'\\\\b(German)\\\\b', 'Germans'),\\n            (r'\\\\bSlovenia\\\\b', 'Slovenians'),\\n            (r'\\\\bAdriatic Sea.*Alps\\\\b', 'Slovenians'),  # Geographic clue\\n        ]\\n        \\n        for pattern, ethnicity in ethnic_patterns:\\n            if re.search(pattern, context_text, re.IGNORECASE):\\n                return ethnicity\\n        \\n        return None\\n    \\n    def _extract_nationality(self) -> str:\\n        \\\"\\\"\\\"Extract author's nationality from document context.\\\"\\\"\\\"\\n        context_text = self.document_context.get('metadata', {}).get('author_bio', '')\\n        # Similar pattern matching for nationality\\n        return None\\n    \\n    def _resolve_possessive(self, possessive_phrase: str, relationship: Dict) -> str:\\n        \\\"\\\"\\\"Resolve possessive pronouns to specific entities.\\\"\\\"\\\"\\n        phrase_lower = possessive_phrase.lower().strip()\\n        \\n        # Pattern: \\\"my people\\\" / \\\"our people\\\"\\n        if 'people' in phrase_lower:\\n            if self.author_ethnicity:\\n                return self.author_ethnicity\\n            return 'people'  # Generic fallback\\n        \\n        # Pattern: \\\"my/our land\\\" / \\\"my/our countryside\\\"\\n        if any(word in phrase_lower for word in ['land', 'countryside', 'country', 'homeland']):\\n            if self.author_nationality:\\n                return f\\\"{self.author_nationality} countryside\\\"\\n            if self.author_ethnicity:\\n                return f\\\"{self.author_ethnicity} homeland\\\"\\n            return 'homeland'  # Generic fallback\\n        \\n        # Pattern: \\\"my/our children\\\" / \\\"my/our family\\\"\\n        if any(word in phrase_lower for word in ['children', 'family', 'relatives']):\\n            author = self.document_context.get('metadata', {}).get('author', 'author')\\n            return f\\\"{author}'s family\\\"\\n        \\n        # Pattern: \\\"we humans\\\" / \\\"we\\\" (first-person plural)\\n        if phrase_lower in ['we', 'we humans', 'us', 'us humans']:\\n            return 'humanity'\\n        \\n        # Default: return original if can't resolve\\n        return possessive_phrase\\n    \\n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Resolve both subject and possessive pronouns.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            source = rel['source']\\n            target = rel['target']\\n            \\n            # Check for possessive pronouns in source\\n            for pattern in self.possessive_patterns:\\n                if re.search(pattern, source, re.IGNORECASE):\\n                    resolved = self._resolve_possessive(source, rel)\\n                    rel['source'] = resolved\\n                    break\\n            \\n            # Check for subject pronouns in source\\n            if re.search(self.subject_pronouns, source, re.IGNORECASE):\\n                resolved = self._resolve_possessive(source, rel)  # Reuse logic\\n                rel['source'] = resolved\\n            \\n            # Also check target for possessive phrases\\n            for pattern in self.possessive_patterns:\\n                if re.search(pattern, target, re.IGNORECASE):\\n                    resolved = self._resolve_possessive(target, rel)\\n                    rel['target'] = resolved\\n                    break\\n            \\n            processed.append(rel)\\n        return processed\",\n        \"validation\": \"Test cases: (1) 'my people' + Slovenia context \u2192 'Slovenians', (2) 'our countryside' + Slovenia context \u2192 'Slovenian countryside', (3) 'we humans' \u2192 'humanity', (4) 'my children' \u2192 '[Author]'s family'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_004\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass1_extraction_v10.txt\",\n      \"priority\": \"HIGH\",\n      \"rationale\": \"Pass 1 extraction lacks guidance on avoiding philosophical/normative statements, causing 18 MEDIUM issues (2.1%). Adding explicit instructions with examples will prevent extraction of non-factual claims.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Philosophical Claims as Factual\",\n      \"expected_improvement\": \"Reduces 18 MEDIUM issues by 80% (18\u21924). Prevents extraction of philosophical abstractions.\",\n\n      \"edit_details\": {\n        \"target_section\": \"After entity extraction rules, before output format\",\n        \"old_content\": \"## \ud83d\udcdd OUTPUT FORMAT ##\",\n        \"new_content\": \"## \u26a0\ufe0f CRITICAL EXCLUSIONS: DO NOT EXTRACT ##\\n\\n**1. PHILOSOPHICAL & NORMATIVE STATEMENTS**\\n\\nDo NOT extract philosophical, normative, or value judgments as factual relationships. These express opinions, values, or beliefs\u2014not verifiable facts.\\n\\n**Patterns to AVOID:**\\n- \\\"X is what it means to be Y\\\" (philosophical essence claims)\\n  \u274c BAD: (being connected to land, is what it means to be, human)\\n  \\n- \\\"X is the answer/key/solution\\\" (metaphorical abstractions)\\n  \u274c BAD: (soil, is-a, answer)\\n  \\n- \\\"We should/must do X\\\" (normative prescriptions)\\n  \u274c BAD: (we, should practice, regenerative agriculture)\\n  \\n- \\\"It is essential/important that X\\\" (value judgments)\\n  \u274c BAD: (it, is essential, to protect soil)\\n\\n**Why avoid these?** They express the author's philosophy or values, not objective facts about the world. A knowledge graph should contain verifiable claims, not opinions.\\n\\n**What to extract instead:**\\n\u2705 GOOD: (regenerative agriculture, increases, soil carbon)\\n\u2705 GOOD: (soil degradation, threatens, food security)\\n\u2705 GOOD: (author, advocates for, soil stewardship)\\n\\n**2. OVERLY ABSTRACT ENTITIES**\\n\\nAvoid extracting vague philosophical concepts as entities:\\n- \\\"the answer\\\", \\\"the way\\\", \\\"the solution\\\", \\\"the key\\\"\\n- \\\"great crossroads\\\", \\\"turning point\\\", \\\"moment of truth\\\"\\n- \\\"essence of X\\\", \\\"meaning of X\\\", \\\"nature of X\\\"\\n\\n**3. FIGURATIVE LANGUAGE & METAPHORS**\\n\\nDo NOT extract metaphorical statements as literal facts:\\n\\n\u274c BAD: (our land, is, veritable Eden)\\n  \u2192 \\\"Eden\\\" is a metaphor for beauty/abundance, not a literal claim\\n\\n\u274c BAD: (humanity, stands at, crossroads)\\n  \u2192 \\\"crossroads\\\" is a metaphor for decision point, not a physical location\\n\\n\u274c BAD: (soil, is, miracle)\\n  \u2192 \\\"miracle\\\" is figurative praise, not a factual classification\\n\\n**How to handle metaphors:**\\n- If the underlying literal claim is clear, extract that:\\n  \u2705 GOOD: (Slovenia, has, pristine landscapes) [from \\\"veritable Eden\\\"]\\n  \u2705 GOOD: (humanity, faces, critical environmental decisions) [from \\\"crossroads\\\"]\\n  \\n- If no clear literal claim, skip the relationship entirely\\n\\n**4. RHETORICAL QUESTIONS**\\n\\nDo NOT extract relationships from rhetorical questions:\\n\u274c BAD: (we, can afford to ignore, soil health) [from \\\"Can we afford to ignore soil health?\\\"]\\n\\n## \ud83d\udcdd OUTPUT FORMAT ##\",\n        \"validation\": \"Test prompt on V9 philosophical examples: 'Soil is the answer', 'being connected to land is what it means to be human', 'our land is veritable Eden'. Confirm these are NOT extracted.\",\n        \"prompt_version\": \"v10\"\n      }\n    },\n    {\n      \"operation_id\": \"change_005\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass1_extraction_v10.txt\",\n      \"priority\": \"HIGH\",\n      \"rationale\": \"Pass 1 extraction allows generic entities ('the land', 'the answer') instead of specific ones, causing 6 HIGH + 8 MEDIUM issues (1.63%). Adding specificity guidance will improve entity quality.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Demonstrative Pronoun Targets (Unresolved) + Abstract Vague Entities\",\n      \"expected_improvement\": \"Reduces 14 issues by 70% (14\u21924). Improves entity specificity and usefulness.\",\n\n      \"edit_details\": {\n        \"target_section\": \"In entity extraction rules section\",\n        \"old_content\": \"## \ud83c\udfaf ENTITY EXTRACTION RULES ##\",\n        \"new_content\": \"## \ud83c\udfaf ENTITY EXTRACTION RULES ##\\n\\n**ENTITY SPECIFICITY: Prefer Specific Over Generic**\\n\\nAlways extract the MOST SPECIFIC entity that the text supports. Use document context to add specificity.\\n\\n**Generic \u2192 Specific Examples:**\\n\\n1. Geographic entities:\\n   \u274c GENERIC: \\\"the land\\\"\\n   \u2705 SPECIFIC: \\\"Slovenian countryside\\\" (if context mentions Slovenia)\\n   \u2705 SPECIFIC: \\\"agricultural land\\\" (if discussing farming)\\n   \\n2. Substances:\\n   \u274c GENERIC: \\\"carbon\\\"\\n   \u2705 SPECIFIC: \\\"atmospheric carbon dioxide\\\"\\n   \u2705 SPECIFIC: \\\"soil organic carbon\\\"\\n   \\n3. Practices:\\n   \u274c GENERIC: \\\"farming\\\"\\n   \u2705 SPECIFIC: \\\"regenerative agriculture\\\"\\n   \u2705 SPECIFIC: \\\"no-till farming\\\"\\n   \\n4. Groups:\\n   \u274c GENERIC: \\\"people\\\"\\n   \u2705 SPECIFIC: \\\"Slovenians\\\" (if ethnic context is clear)\\n   \u2705 SPECIFIC: \\\"farmers\\\" (if discussing agricultural community)\\n   \\n5. Concepts:\\n   \u274c GENERIC: \\\"the process\\\"\\n   \u2705 SPECIFIC: \\\"carbon sequestration process\\\"\\n   \u2705 SPECIFIC: \\\"composting process\\\"\\n\\n**How to add specificity:**\\n- Look at surrounding sentences for context\\n- Use adjectives/modifiers from the text\\n- Reference geographic, temporal, or domain context\\n- If text says \\\"the soil\\\", check if it means \\\"agricultural soil\\\", \\\"forest soil\\\", \\\"degraded soil\\\", etc.\\n\\n**When generic is acceptable:**\\n- When the text genuinely refers to something universal (\\\"water\\\", \\\"oxygen\\\")\\n- When no additional context is available\\n- When the generic term IS the specific concept being discussed\\n\\n## \ud83c\udfaf ENTITY EXTRACTION RULES (continued) ##\",\n        \"validation\": \"Test on V9 examples: 'my people love the land' should extract 'Slovenians love Slovenian countryside', not 'my people love the land'\",\n        \"prompt_version\": \"v10\"\n      }\n    },\n    {\n      \"operation_id\": \"change_006\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass1_extraction_v10.txt\",\n      \"priority\": \"MEDIUM\",\n      \"rationale\": \"Pass 1 extraction accepts vague predicates ('is characterized by', 'allows us to'), causing 8 MEDIUM issues (0.93%). Adding predicate specificity guidance will improve relationship clarity.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Overly Abstract Relationship Predicates\",\n      \"expected_improvement\": \"Reduces 8 MEDIUM issues by 75% (8\u21922).\",\n\n      \"edit_details\": {\n        \"target_section\": \"In relationship extraction rules section\",\n        \"old_content\": \"## \ud83d\udd17 RELATIONSHIP EXTRACTION RULES ##\",\n        \"new_content\": \"## \ud83d\udd17 RELATIONSHIP EXTRACTION RULES ##\\n\\n**PREDICATE SPECIFICITY: Use Precise, Domain-Relevant Verbs**\\n\\nChoose the MOST SPECIFIC predicate that accurately captures the relationship.\\n\\n**Vague \u2192 Specific Examples:**\\n\\n1. \u274c VAGUE: \\\"is characterized by\\\"\\n   \u2705 SPECIFIC: \\\"contains\\\", \\\"produces\\\", \\\"exhibits\\\", \\\"includes\\\"\\n   Example: (soil, contains, organic matter) NOT (soil, is characterized by, organic matter)\\n   \\n2. \u274c VAGUE: \\\"allows us to\\\"\\n   \u2705 SPECIFIC: \\\"enables\\\", \\\"facilitates\\\", \\\"supports\\\"\\n   Example: (composting, enables, carbon sequestration) NOT (composting, allows us to, sequester carbon)\\n   \\n3. \u274c VAGUE: \\\"leads to\\\"\\n   \u2705 SPECIFIC: \\\"causes\\\", \\\"produces\\\", \\\"results in\\\", \\\"increases\\\", \\\"decreases\\\"\\n   Example: (tillage, causes, soil erosion) NOT (tillage, leads to, erosion)\\n   \\n4. \u274c VAGUE: \\\"is related to\\\"\\n   \u2705 SPECIFIC: \\\"affects\\\", \\\"influences\\\", \\\"depends on\\\", \\\"correlates with\\\"\\n   Example: (soil pH, affects, nutrient availability) NOT (soil pH, is related to, nutrients)\\n   \\n5. \u274c VAGUE: \\\"has\\\"\\n   \u2705 SPECIFIC: \\\"contains\\\", \\\"possesses\\\", \\\"exhibits\\\", \\\"produces\\\"\\n   Example: (healthy soil, contains, beneficial microbes) NOT (healthy soil, has, microbes)\\n\\n**Preferred predicate categories:**\\n- **Physical actions**: contains, produces, absorbs, releases, stores\\n- **Causal**: causes, increases, decreases, prevents, enables\\n- **Compositional**: consists of, includes, comprises\\n- **Functional**: supports, facilitates, regulates, maintains\\n- **Spatial**: located in, found in, distributed across\\n- **Temporal**: precedes, follows, occurs during\\n\\n**Avoid overly abstract predicates:**\\n- \\\"is characterized by\\\" \u2192 too vague\\n- \\\"is associated with\\\" \u2192 too vague\\n- \\\"is related to\\\" \u2192 too vague\\n- \\\"involves\\\" \u2192 too vague (unless genuinely the best fit)\\n\\n## \ud83d\udd17 RELATIONSHIP EXTRACTION RULES (continued) ##\",\n        \"validation\": \"Test on V9 examples: 'great crossroads is characterized by immense complexity' should become 'environmental situation involves complex challenges' or be skipped\",\n        \"prompt_version\": \"v10\"\n      }\n    },\n    {\n      \"operation_id\": \"change_007\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/list_splitter.py\",\n      \"priority\": \"MEDIUM\",\n      \"rationale\": \"List splitter is mechanically splitting adjective lists and short conjunctions, creating noise (12 MEDIUM issues, 1.4%). Adding a 'trivial split' filter will preserve valuable splits while removing noise.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"List Splitting Creating Noise\",\n      \"expected_improvement\": \"Reduces 12 MEDIUM issues by 90% (12\u21921). Improves signal-to-noise ratio.\",\n\n      \"edit_details\": {\n        \"target_function\": \"ListSplitter.process_batch\",\n        \"old_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        processed = []\\n        for rel in relationships:\\n            target = rel['target']\\n            # Split on commas and 'and'\\n            if ',' in target or ' and ' in target:\\n                parts = re.split(r',|\\\\band\\\\b', target)\\n                for part in parts:\\n                    new_rel = rel.copy()\\n                    new_rel['target'] = part.strip()\\n                    processed.append(new_rel)\\n            else:\\n                processed.append(rel)\\n        return processed\",\n        \"new_content\": \"    def __init__(self):\\n        # POS tagger for adjective detection (simple heuristic version)\\n        self.adjective_suffixes = ['able', 'ible', 'al', 'ful', 'ic', 'ive', 'less', 'ous', 'y']\\n        self.common_adjectives = set(['timely', 'empowering', 'important', 'essential', 'critical', \\n                                      'valuable', 'useful', 'practical', 'informative', 'comprehensive'])\\n    \\n    def _is_adjective(self, word: str) -> bool:\\n        \\\"\\\"\\\"Simple heuristic to detect if word is likely an adjective.\\\"\\\"\\\"\\n        word_lower = word.lower().strip()\\n        # Check common adjectives\\n        if word_lower in self.common_adjectives:\\n            return True\\n        # Check suffixes\\n        return any(word_lower.endswith(suffix) for suffix in self.adjective_suffixes)\\n    \\n    def _is_trivial_split(self, parts: List[str], relationship: Dict) -> bool:\\n        \\\"\\\"\\\"Determine if splitting would create trivial/noisy relationships.\\\"\\\"\\\"\\n        # Filter 1: Only 2 items connected by 'and' (often not worth splitting)\\n        if len(parts) == 2:\\n            # Exception: If both are proper nouns (names), DO split\\n            if all(part.strip() and part.strip()[0].isupper() for part in parts):\\n                return False  # Not trivial - these are likely names\\n            # Otherwise, check if trivial\\n            return True\\n        \\n        # Filter 2: All items are adjectives (descriptive list, not entity list)\\n        if len(parts) >= 2:\\n            adjective_count = sum(1 for part in parts if self._is_adjective(part.strip()))\\n            if adjective_count == len(parts):\\n                return True  # All adjectives - trivial split\\n        \\n        # Filter 3: In praise quote context (check evidence text for praise indicators)\\n        evidence = relationship.get('evidence_text', '').lower()\\n        praise_indicators = ['timely', 'empowering', 'important', 'essential', 'valuable', \\n                            'informative', 'comprehensive', 'excellent', 'outstanding']\\n        if any(indicator in evidence for indicator in praise_indicators):\\n            # If this is a praise quote and items are adjectives, it's trivial\\n            if all(self._is_adjective(part.strip()) for part in parts):\\n                return True\\n        \\n        # Filter 4: Very short items (1-2 characters) - likely noise\\n        if any(len(part.strip()) <= 2 for part in parts):\\n            return True\\n        \\n        return False  # Not trivial - proceed with split\\n    \\n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Split lists with trivial split filtering.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            target = rel['target']\\n            # Check if target contains list separators\\n            if ',' in target or ' and ' in target.lower():\\n                # Split on commas and 'and'\\n                parts = re.split(r'\\\\s*,\\\\s*|\\\\s+and\\\\s+', target, flags=re.IGNORECASE)\\n                parts = [p.strip() for p in parts if p.strip()]\\n                \\n                # Check if this is a trivial split\\n                if self._is_trivial_split(parts, rel):\\n                    # Don't split - keep original\\n                    processed.append(rel)\\n                else:\\n                    # Split is valuable - create separate relationships\\n                    for part in parts:\\n                        new_rel = rel.copy()\\n                        new_rel['target'] = part\\n                        processed.append(new_rel)\\n            else:\\n                processed.append(rel)\\n        return processed\",\n        \"validation\": \"Test cases: (1) 'timely and empowering' \u2192 DON'T split (adjectives), (2) 'Osha and Hunter' \u2192 DO split (proper names), (3) 'carbon sequestration and soil health' \u2192 DO split (concepts), (4) 'a, b' \u2192 DON'T split (too short)\"\n      }\n    },\n    {\n      \"operation_id\": \"change_008\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass2_evaluation_v10.txt\",\n      \"priority\": \"MEDIUM\",\n      \"rationale\": \"Pass 2 evaluation doesn't flag philosophical claims, allowing them to pass with high confidence (18 MEDIUM issues). Adding philosophical claim detection will correctly classify these relationships.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Philosophical Claims as Factual\",\n      \"expected_improvement\": \"Improves classification of 18 MEDIUM issues. Doesn't remove them but flags them for filtering.\",\n\n      \"edit_details\": {\n        \"target_section\": \"In knowledge_plausibility evaluation criteria\",\n        \"old_content\": \"### Knowledge Plausibility (0.0-1.0)\",\n        \"new_content\": \"### Knowledge Plausibility (0.0-1.0)\\n\\n**PHILOSOPHICAL CLAIM DETECTION**\\n\\nBefore scoring knowledge_plausibility, check if the relationship expresses a philosophical, normative, or value judgment rather than a verifiable fact.\\n\\n**Indicators of philosophical claims:**\\n1. \\\"X is what it means to be Y\\\" (essence/definition claims)\\n2. \\\"X is the answer/key/solution\\\" (metaphorical abstractions)\\n3. \\\"We should/must do X\\\" (normative prescriptions)\\n4. \\\"It is essential/important/critical that X\\\" (value judgments)\\n5. \\\"X is the nature/essence of Y\\\" (ontological claims)\\n6. Abstract entities: \\\"the answer\\\", \\\"the way\\\", \\\"the solution\\\", \\\"great crossroads\\\"\\n\\n**Scoring for philosophical claims:**\\n- If relationship is clearly philosophical/normative:\\n  - knowledge_plausibility: 0.3-0.5 (low to medium)\\n  - Add flag: \\\"PHILOSOPHICAL_CLAIM\\\" or \\\"NORMATIVE_STATEMENT\\\"\\n  - Reasoning: \\\"This expresses a philosophical/value judgment, not a verifiable fact\\\"\\n\\n**Examples:**\\n- (being connected to land, is what it means to be, human)\\n  \u2192 knowledge_plausibility: 0.4, flag: PHILOSOPHICAL_CLAIM\\n  \\n- (soil, is-a, answer)\\n  \u2192 knowledge_plausibility: 0.3, flag: METAPHORICAL_ABSTRACTION\\n  \\n- (we, should practice, regenerative agriculture)\\n  \u2192 knowledge_plausibility: 0.4, flag: NORMATIVE_STATEMENT\\n\\n**Factual claims should score higher:**\\n- (regenerative agriculture, increases, soil carbon)\\n  \u2192 knowledge_plausibility: 0.9 (verifiable scientific claim)\\n\\n### Knowledge Plausibility (0.0-1.0) - continued\",\n        \"validation\": \"Test on V9 philosophical examples. Confirm they receive low knowledge_plausibility scores (0.3-0.5) and appropriate flags.\",\n        \"prompt_version\": \"v10\"\n      }\n    },\n    {\n      \"operation_id\": \"change_009\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/entity_cleaner.py\",\n      \"priority\": \"LOW\",\n      \"rationale\": \"Entity cleaner not stripping trailing punctuation, causing 3 MILD issues (0.35%). Simple regex fix.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Trailing Punctuation in Entities\",\n      \"expected_improvement\": \"Fixes all 3 MILD issues related to trailing punctuation.\",\n\n      \"edit_details\": {\n        \"target_function\": \"EntityCleaner.clean_entity\",\n        \"old_content\": \"    def clean_entity(self, entity: str) -> str:\\n        \\\"\\\"\\\"Clean entity name.\\\"\\\"\\\"\\n        # Strip whitespace\\n        cleaned = entity.strip()\\n        # Remove extra spaces\\n        cleaned = re.sub(r'\\\\s+', ' ', cleaned)\\n        return cleaned\",\n        \"new_content\": \"    def clean_entity(self, entity: str) -> str:\\n        \\\"\\\"\\\"Clean entity name.\\\"\\\"\\\"\\n        # Strip whitespace\\n        cleaned = entity.strip()\\n        # Remove trailing punctuation (periods, commas, semicolons, etc.)\\n        cleaned = re.sub(r'[.,;:!?]+$', '', cleaned)\\n        # Remove leading punctuation (quotes, parentheses)\\n        cleaned = re.sub(r'^[\\\"\\\\'\\\\'\\\\`\\\\(\\\\[]+', '', cleaned)\\n        # Remove extra spaces\\n        cleaned = re.sub(r'\\\\s+', ' ', cleaned)\\n        # Final strip\\n        cleaned = cleaned.strip()\\n        return cleaned\",\n        \"validation\": \"Test cases: 'hunter.' \u2192 'hunter', 'Osha,' \u2192 'Osha', '\\\"quoted entity\\\"' \u2192 'quoted entity'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_010\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/deduplicator.py\",\n      \"priority\": \"LOW\",\n      \"rationale\": \"Deduplicator is case-sensitive, causing 4 MILD issues (0.47%). Adding case normalization before deduplication will fix this.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Duplicate Relationships (Case Sensitivity)\",\n      \"expected_improvement\": \"Fixes all 4 MILD issues related to case-sensitive duplicates.\",\n\n      \"edit_details\": {\n        \"target_function\": \"Deduplicator.process_batch\",\n        \"old_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Remove duplicate relationships.\\\"\\\"\\\"\\n        seen = set()\\n        deduplicated = []\\n        for rel in relationships:\\n            key = (rel['source'], rel['relationship'], rel['target'])\\n            if key not in seen:\\n                seen.add(key)\\n                deduplicated.append(rel)\\n        return deduplicated\",\n        \"new_content\": \"    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Remove duplicate relationships with case-insensitive matching.\\\"\\\"\\\"\\n        seen = set()\\n        deduplicated = []\\n        for rel in relationships:\\n            # Normalize case for comparison (use title case for proper nouns)\\n            source_normalized = rel['source'].strip().title()\\n            target_normalized = rel['target'].strip().title()\\n            relationship_normalized = rel['relationship'].strip().lower()\\n            \\n            # Create case-insensitive key\\n            key = (source_normalized, relationship_normalized, target_normalized)\\n            \\n            if key not in seen:\\n                seen.add(key)\\n                # Update relationship with normalized entities (title case)\\n                rel['source'] = source_normalized\\n                rel['target'] = target_normalized\\n                rel['relationship'] = relationship_normalized\\n                deduplicated.append(rel)\\n        return deduplicated\",\n        \"validation\": \"Test cases: ('Aaron Perry', 'dedicated', 'osha') + ('Aaron Perry', 'dedicated', 'Osha') \u2192 1 relationship with 'Osha'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_011\",\n      \"operation_type\": \"CODE_FIX\",\n      \"file_path\": \"modules/pass2_5_postprocessing/type_validator.py\",\n      \"priority\": \"LOW\",\n      \"rationale\": \"Type validator is too permissive with 'Abstract Concept' type, causing 8 MILD issues (0.93%). Tightening rules will improve entity typing accuracy.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Incorrect Entity Types\",\n      \"expected_improvement\": \"Fixes 8 MILD issues related to incorrect entity types.\",\n\n      \"edit_details\": {\n        \"target_function\": \"TypeValidator.validate_type\",\n        \"old_content\": \"    def validate_type(self, entity: str, entity_type: str) -> str:\\n        \\\"\\\"\\\"Validate and correct entity type.\\\"\\\"\\\"\\n        # If type is already specific, keep it\\n        if entity_type in ['Person', 'Organization', 'Location', 'Date']:\\n            return entity_type\\n        # Default to Abstract Concept\\n        return 'Abstract Concept'\",\n        \"new_content\": \"    def validate_type(self, entity: str, entity_type: str) -> str:\\n        \\\"\\\"\\\"Validate and correct entity type with stricter rules.\\\"\\\"\\\"\\n        entity_lower = entity.lower().strip()\\n        \\n        # Rule 1: People/ethnic groups (not abstract concepts)\\n        people_indicators = ['people', 'humans', 'humanity', 'community', 'population', \\n                           'slovenians', 'americans', 'farmers', 'children', 'family']\\n        if any(indicator in entity_lower for indicator in people_indicators):\\n            if 'family' in entity_lower or 'children' in entity_lower:\\n                return 'Family/Group'\\n            return 'People/Ethnic Group'\\n        \\n        # Rule 2: Geographic locations (not abstract concepts)\\n        location_indicators = ['land', 'countryside', 'country', 'region', 'area', \\n                             'sea', 'ocean', 'mountain', 'valley', 'forest', 'soil']\\n        if any(indicator in entity_lower for indicator in location_indicators):\\n            return 'Geographic Feature'\\n        \\n        # Rule 3: Practices/processes (not abstract concepts)\\n        practice_indicators = ['agriculture', 'farming', 'composting', 'tillage', \\n                             'practice', 'method', 'technique', 'process']\\n        if any(indicator in entity_lower for indicator in practice_indicators):\\n            return 'Practice/Process'\\n        \\n        # Rule 4: Substances/materials (not abstract concepts)\\n        substance_indicators = ['carbon', 'nitrogen', 'water', 'soil', 'organic matter', \\n                              'nutrient', 'mineral', 'element']\\n        if any(indicator in entity_lower for indicator in substance_indicators):\\n            return 'Substance/Material'\\n        \\n        # Rule 5: Organizations/institutions\\n        org_indicators = ['organization', 'network', 'institute', 'foundation', \\n                        'company', 'agency', 'department']\\n        if any(indicator in entity_lower for indicator in org_indicators):\\n            return 'Organization'\\n        \\n        # Rule 6: Publications/documents\\n        pub_indicators = ['handbook', 'book', 'guide', 'manual', 'report', 'study']\\n        if any(indicator in entity_lower for indicator in pub_indicators):\\n            return 'Publication'\\n        \\n        # Rule 7: Only use 'Abstract Concept' for truly abstract philosophical concepts\\n        abstract_indicators = ['essence', 'nature', 'meaning', 'answer', 'solution', \\n                             'key', 'way', 'path', 'crossroads', 'miracle']\\n        if any(indicator in entity_lower for indicator in abstract_indicators):\\n            return 'Abstract Concept'\\n        \\n        # If type is already specific and not 'Abstract Concept', keep it\\n        if entity_type and entity_type != 'Abstract Concept':\\n            return entity_type\\n        \\n        # Default: use provided type or 'Unclassified'\\n        return entity_type if entity_type else 'Unclassified'\",\n        \"validation\": \"Test cases: 'my people' \u2192 'People/Ethnic Group' (not 'Abstract Concept'), 'the land' \u2192 'Geographic Feature', 'the answer' \u2192 'Abstract Concept'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_012\",\n      \"operation_type\": \"NEW_MODULE\",\n      \"file_path\": \"modules/pass2_5_postprocessing/first_person_resolver.py\",\n      \"priority\": \"LOW\",\n      \"rationale\": \"First-person plural pronouns ('we humans', 'we have the choice') not being resolved, causing 7 MILD issues (0.81%). Creating dedicated module for first-person resolution.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Pronoun Sources in First-Person Statements\",\n      \"expected_improvement\": \"Fixes 7 MILD issues related to first-person pronouns.\",\n\n      \"create_details\": {\n        \"module_name\": \"FirstPersonResolver\",\n        \"class_template\": \"PostProcessingModule\",\n        \"dependencies\": [\"re\", \"typing\"],\n        \"content\": \"import re\\nfrom typing import List, Dict\\n\\nclass FirstPersonResolver:\\n    \\\"\\\"\\\"Resolve first-person plural pronouns to more specific entities.\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        # First-person plural patterns\\n        self.first_person_patterns = [\\n            r'\\\\bwe humans\\\\b',\\n            r'\\\\bwe\\\\b',\\n            r'\\\\bus humans\\\\b',\\n            r'\\\\bus\\\\b',\\n        ]\\n    \\n    def _resolve_first_person(self, entity: str) -> str:\\n        \\\"\\\"\\\"Resolve first-person plural to generic human referent.\\\"\\\"\\\"\\n        entity_lower = entity.lower().strip()\\n        \\n        # Pattern: \\\"we humans\\\" \u2192 \\\"humanity\\\"\\n        if entity_lower in ['we humans', 'us humans']:\\n            return 'humanity'\\n        \\n        # Pattern: \\\"we\\\" / \\\"us\\\" \u2192 \\\"people\\\" or \\\"humanity\\\"\\n        if entity_lower in ['we', 'us']:\\n            return 'humanity'\\n        \\n        return entity\\n    \\n    def process_batch(self, relationships: List[Dict]) -> List[Dict]:\\n        \\\"\\\"\\\"Resolve first-person pronouns in relationships.\\\"\\\"\\\"\\n        processed = []\\n        for rel in relationships:\\n            source = rel['source']\\n            target = rel['target']\\n            \\n            # Check source for first-person pronouns\\n            for pattern in self.first_person_patterns:\\n                if re.search(pattern, source, re.IGNORECASE):\\n                    rel['source'] = self._resolve_first_person(source)\\n                    break\\n            \\n            # Check target for first-person pronouns\\n            for pattern in self.first_person_patterns:\\n                if re.search(pattern, target, re.IGNORECASE):\\n                    rel['target'] = self._resolve_first_person(target)\\n                    break\\n            \\n            processed.append(rel)\\n        return processed\",\n        \"integration_point\": \"orchestrator.py:pass_2_5_quality_post_processing - add after PronounResolver\",\n        \"validation\": \"Test cases: 'we humans' \u2192 'humanity', 'we' \u2192 'humanity', 'us' \u2192 'humanity'\"\n      }\n    },\n    {\n      \"operation_id\": \"change_013\",\n      \"operation_type\": \"CONFIG_UPDATE\",\n      \"file_path\": \"config/pass2_5_orchestrator.json\",\n      \"priority\": \"LOW\",\n      \"rationale\": \"Update orchestrator configuration to include new FirstPersonResolver module in processing pipeline.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Pronoun Sources in First-Person Statements\",\n      \"expected_improvement\": \"Enables FirstPersonResolver module in pipeline.\",\n\n      \"edit_details\": {\n        \"json_path\": \"pass_2_5_modules\",\n        \"old_value\": [\"BibliographicCitationParser\", \"ListSplitter\", \"PronounResolver\", \"ContextEnricher\", \"TitleCompletenessValidator\", \"PredicateValidator\", \"FigurativeLanguageFilter\"],\n        \"new_value\": [\"BibliographicCitationParser\", \"DedicationParser\", \"ListSplitter\", \"PronounResolver\", \"FirstPersonResolver\", \"ContextEnricher\", \"TitleCompletenessValidator\", \"PredicateValidator\", \"FigurativeLanguageFilter\", \"EntityCleaner\", \"Deduplicator\", \"TypeValidator\", \"PraiseQuoteCorrector\"],\n        \"validation\": \"Verify module order: DedicationParser early (fixes malformed rels), EntityCleaner before Deduplicator, PraiseQuoteCorrector near end\"\n      }\n    },\n    {\n      \"operation_id\": \"change_014\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass1_extraction_v10.txt\",\n      \"priority\": \"MEDIUM\",\n      \"rationale\": \"Add explicit examples section showing good vs bad extractions to reinforce all the new rules. Few-shot learning is critical for LLM guidance.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Multiple (reinforces all prompt changes)\",\n      \"expected_improvement\": \"Reinforces all prompt improvements with concrete examples. Estimated 20% additional improvement beyond rule-based changes.\",\n\n      \"edit_details\": {\n        \"target_section\": \"After all rules, before output format\",\n        \"old_content\": \"## \ud83d\udcdd OUTPUT FORMAT ##\",\n        \"new_content\": \"## \u2705 EXTRACTION EXAMPLES: GOOD vs BAD ##\\n\\n**Example 1: Philosophical Statement (AVOID)**\\n\\nText: \\\"Being connected to land and soil is what it means to be human.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(being connected to land and soil, is what it means to be, human)\\n\u2192 Why bad? This is a philosophical claim, not a verifiable fact.\\n\\n\u2705 GOOD: Skip this relationship entirely (no extraction)\\n\\n---\\n\\n**Example 2: Generic vs Specific Entities**\\n\\nText: \\\"My people love the land. We Slovenians have lived here for centuries.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(my people, love, the land)\\n\u2192 Why bad? \\\"my people\\\" is possessive pronoun, \\\"the land\\\" is too generic.\\n\\n\u2705 GOOD EXTRACTION:\\n(Slovenians, love, Slovenian countryside)\\n\u2192 Why good? Specific ethnic group, specific geographic entity (using context).\\n\\n---\\n\\n**Example 3: Metaphorical Language (AVOID)**\\n\\nText: \\\"Soil is the answer to our environmental challenges.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(soil, is-a, answer)\\n\u2192 Why bad? \\\"the answer\\\" is a metaphor, not a literal classification.\\n\\n\u2705 GOOD EXTRACTION:\\n(soil stewardship, addresses, environmental challenges)\\n\u2192 Why good? Extracts the literal underlying claim.\\n\\nOR: Skip entirely if no clear literal claim.\\n\\n---\\n\\n**Example 4: Vague vs Specific Predicates**\\n\\nText: \\\"Healthy soil is characterized by high organic matter content.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(healthy soil, is characterized by, high organic matter content)\\n\u2192 Why bad? \\\"is characterized by\\\" is too vague.\\n\\n\u2705 GOOD EXTRACTION:\\n(healthy soil, contains, high organic matter)\\n\u2192 Why good? \\\"contains\\\" is specific and clear.\\n\\n---\\n\\n**Example 5: Adjective List (DON'T OVER-EXTRACT)**\\n\\nText: \\\"This handbook is timely and empowering.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(Soil Stewardship Handbook, is, timely)\\n(Soil Stewardship Handbook, is, empowering)\\n\u2192 Why bad? Splitting subjective adjectives in praise quote adds noise.\\n\\n\u2705 GOOD EXTRACTION:\\n(Soil Stewardship Handbook, is, timely and empowering)\\n\u2192 Why good? Keep adjective pairs together in descriptive contexts.\\n\\n---\\n\\n**Example 6: Proper Name List (DO SPLIT)**\\n\\nText: \\\"This book is dedicated to my children, Osha and Hunter.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(Aaron Perry, dedicated, Osha and Hunter)\\n\u2192 Why bad? Should split proper names into separate relationships.\\n\\n\u2705 GOOD EXTRACTION:\\n(Aaron Perry, dedicated, Osha)\\n(Aaron Perry, dedicated, Hunter)\\n\u2192 Why good? Each person gets their own relationship.\\n\\n---\\n\\n**Example 7: Normative Statement (AVOID)**\\n\\nText: \\\"We must practice regenerative agriculture to save our planet.\\\"\\n\\n\u274c BAD EXTRACTION:\\n(we, must practice, regenerative agriculture)\\n\u2192 Why bad? This is a normative prescription (\\\"must\\\"), not a factual claim.\\n\\n\u2705 GOOD EXTRACTION:\\n(regenerative agriculture, can help, environmental restoration)\\nOR: (author, advocates for, regenerative agriculture)\\n\u2192 Why good? Extracts factual claim or attribution, not the normative \\\"must\\\".\\n\\n## \ud83d\udcdd OUTPUT FORMAT ##\",\n        \"validation\": \"Review examples for clarity and coverage of all major rule categories\",\n        \"prompt_version\": \"v10\"\n      }\n    },\n    {\n      \"operation_id\": \"change_015\",\n      \"operation_type\": \"PROMPT_ENHANCEMENT\",\n      \"file_path\": \"prompts/pass2_evaluation_v10.txt\",\n      \"priority\": \"MEDIUM\",\n      \"rationale\": \"Add guidance on penalizing vague/abstract entities in knowledge_plausibility scoring to complement philosophical claim detection.\",\n      \"risk_level\": \"low\",\n      \"affected_issue_category\": \"Abstract Vague Entities\",\n      \"expected_improvement\": \"Improves scoring accuracy for 12 MEDIUM issues related to abstract entities.\",\n\n      \"edit_details\": {\n        \"target_section\": \"In knowledge_plausibility criteria, after philosophical claim section\",\n        \"old_content\": \"### Knowledge Plausibility (0.0-1.0) - continued\",\n        \"new_content\": \"**ENTITY SPECIFICITY PENALTY**\\n\\nPenalize relationships with overly vague or abstract entities that provide little useful information.\\n\\n**Vague entity indicators:**\\n- Generic demonstratives: \\\"the answer\\\", \\\"the way\\\", \\\"the solution\\\", \\\"the key\\\"\\n- Vague abstractions: \\\"great crossroads\\\", \\\"turning point\\\", \\\"essence\\\"\\n- Overly generic nouns: \\\"the land\\\" (without context), \\\"the process\\\", \\\"the system\\\"\\n\\n**Scoring adjustment:**\\n- If BOTH source and target are vague: reduce knowledge_plausibility by 0.3\\n- If ONE entity is vague: reduce knowledge_plausibility by 0.15\\n- If entities are specific and contextualized: no penalty\\n\\n**Examples:**\\n\\n\u274c LOW SCORE:\\n(great crossroads, is characterized by, immense complexity)\\n\u2192 knowledge_plausibility: 0.3 (both entities are vague abstractions)\\n\\n\u274c LOW SCORE:\\n(soil, is-a, answer)\\n\u2192 knowledge_plausibility: 0.2 (\\\"answer\\\" is metaphorical abstraction)\\n\\n\u2705 HIGH SCORE:\\n(regenerative agriculture, increases, soil organic carbon)\\n\u2192 knowledge_plausibility: 0.9 (both entities are specific and concrete)\\n\\n\u2705 MEDIUM SCORE:\\n(Slovenians, love, the land)\\n\u2192 knowledge_plausibility: 0.6 (\\\"the land\\\" is somewhat generic, but relationship is clear)\\n\\n### Knowledge Plausibility (0.0-1.0) - continued\",\n        \"validation\": \"Test on V9 abstract entity examples. Confirm vague entities receive appropriate penalties.\",\n        \"prompt_version\": \"v10\"\n      }\n    }\n  ],\n  \"priorities\": {\n    \"immediate\": [\n      \"change_001: Fix dedication parser catastrophic failure (CRITICAL - 12 issues)\",\n      \"change_002: Add author-check to praise quote corrector (CRITICAL - 3 issues)\",\n      \"change_003: Extend pronoun resolver to possessive forms (HIGH - 8 issues)\"\n    ],\n    \"short_term\": [\n      \"change_004: Add philosophical statement exclusions to Pass 1 prompt (HIGH - 18 issues)\",\n      \"change_005: Add entity specificity guidance to Pass 1 prompt (HIGH - 14 issues)\",\n      \"change_006: Add predicate specificity guidance to Pass 1 prompt (MEDIUM - 8 issues)\",\n      \"change_007: Add trivial split filter to list splitter (MEDIUM - 12 issues)\",\n      \"change_008: Add philosophical claim detection to Pass 2 evaluation (MEDIUM - 18 issues)\"\n    ],\n    \"long_term\": [\n      \"change_009: Add trailing punctuation stripping to entity cleaner (LOW - 3 issues)\",\n      \"change_010: Add case normalization to deduplicator (LOW - 4 issues)\",\n      \"change_011: Tighten type validator rules (LOW - 8 issues)\",\n      \"change_012: Create first-person resolver module (LOW - 7 issues)\",\n      \"change_013: Update orchestrator config for new modules (LOW)\",\n      \"change_014: Add comprehensive examples to Pass 1 prompt (MEDIUM - reinforcement)\",\n      \"change_015: Add entity specificity penalty to Pass 2 evaluation (MEDIUM - 12 issues)\"\n    ]\n  },\n  \"testing_strategy\": {\n    \"unit_tests\": [\n      \"Test DedicationParser with V9 dedication examples (expect 2 clean relationships, not 6 malformed)\",\n      \"Test PraiseQuoteCorrector with author vs non-author sources\",\n      \"Test PronounResolver with possessive pronouns ('my people', 'our land')\",\n      \"Test ListSplitter with adjective lists (should NOT split) and name lists (should split)\",\n      \"Test EntityCleaner with trailing punctuation\",\n      \"Test Deduplicator with case variations\",\n      \"Test TypeValidator with 'my people' (expect 'People/Ethnic Group', not 'Abstract Concept')\",\n      \"Test FirstPersonResolver with 'we humans' (expect 'humanity')\"\n    ],\n    \"integration_tests\": [\n      \"Run V10 extraction on Soil Handbook foreword (pages 5-11)\",\n      \"Compare V10 output to V9 output on same text\",\n      \"Verify critical issues eliminated (dedication parsing, author misattribution)\",\n      \"Verify high-priority issues reduced by 70%+\",\n      \"Check for any new issues introduced by changes\"\n    ],\n    \"regression_tests\": [\n      \"Run V10 on V7 test set (ensure no regression from V7 baseline)\",\n      \"Compare V10 metrics to V7 metrics (target: match or exceed V7's 6.71% issue rate)\",\n      \"Verify V10 issue rate < 5% (production threshold)\"\n    ],\n    \"success_criteria\": [\n      \"Critical issues: 12 \u2192 0 (100% reduction)\",\n      \"High priority issues: 38 \u2192 8 (79% reduction)\",\n      \"Medium priority issues: 45 \u2192 15 (67% reduction)\",\n      \"Overall issue rate: 13.6% \u2192 3.2% (76% reduction)\",\n      \"Grade improvement: C+ \u2192 A- (meets production threshold)\"\n    ]\n  },\n  \"rollback_plan\": {\n    \"backup_location\": \"kg_extraction_playbook_backups/v9/\",\n    \"backup_command\": \"python scripts/backup_version.py --version v9\",\n    \"rollback_command\": \"python scripts/rollback_version.py --to v9\",\n    \"rollback_conditions\": [\n      \"If V10 quality worse than V9 (issue rate > 13.6%)\",\n      \"If V10 introduces new critical issues not present in V9\",\n      \"If V10 extraction crashes or fails on test corpus\",\n      \"If unit tests fail for critical modules (DedicationParser, PronounResolver)\",\n      \"If integration tests show >20% regression in any category\"\n    ],\n    \"rollback_verification\": [\n      \"Verify V9 files restored from backup\",\n      \"Run V9 extraction on test sample\",\n      \"Confirm V9 metrics match pre-V10 baseline\"\n    ]\n  },\n  \"risk_assessment\": {\n    \"high_risk_changes\": [],\n    \"medium_risk_changes\": [\n      \"change_001: Dedication parser rewrite (complex logic, but isolated module)\",\n      \"change_003: Pronoun resolver extension (adds context dependency, may need tuning)\"\n    ],\n    \"low_risk_changes\": [\n      \"change_002: Praise quote corrector (simple author check)\",\n      \"change_004-008: Prompt enhancements (easily reversible)\",\n      \"change_009-011: Simple code fixes (punctuation, case, types)\",\n      \"change_012-013: New module (isolated, can be disabled)\",\n      \"change_014-015: Additional prompt guidance (reinforcement only)\"\n    ],\n    \"mitigation_strategies\": {\n      \"dedication_parser\": \"Extensive unit tests with V9 examples before deployment. Fallback: disable module if issues persist.\",\n      \"pronoun_resolver\": \"Test with diverse document contexts. Add confidence scoring to flag uncertain resolutions.\",\n      \"prompt_changes\": \"A/B test prompts on sample corpus. Monitor for unintended extraction changes.\"\n    }\n  },\n  \"estimated_timeline\": {\n    \"development\": \"3-4 days (2 days for critical fixes, 1-2 days for prompt enhancements and testing)\",\n    \"testing\": \"2 days (1 day unit tests, 1 day integration/regression tests)\",\n    \"deployment\": \"0.5 days (backup V9, deploy V10, monitor initial runs)\",\n    \"total\": \"5.5-6.5 days\"\n  },\n  \"impact_summary\": {\n    \"before_v10\": {\n      \"critical_issues\": 12,\n      \"high_priority_issues\": 38,\n      \"medium_priority_issues\": 45,\n      \"mild_issues\": 22,\n      \"total_issues\": 117,\n      \"issue_rate\": \"13.6%\",\n      \"grade\": \"C+\"\n    },\n    \"after_v10_estimated\": {\n      \"critical_issues\": 0,\n      \"high_priority_issues\": 8,\n      \"medium_priority_issues\": 15,\n      \"mild_issues\": 5,\n      \"total_issues\": 28,\n      \"issue_rate\": \"3.2%\",\n      \"grade\": \"A-\"\n    },\n    \"improvement\": {\n      \"critical_reduction\": \"100% (12\u2192",
  "metadata": {
    "curation_date": "2025-10-13T08:41:45.065003",
    "source_version": 9,
    "target_version": 10,
    "reflector_analysis_id": "2025-10-13T08:26:43.410997",
    "curator_version": "1.0_claude",
    "model_used": "claude-sonnet-4-5-20250929"
  }
}