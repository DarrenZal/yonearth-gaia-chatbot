#!/usr/bin/env python3
"""
Generate Voronoi 4 "Progressive Disclosure" hierarchy.

Pipeline (Hybrid Layout & Constraint):
1) Hybrid Supervised UMAP over text embeddings with high supervision (target_weight=0.9)
   using Level 2 community IDs as targets (fallback to L1 or "Unclassified" so no rows drop).
2) Bottom-up geometry using Shapely:
   - Build Voronoi polygons for lowest available clusters (L1) from UMAP centroids.
   - L2 polygons = unary_union(child L1 polygons).
   - L3 polygons = unary_union(child L2 polygons).
   This guarantees hierarchical containment.
3) Enforcer: clamp every entity inside its L1 polygon along the vector to the polygon centroid.
4) Emit nested JSON (L3 -> L2 -> L1 -> entities) at data/voronoi4_hierarchy.json.
"""

import json
import sys
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from shapely.geometry import LineString, Point, Polygon, box
from shapely.ops import unary_union

# --------------------------------------------------------------------------------------
# Paths & constants
# --------------------------------------------------------------------------------------

ROOT = Path("/home/claudeuser/yonearth-gaia-chatbot")
HIERARCHY_PATH = ROOT / "data/graphrag_hierarchy/graphrag_hierarchy.json"
EMBEDDINGS_CACHE = ROOT / "data/graphrag_hierarchy/entity_embeddings_cache.npy"
OUTPUT_JSON = ROOT / "data/graphrag_hierarchy/voronoi4_hierarchy.json"

UNCLASSIFIED_L1 = "unclassified_l1"
UNCLASSIFIED_L2 = "unclassified_l2"
UNCLASSIFIED_L3 = "unclassified_l3"
UNCLASSIFIED_LABEL = "Unclassified"

# Hybrid supervised UMAP parameters
UMAP_N_NEIGHBORS = 50
UMAP_MIN_DIST = 0.1
UMAP_TARGET_WEIGHT = 0.9
UMAP_METRIC = "cosine"
UMAP_RANDOM_STATE = 42


# --------------------------------------------------------------------------------------
# Data loading helpers
# --------------------------------------------------------------------------------------

def load_hierarchy(path: Path) -> dict:
    print(f"Loading hierarchy from {path}...")
    with path.open() as f:
        data = json.load(f)

    print(
        f"  Entities: {len(data.get('entities', {})):,} | "
        f"L1 clusters: {len(data.get('clusters', {}).get('level_1', {})):,} | "
        f"L2 clusters: {len(data.get('clusters', {}).get('level_2', {})):,} | "
        f"L3 clusters: {len(data.get('clusters', {}).get('level_3', {})):,}"
    )
    return data


def load_cached_embeddings(cache_path: Path, entities: Dict[str, dict]) -> Tuple[np.ndarray, List[str]]:
    """
    Load cached text embeddings (generated by layout_supervised_semantic.py).
    Falls back to entity order from the cache to keep alignment.
    """
    if not cache_path.exists():
        print(f"ERROR: Embedding cache missing at {cache_path}")
        print("Generate embeddings first (layout_supervised_semantic.py).")
        sys.exit(1)

    cache = np.load(cache_path, allow_pickle=True).item()
    embeddings = cache.get("embeddings")
    nodes = list(cache.get("nodes", []))

    if embeddings is None or not nodes:
        print("ERROR: Cache file is malformed (missing embeddings or node list).")
        sys.exit(1)

    missing = set(nodes) - set(entities.keys())
    if missing:
        print(f"WARNING: {len(missing)} cached nodes not present in entities. They will still be kept.")

    print(f"Loaded cached embeddings: {embeddings.shape}")
    return embeddings, nodes


# --------------------------------------------------------------------------------------
# Cluster mapping helpers
# --------------------------------------------------------------------------------------

def ensure_unclassified_clusters(
    clusters: dict, entities: Dict[str, dict]
) -> Tuple[Dict[str, dict], Dict[str, dict], Dict[str, dict]]:
    """
    Add synthetic unclassified clusters when entities lack L1/L2 coverage.
    """
    l1_clusters = dict(clusters.get("level_1", {}))
    l2_clusters = dict(clusters.get("level_2", {}))
    l3_clusters = dict(clusters.get("level_3", {}))

    # Build entity -> L1 map
    entity_to_l1 = {}
    for cid, cdata in l1_clusters.items():
        for ent in cdata.get("entities", []):
            entity_to_l1[ent] = cid

    missing_entities = set(entities.keys()) - set(entity_to_l1.keys())
    if missing_entities:
        print(f"  Adding {len(missing_entities):,} entities to synthetic {UNCLASSIFIED_L1}")
        l1_clusters[UNCLASSIFIED_L1] = {
            "id": UNCLASSIFIED_L1,
            "title": UNCLASSIFIED_LABEL,
            "name": UNCLASSIFIED_LABEL,
            "entities": sorted(missing_entities),
            "children": [],
        }

    # Map L1 -> L2
    l1_to_l2 = {}
    for l2_id, cdata in l2_clusters.items():
        for child in cdata.get("children", []):
            l1_to_l2[child] = l2_id

    if UNCLASSIFIED_L1 in l1_clusters and UNCLASSIFIED_L1 not in l1_to_l2:
        print(f"  Creating synthetic L2 parent {UNCLASSIFIED_L2} for {UNCLASSIFIED_L1}")
        l2_clusters[UNCLASSIFIED_L2] = {
            "id": UNCLASSIFIED_L2,
            "title": UNCLASSIFIED_LABEL,
            "name": UNCLASSIFIED_LABEL,
            "children": [UNCLASSIFIED_L1],
        }
        l1_to_l2[UNCLASSIFIED_L1] = UNCLASSIFIED_L2

    # Map L2 -> L3
    l2_to_l3 = {}
    for l3_id, cdata in l3_clusters.items():
        for child in cdata.get("children", []):
            l2_to_l3[child] = l3_id

    if UNCLASSIFIED_L2 in l2_clusters and UNCLASSIFIED_L2 not in l2_to_l3:
        print(f"  Creating synthetic L3 parent {UNCLASSIFIED_L3} for {UNCLASSIFIED_L2}")
        l3_clusters[UNCLASSIFIED_L3] = {
            "id": UNCLASSIFIED_L3,
            "title": UNCLASSIFIED_LABEL,
            "name": UNCLASSIFIED_LABEL,
            "children": [UNCLASSIFIED_L2],
        }
        l2_to_l3[UNCLASSIFIED_L2] = UNCLASSIFIED_L3

    return l1_clusters, l2_clusters, l3_clusters


def build_entity_cluster_maps(
    l1_clusters: Dict[str, dict],
    l2_clusters: Dict[str, dict],
    l3_clusters: Dict[str, dict],
    nodes: List[str],
) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, str]]:
    """
    Returns entity -> l1, entity -> l2, entity -> l3 maps with fallbacks.
    """
    l1_entity_map = {}
    for l1_id, cdata in l1_clusters.items():
        for ent in cdata.get("entities", []):
            l1_entity_map[ent] = l1_id

    l1_to_l2 = {}
    for l2_id, cdata in l2_clusters.items():
        for child in cdata.get("children", []):
            l1_to_l2[child] = l2_id

    l2_to_l3 = {}
    for l3_id, cdata in l3_clusters.items():
        for child in cdata.get("children", []):
            l2_to_l3[child] = l3_id

    entity_to_l1 = {}
    entity_to_l2 = {}
    entity_to_l3 = {}

    for ent in nodes:
        l1 = l1_entity_map.get(ent, UNCLASSIFIED_L1)
        l2 = l1_to_l2.get(l1, UNCLASSIFIED_L2)
        l3 = l2_to_l3.get(l2, UNCLASSIFIED_L3)
        entity_to_l1[ent] = l1
        entity_to_l2[ent] = l2
        entity_to_l3[ent] = l3

    return entity_to_l1, entity_to_l2, entity_to_l3


# --------------------------------------------------------------------------------------
# UMAP + geometry helpers
# --------------------------------------------------------------------------------------

def run_hybrid_umap(embeddings: np.ndarray, labels: List[str]) -> np.ndarray:
    print("\nRunning Hybrid Supervised UMAP...")
    print(
        f"  n_neighbors={UMAP_N_NEIGHBORS}, min_dist={UMAP_MIN_DIST}, "
        f"target_weight={UMAP_TARGET_WEIGHT}, metric={UMAP_METRIC}"
    )
    try:
        import umap
    except ImportError:
        print("ERROR: umap-learn not installed. pip install umap-learn")
        sys.exit(1)

    unique_labels = sorted(set(labels))
    label_to_idx = {label: i for i, label in enumerate(unique_labels)}
    numeric_labels = np.array([label_to_idx[label] for label in labels])

    reducer = umap.UMAP(
        n_components=2,
        n_neighbors=UMAP_N_NEIGHBORS,
        min_dist=UMAP_MIN_DIST,
        metric=UMAP_METRIC,
        target_weight=UMAP_TARGET_WEIGHT,
        random_state=UMAP_RANDOM_STATE,
        verbose=True,
    )

    coords = reducer.fit_transform(embeddings, y=numeric_labels)
    print(f"  Output coordinates: {coords.shape}")
    print(f"  X range [{coords[:,0].min():.3f}, {coords[:,0].max():.3f}]")
    print(f"  Y range [{coords[:,1].min():.3f}, {coords[:,1].max():.3f}]")
    return coords


def compute_bounds(df: pd.DataFrame) -> Tuple[float, float, float, float]:
    min_x, max_x = df["x"].min(), df["x"].max()
    min_y, max_y = df["y"].min(), df["y"].max()
    span = max(max_x - min_x, max_y - min_y)
    padding = span * 0.1
    return min_x - padding, min_y - padding, max_x + padding, max_y + padding


def compute_l1_centroids(
    df: pd.DataFrame, l1_clusters: Dict[str, dict]
) -> Dict[str, Tuple[float, float]]:
    centroids = {}
    for l1_id, cdata in l1_clusters.items():
        subset = df[df["l1"] == l1_id]
        if not subset.empty:
            centroids[l1_id] = (subset["x"].mean(), subset["y"].mean())
        elif cdata.get("umap_position"):
            centroids[l1_id] = tuple(cdata["umap_position"][:2])
        elif cdata.get("position"):
            centroids[l1_id] = tuple(cdata["position"][:2])
    print(f"  Computed centroids for {len(centroids)} L1 clusters")
    return centroids


def build_voronoi_polygons(
    centroids: Dict[str, Tuple[float, float]], bounds: Tuple[float, float, float, float]
) -> Dict[str, Polygon]:
    from scipy.spatial import Voronoi

    print("\nGenerating L1 Voronoi polygons...")
    cluster_ids = list(centroids.keys())
    points = np.array([centroids[cid] for cid in cluster_ids])
    min_x, min_y, max_x, max_y = bounds
    span = max(max_x - min_x, max_y - min_y)
    pad = span * 0.25

    # Bounding box and guard points to keep regions finite
    bbox = box(min_x - pad, min_y - pad, max_x + pad, max_y + pad)
    guard = np.array(
        [
            [min_x - pad, min_y - pad],
            [min_x - pad, max_y + pad],
            [max_x + pad, min_y - pad],
            [max_x + pad, max_y + pad],
            [min_x - pad, (min_y + max_y) / 2],
            [max_x + pad, (min_y + max_y) / 2],
            [(min_x + max_x) / 2, min_y - pad],
            [(min_x + max_x) / 2, max_y + pad],
        ]
    )

    vor = Voronoi(np.vstack([points, guard]))
    polygons = {}

    for idx, cid in enumerate(cluster_ids):
        region_idx = vor.point_region[idx]
        region = vor.regions[region_idx]
        cx, cy = centroids[cid]

        if not region or -1 in region:
            poly = Point(cx, cy).buffer(span * 0.01)
        else:
            try:
                verts = [vor.vertices[i] for i in region]
                poly = Polygon(verts)
            except Exception:
                poly = Point(cx, cy).buffer(span * 0.01)

        clipped = poly.intersection(bbox)
        if clipped.is_empty:
            clipped = poly
        polygons[cid] = clipped

    print(f"  Built {len(polygons)} L1 polygons")
    return polygons


def union_polygons(child_map: Dict[str, List[str]], source: Dict[str, Polygon]) -> Dict[str, Polygon]:
    result = {}
    for parent, children in child_map.items():
        geoms = [source[c] for c in children if c in source]
        if not geoms:
            continue
        merged = unary_union(geoms)
        if merged.geom_type == "GeometryCollection":
            merged = unary_union([g for g in merged.geoms if g.area > 0])
        result[parent] = merged
    print(f"  Built {len(result)} polygons from unions")
    return result


def clamp_point_to_polygon(pt: Point, poly: Polygon) -> Tuple[float, float]:
    if poly.is_empty:
        return pt.x, pt.y

    centroid = poly.centroid
    line = LineString([centroid, pt])
    hit = line.intersection(poly.boundary)

    target = None
    if hit.is_empty:
        target = centroid
    elif isinstance(hit, Point):
        target = hit
    elif hit.geom_type == "MultiPoint":
        pts = list(hit.geoms)
        # Choose the intersection farthest from centroid (toward the outside)
        target = max(pts, key=lambda p: p.distance(centroid))
    elif hit.geom_type in ("LineString", "MultiLineString"):
        coords = list(hit.coords) if hit.geom_type == "LineString" else list(hit.geoms)[0].coords
        target = Point(coords[-1])
    else:
        target = centroid

    vec = np.array(target.coords[0]) - np.array(centroid.coords[0])
    clamped = np.array(centroid.coords[0]) + vec * 0.98
    return float(clamped[0]), float(clamped[1])


def enforce_containment(df: pd.DataFrame, l1_polys: Dict[str, Polygon]) -> int:
    moved = 0
    new_x = []
    new_y = []

    for _, row in df.iterrows():
        poly = l1_polys.get(row["l1"])
        pt = Point(row["x"], row["y"])
        if poly is None or poly.buffer(1e-9).contains(pt):
            new_x.append(row["x"])
            new_y.append(row["y"])
            continue

        clamped_x, clamped_y = clamp_point_to_polygon(pt, poly)
        new_x.append(clamped_x)
        new_y.append(clamped_y)
        moved += 1

    df["x"] = new_x
    df["y"] = new_y
    print(f"  Enforcer moved {moved} entities back inside their L1 polygons")
    return moved


# --------------------------------------------------------------------------------------
# Serialization helpers
# --------------------------------------------------------------------------------------

def round_point(point: Tuple[float, float], ndigits: int = 6) -> List[float]:
    return [round(point[0], ndigits), round(point[1], ndigits)]


def geometry_to_rings(geom: Polygon, ndigits: int = 6) -> List[List[List[float]]]:
    if geom is None or geom.is_empty:
        return []

    geoms = [geom] if geom.geom_type == "Polygon" else list(geom.geoms)
    rings = []
    for g in geoms:
        coords = list(g.exterior.coords[:-1])
        if len(coords) < 3:
            continue
        rings.append([round_point(pt, ndigits) for pt in coords])
    return rings


def build_hierarchy_payload(
    l3_clusters: Dict[str, dict],
    l2_clusters: Dict[str, dict],
    l1_clusters: Dict[str, dict],
    l3_polys: Dict[str, Polygon],
    l2_polys: Dict[str, Polygon],
    l1_polys: Dict[str, Polygon],
    df: pd.DataFrame,
    entity_meta: Dict[str, dict],
) -> List[dict]:
    l1_entities = defaultdict(list)
    for _, row in df.iterrows():
        ent = row["entity"]
        meta = entity_meta.get(ent, {})
        l1_entities[row["l1"]].append(
            {
                "id": ent,
                "x": round(row["x"], 6),
                "y": round(row["y"], 6),
                "type": meta.get("type", "UNKNOWN"),
            }
        )

    l2_children = defaultdict(list)
    for l2_id, cdata in l2_clusters.items():
        for child in cdata.get("children", []):
            l2_children[l2_id].append(child)

    l3_children = defaultdict(list)
    for l3_id, cdata in l3_clusters.items():
        for child in cdata.get("children", []):
            l3_children[l3_id].append(child)

    # Build nested structure
    payload = []
    for l3_id, cdata in l3_clusters.items():
        l3_node = {
            "id": l3_id,
            "name": cdata.get("title") or cdata.get("name") or l3_id,
            "level": 3,
            "centroid": round_point(l3_polys[l3_id].centroid.coords[0]) if l3_id in l3_polys else None,
            "polygons": geometry_to_rings(l3_polys.get(l3_id)),
            "children": [],
        }

        for l2_id in l3_children.get(l3_id, []):
            if l2_id not in l2_clusters:
                continue
            l2_data = l2_clusters[l2_id]
            l2_node = {
                "id": l2_id,
                "name": l2_data.get("title") or l2_data.get("name") or l2_id,
                "level": 2,
                "centroid": round_point(l2_polys[l2_id].centroid.coords[0]) if l2_id in l2_polys else None,
                "polygons": geometry_to_rings(l2_polys.get(l2_id)),
                "children": [],
            }

            for l1_id in l2_children.get(l2_id, []):
                if l1_id not in l1_clusters:
                    continue
                l1_data = l1_clusters[l1_id]
                l1_node = {
                    "id": l1_id,
                    "name": l1_data.get("title") or l1_data.get("name") or l1_id,
                    "level": 1,
                    "centroid": round_point(l1_polys[l1_id].centroid.coords[0]) if l1_id in l1_polys else None,
                    "polygons": geometry_to_rings(l1_polys.get(l1_id)),
                    "entities": l1_entities.get(l1_id, []),
                }
                l2_node["children"].append(l1_node)

            l3_node["children"].append(l2_node)

        payload.append(l3_node)

    return payload


# --------------------------------------------------------------------------------------
# Main
# --------------------------------------------------------------------------------------

def main():
    data = load_hierarchy(HIERARCHY_PATH)
    entities = data.get("entities", {})
    clusters = data.get("clusters", {})

    embeddings, node_list = load_cached_embeddings(EMBEDDINGS_CACHE, entities)
    l1_clusters, l2_clusters, l3_clusters = ensure_unclassified_clusters(clusters, entities)
    entity_to_l1, entity_to_l2, entity_to_l3 = build_entity_cluster_maps(
        l1_clusters, l2_clusters, l3_clusters, node_list
    )

    labels = [entity_to_l2.get(node, UNCLASSIFIED_L2) for node in node_list]
    coords = run_hybrid_umap(embeddings, labels)

    df = pd.DataFrame(
        {
            "entity": node_list,
            "x": coords[:, 0],
            "y": coords[:, 1],
            "l1": [entity_to_l1.get(node, UNCLASSIFIED_L1) for node in node_list],
            "l2": [entity_to_l2.get(node, UNCLASSIFIED_L2) for node in node_list],
            "l3": [entity_to_l3.get(node, UNCLASSIFIED_L3) for node in node_list],
        }
    )

    bounds = compute_bounds(df)
    l1_centroids = compute_l1_centroids(df, l1_clusters)
    l1_polys = build_voronoi_polygons(l1_centroids, bounds)

    l2_child_map = {
        l2_id: cdata.get("children", []) for l2_id, cdata in l2_clusters.items()
    }
    l3_child_map = {
        l3_id: cdata.get("children", []) for l3_id, cdata in l3_clusters.items()
    }

    l2_polys = union_polygons(l2_child_map, l1_polys)
    l3_polys = union_polygons(l3_child_map, l2_polys)

    moved = enforce_containment(df, l1_polys)

    # Recompute bounds after clamping for nicer scaling
    final_bounds = compute_bounds(df)

    hierarchy_payload = build_hierarchy_payload(
        l3_clusters,
        l2_clusters,
        l1_clusters,
        l3_polys,
        l2_polys,
        l1_polys,
        df,
        entities,
    )

    output = {
        "metadata": {
            "layout_type": "voronoi4_progressive",
            "total_entities": len(df),
            "total_l1": len(l1_clusters),
            "total_l2": len(l2_clusters),
            "total_l3": len(l3_clusters),
            "moved_entities": moved,
            "umap_params": {
                "n_neighbors": UMAP_N_NEIGHBORS,
                "min_dist": UMAP_MIN_DIST,
                "target_weight": UMAP_TARGET_WEIGHT,
                "metric": UMAP_METRIC,
            },
            "bounds": {
                "min_x": final_bounds[0],
                "min_y": final_bounds[1],
                "max_x": final_bounds[2],
                "max_y": final_bounds[3],
            },
        },
        "clusters": hierarchy_payload,
    }

    OUTPUT_JSON.parent.mkdir(parents=True, exist_ok=True)
    with OUTPUT_JSON.open("w", encoding="utf-8") as f:
        json.dump(output, f, indent=2)

    print(f"\nSaved Voronoi 4 hierarchy to {OUTPUT_JSON}")


if __name__ == "__main__":
    main()
