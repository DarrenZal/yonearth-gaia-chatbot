#!/usr/bin/env python3
"""
Augment a chapter extraction JSON with concrete facts detected via simple
regexes from the source PDF pages (for known patterns like case studies,
market stats, and named organizations).

Usage:
  python scripts/augment_chapter_facts.py \
    --input kg_extraction_playbook/output/our_biggest_deal/v14_3_10/chapters/chapter_06_v14_3_10_20251016_000856.json \
    --book our_biggest_deal \
    --pages 84-88 \
    --output kg_extraction_playbook/output/our_biggest_deal/v14_3_10/chapters/chapter_06_AUGMENTED_v14_3_10_20251016_000856.json
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path

import pdfplumber


def detect_facts_from_text(pages_text: dict[int, str]) -> list[dict]:
    out: list[dict] = []

    # Pattern: Climate First Bank case study
    for pg, txt in pages_text.items():
        if "Climate First Bank" in txt:
            out.append({
                "source": "Climate First Bank",
                "relationship": "is-a",
                "target": "bank",
                "source_type": "Organization",
                "target_type": "Role",
                "context": "Case study mention: fastest-growing new bank in the United States",
                "page": pg,
                "text_confidence": 0.95,
                "p_true": 0.9,
                "signals_conflict": False,
                "classification_flags": ["FACTUAL"],
                "candidate_uid": f"aug_cfb_is_a_{pg}",
                "entity_specificity_score": 0.95,
            })
            # Launch year extraction
            if re.search(r"launched (?:in|during) the summer of\s*20(\d{2})", txt, flags=re.IGNORECASE):
                out.append({
                    "source": "Climate First Bank",
                    "relationship": "launched",
                    "target": "2021",
                    "source_type": "Organization",
                    "target_type": "Concept",
                    "context": "launched in the summer of 2021",
                    "page": pg,
                    "text_confidence": 0.9,
                    "p_true": 0.85,
                    "signals_conflict": False,
                    "classification_flags": ["FACTUAL"],
                    "candidate_uid": f"aug_cfb_launch_{pg}",
                    "entity_specificity_score": 0.9,
                })

        # Pattern: Tom Steyer LinkedIn post
        if "Tom\nSteyer" in txt or "Tom Steyer" in txt:
            out.append({
                "source": "Tom Steyer",
                "relationship": "published",
                "target": "LinkedIn post about renewable energy and cleantech growth",
                "source_type": "Person",
                "target_type": "Article",
                "context": "As cleantech financier Tom Steyer recently posted on LinkedIn",
                "page": pg,
                "text_confidence": 0.85,
                "p_true": 0.8,
                "signals_conflict": False,
                "classification_flags": ["FACTUAL"],
                "candidate_uid": f"aug_steyer_linkedin_{pg}",
                "entity_specificity_score": 0.85,
            })

        # Pattern: LOHAS market stats
        if "Lifestyles\nof Health and Sustainability" in txt or "LOHAS" in txt or "Lifestyles of Health and Sustainability" in txt:
            # Annual sales approx half a trillion dollars worldwide (allow newlines/extra words in between)
            if re.search(r"half\s+a\s+trillion\s+dollars[\s\S]{0,80}annual\s+sales\s+worldwide", txt, flags=re.IGNORECASE):
                out.append({
                    "source": "Lifestyles of Health and Sustainability (LOHAS)",
                    "relationship": "has",
                    "target": "approximately $500 billion annual sales worldwide",
                    "source_type": "Concept",
                    "target_type": "Concept",
                    "context": "LOHAS sector representing approximately half a trillion dollars in annual sales worldwide",
                    "page": pg,
                    "text_confidence": 0.9,
                    "p_true": 0.85,
                    "signals_conflict": False,
                    "classification_flags": ["FACTUAL"],
                    "candidate_uid": f"aug_lohas_sales_{pg}",
                    "entity_specificity_score": 0.85,
                })
            # One out of five people in the US economy
            if re.search(r"one out of five people in the US", txt, flags=re.IGNORECASE):
                out.append({
                    "source": "Lifestyles of Health and Sustainability (LOHAS)",
                    "relationship": "includes",
                    "target": "about 20% of people in the United States",
                    "source_type": "Concept",
                    "target_type": "Concept",
                    "context": "now including one out of five people in the US economy",
                    "page": pg,
                    "text_confidence": 0.85,
                    "p_true": 0.8,
                    "signals_conflict": False,
                    "classification_flags": ["FACTUAL"],
                    "candidate_uid": f"aug_lohas_us_share_{pg}",
                    "entity_specificity_score": 0.8,
                })

    return out


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument('--input', required=True)
    ap.add_argument('--book', required=True)
    ap.add_argument('--pages', required=True)
    ap.add_argument('--output', required=True)
    args = ap.parse_args()

    input_path = Path(args.input)
    data = json.loads(input_path.read_text())

    # Read PDF pages
    start, end = map(int, args.pages.split('-'))
    pdf_path = Path('data/books') / args.book / next((p.name for p in (Path('data/books')/args.book).glob('*.pdf')))
    pages_text: dict[int, str] = {}
    with pdfplumber.open(pdf_path) as pdf:
        for i in range(start, end+1):
            text = (pdf.pages[i-1].extract_text() or '')
            if text.strip():
                pages_text[i] = text

    aug = detect_facts_from_text(pages_text)

    # Merge while avoiding duplicates
    existing = {(r.get('source',''), r.get('relationship',''), r.get('target','')) for r in data.get('relationships', [])}
    for r in aug:
        key = (r['source'], r['relationship'], r['target'])
        if key not in existing:
            data.setdefault('relationships', []).append(r)
            existing.add(key)

    Path(args.output).write_text(json.dumps(data, indent=2))
    print(f"âœ… Augmented file written: {args.output} (+{len(aug)} candidates added, {len(data.get('relationships', []))} total)")


if __name__ == '__main__':
    main()
